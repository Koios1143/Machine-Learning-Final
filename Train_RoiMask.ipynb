{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgTxEZ13Xs51",
    "outputId": "f1dba251-cae8-4a86-8a6b-002d71c1411f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# %cd '/content/drive/My Drive/MindEye'\n",
    "# os.chdir('/content/drive/My Drive/MindEye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HHkuK63-YIem"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils import load_image, save_image, encode_img, decode_img, to_PIL\n",
    "import torch.nn.functional as F\n",
    "from diffusers.models.vae import Decoder\n",
    "from diffusers.models import AutoencoderKL\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y4Vtj4v0YR2a"
   },
   "outputs": [],
   "source": [
    "def DoOneRoiMask(data_path, subject_num, data, LR, roi, maskedFmri):\n",
    "\n",
    "  if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
    "      roi_class = 'prf-visualrois'\n",
    "  elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
    "      roi_class = 'floc-bodies'\n",
    "  elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
    "      roi_class = 'floc-faces'\n",
    "  elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
    "      roi_class = 'floc-places'\n",
    "  elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
    "      roi_class = 'floc-words'\n",
    "  elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
    "      roi_class = 'streams'\n",
    "\n",
    "  # load mask file\n",
    "  roi_space_dir = os.path.join(data_path, 'subj0{}'.format(subject_num), 'roi_masks',\n",
    "      LR[0]+'h.'+roi_class+'_space.npy')\n",
    "  roi_map_dir = os.path.join(data_path, 'subj0{}'.format(subject_num), 'roi_masks',\n",
    "      'mapping_'+roi_class+'.npy')\n",
    "  roi_space = np.load(roi_space_dir)\n",
    "  roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n",
    "\n",
    "  # Select the vertices corresponding to the ROI of interest\n",
    "  roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
    "\n",
    "  target_index = np.where(np.isin(roi_space, roi_mapping))[0]\n",
    "\n",
    "  for i in target_index:\n",
    "    maskedFmri[:, i] = data[:, i]\n",
    "\n",
    "  return maskedFmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FdCcMGlUY6Ba"
   },
   "outputs": [],
   "source": [
    "def GetRoiMaskedFmri(data_path, subject_num, LR, regions):\n",
    "  \"\"\"\n",
    "    data_path: string -- path to folder for subj0x\n",
    "    subject_num: int -- subject number for training ex: 1\n",
    "    LR: string \"left\" or \"right\" -- specify the fmri data is left or right hemisphere\n",
    "    regoins: list of strings of interseted regions, split by bankspace -- ex: [\"FFA-1\", \"OPA\"]\n",
    "\n",
    "    return: masked fmri data -- (n, 19004) or (n, 20054)\n",
    "  \"\"\"\n",
    "  if LR == \"left\":\n",
    "    data = np.load(os.path.join(data_path, 'subj0{}'.format(subject_num), 'training_split/training_fmri',\n",
    "    LR[0]+'h_training_fmri.npy'))\n",
    "    assert (data.shape[1] == 19004)\n",
    "  elif LR == \"right\":\n",
    "    data = np.load(os.path.join(data_path, 'subj0{}'.format(subject_num), 'training_split/training_fmri',\n",
    "    LR[0]+'h_training_fmri.npy'))\n",
    "    assert (data.shape[1] == 20544)\n",
    "\n",
    "  maskedFmriData = np.zeros_like(data, dtype=float)\n",
    "\n",
    "  for region in regions:\n",
    "    DoOneRoiMask(data_path=data_path, subject_num=subject_num, data=data, LR=LR, roi=region, maskedFmri=maskedFmriData)\n",
    "\n",
    "  return maskedFmriData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A4UG6FDCZhSZ"
   },
   "outputs": [],
   "source": [
    "# usage\n",
    "\n",
    "# determine the datapath and training subject\n",
    "data_path = './dataset'\n",
    "subject_num = 1\n",
    "\n",
    "# get masked fmri data\n",
    "ROIs_test1 = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"FFA-1\", \"FFA-2\", \"PPA\"]\n",
    "ROIs_test2 = [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\"]\n",
    "ROIs_test3 = [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]\n",
    "\n",
    "lh = GetRoiMaskedFmri(data_path=data_path, subject_num=subject_num, LR=\"left\", regions=ROIs_test1)\n",
    "rh = GetRoiMaskedFmri(data_path=data_path, subject_num=subject_num, LR=\"right\", regions=ROIs_test1)\n",
    "lrh = np.concatenate((lh, rh), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ozpzTPeQd_vj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voxel2StableDiffusionModel(torch.nn.Module):\n",
    "    # define the prototype of the module\n",
    "    def __init__(self, in_dim=39548, h=2048, n_blocks=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin0 = nn.Sequential(\n",
    "            nn.Linear(in_dim, h, bias=False),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(h, h, bias=False),\n",
    "                nn.LayerNorm(h),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.3),\n",
    "            ) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.lin1 = nn.Linear(h, 16384, bias=False)\n",
    "        self.norm = nn.GroupNorm(1, 64)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "        init_weights(self.lin0)\n",
    "        self.mlp.apply(init_weights)\n",
    "        init_weights(self.lin1)\n",
    "\n",
    "        self.upsampler = Decoder(\n",
    "            in_channels=64,\n",
    "            out_channels=4,\n",
    "            up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
    "            block_out_channels=[64, 128, 256],\n",
    "            layers_per_block=1,\n",
    "        )\n",
    "        for parm in self.upsampler.parameters():\n",
    "            parm.require_grad = False\n",
    "        self.upsampler.eval()\n",
    "\n",
    "    # define how it forward, using the module defined above\n",
    "    def forward(self, x):\n",
    "        x = self.lin0(x)\n",
    "        residual = x\n",
    "        for res_block in self.mlp:\n",
    "            x = res_block(x)\n",
    "            x = x + residual\n",
    "            residual = x\n",
    "        x = x.reshape(len(x), -1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.norm(x.reshape(x.shape[0], -1, 16, 16).contiguous())\n",
    "        return self.upsampler(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Voxel2StableDiffusionModel(\n",
       "  (lin0): Sequential(\n",
       "    (0): Linear(in_features=39548, out_features=2048, bias=False)\n",
       "    (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): SiLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (mlp): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (lin1): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "  (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "  (upsampler): Decoder(\n",
       "    (conv_in): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): LoRACompatibleConv(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (to_q): LoRACompatibleLinear(in_features=256, out_features=256, bias=True)\n",
       "          (to_k): LoRACompatibleLinear(in_features=256, out_features=256, bias=True)\n",
       "          (to_v): LoRACompatibleLinear(in_features=256, out_features=256, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): LoRACompatibleLinear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxel2sd = Voxel2StableDiffusionModel()\n",
    "voxel2sd.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 2048]      80,994,304\n",
      "         LayerNorm-2                 [-1, 2048]           4,096\n",
      "              SiLU-3                 [-1, 2048]               0\n",
      "           Dropout-4                 [-1, 2048]               0\n",
      "            Linear-5                 [-1, 2048]       4,194,304\n",
      "         LayerNorm-6                 [-1, 2048]           4,096\n",
      "              SiLU-7                 [-1, 2048]               0\n",
      "           Dropout-8                 [-1, 2048]               0\n",
      "            Linear-9                 [-1, 2048]       4,194,304\n",
      "        LayerNorm-10                 [-1, 2048]           4,096\n",
      "             SiLU-11                 [-1, 2048]               0\n",
      "          Dropout-12                 [-1, 2048]               0\n",
      "           Linear-13                 [-1, 2048]       4,194,304\n",
      "        LayerNorm-14                 [-1, 2048]           4,096\n",
      "             SiLU-15                 [-1, 2048]               0\n",
      "          Dropout-16                 [-1, 2048]               0\n",
      "           Linear-17                 [-1, 2048]       4,194,304\n",
      "        LayerNorm-18                 [-1, 2048]           4,096\n",
      "             SiLU-19                 [-1, 2048]               0\n",
      "          Dropout-20                 [-1, 2048]               0\n",
      "           Linear-21                [-1, 16384]      33,554,432\n",
      "        GroupNorm-22           [-1, 64, 16, 16]             128\n",
      "           Conv2d-23          [-1, 256, 16, 16]         147,712\n",
      "        GroupNorm-24          [-1, 256, 16, 16]             512\n",
      "             SiLU-25          [-1, 256, 16, 16]               0\n",
      "             SiLU-26          [-1, 256, 16, 16]               0\n",
      "             SiLU-27          [-1, 256, 16, 16]               0\n",
      "             SiLU-28          [-1, 256, 16, 16]               0\n",
      "             SiLU-29          [-1, 256, 16, 16]               0\n",
      "             SiLU-30          [-1, 256, 16, 16]               0\n",
      "             SiLU-31          [-1, 256, 16, 16]               0\n",
      "             SiLU-32          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-33          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-34          [-1, 256, 16, 16]             512\n",
      "             SiLU-35          [-1, 256, 16, 16]               0\n",
      "             SiLU-36          [-1, 256, 16, 16]               0\n",
      "             SiLU-37          [-1, 256, 16, 16]               0\n",
      "             SiLU-38          [-1, 256, 16, 16]               0\n",
      "             SiLU-39          [-1, 256, 16, 16]               0\n",
      "             SiLU-40          [-1, 256, 16, 16]               0\n",
      "             SiLU-41          [-1, 256, 16, 16]               0\n",
      "             SiLU-42          [-1, 256, 16, 16]               0\n",
      "          Dropout-43          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-44          [-1, 256, 16, 16]         590,080\n",
      "    ResnetBlock2D-45          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-46             [-1, 256, 256]             512\n",
      "LoRACompatibleLinear-47             [-1, 256, 256]          65,792\n",
      "LoRACompatibleLinear-48             [-1, 256, 256]          65,792\n",
      "LoRACompatibleLinear-49             [-1, 256, 256]          65,792\n",
      "LoRACompatibleLinear-50             [-1, 256, 256]          65,792\n",
      "          Dropout-51             [-1, 256, 256]               0\n",
      "        Attention-52          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-53          [-1, 256, 16, 16]             512\n",
      "             SiLU-54          [-1, 256, 16, 16]               0\n",
      "             SiLU-55          [-1, 256, 16, 16]               0\n",
      "             SiLU-56          [-1, 256, 16, 16]               0\n",
      "             SiLU-57          [-1, 256, 16, 16]               0\n",
      "             SiLU-58          [-1, 256, 16, 16]               0\n",
      "             SiLU-59          [-1, 256, 16, 16]               0\n",
      "             SiLU-60          [-1, 256, 16, 16]               0\n",
      "             SiLU-61          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-62          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-63          [-1, 256, 16, 16]             512\n",
      "             SiLU-64          [-1, 256, 16, 16]               0\n",
      "             SiLU-65          [-1, 256, 16, 16]               0\n",
      "             SiLU-66          [-1, 256, 16, 16]               0\n",
      "             SiLU-67          [-1, 256, 16, 16]               0\n",
      "             SiLU-68          [-1, 256, 16, 16]               0\n",
      "             SiLU-69          [-1, 256, 16, 16]               0\n",
      "             SiLU-70          [-1, 256, 16, 16]               0\n",
      "             SiLU-71          [-1, 256, 16, 16]               0\n",
      "          Dropout-72          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-73          [-1, 256, 16, 16]         590,080\n",
      "    ResnetBlock2D-74          [-1, 256, 16, 16]               0\n",
      "   UNetMidBlock2D-75          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-76          [-1, 256, 16, 16]             512\n",
      "             SiLU-77          [-1, 256, 16, 16]               0\n",
      "             SiLU-78          [-1, 256, 16, 16]               0\n",
      "             SiLU-79          [-1, 256, 16, 16]               0\n",
      "             SiLU-80          [-1, 256, 16, 16]               0\n",
      "             SiLU-81          [-1, 256, 16, 16]               0\n",
      "             SiLU-82          [-1, 256, 16, 16]               0\n",
      "             SiLU-83          [-1, 256, 16, 16]               0\n",
      "             SiLU-84          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-85          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-86          [-1, 256, 16, 16]             512\n",
      "             SiLU-87          [-1, 256, 16, 16]               0\n",
      "             SiLU-88          [-1, 256, 16, 16]               0\n",
      "             SiLU-89          [-1, 256, 16, 16]               0\n",
      "             SiLU-90          [-1, 256, 16, 16]               0\n",
      "             SiLU-91          [-1, 256, 16, 16]               0\n",
      "             SiLU-92          [-1, 256, 16, 16]               0\n",
      "             SiLU-93          [-1, 256, 16, 16]               0\n",
      "             SiLU-94          [-1, 256, 16, 16]               0\n",
      "          Dropout-95          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-96          [-1, 256, 16, 16]         590,080\n",
      "    ResnetBlock2D-97          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-98          [-1, 256, 16, 16]             512\n",
      "             SiLU-99          [-1, 256, 16, 16]               0\n",
      "            SiLU-100          [-1, 256, 16, 16]               0\n",
      "            SiLU-101          [-1, 256, 16, 16]               0\n",
      "            SiLU-102          [-1, 256, 16, 16]               0\n",
      "            SiLU-103          [-1, 256, 16, 16]               0\n",
      "            SiLU-104          [-1, 256, 16, 16]               0\n",
      "            SiLU-105          [-1, 256, 16, 16]               0\n",
      "            SiLU-106          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-107          [-1, 256, 16, 16]         590,080\n",
      "       GroupNorm-108          [-1, 256, 16, 16]             512\n",
      "            SiLU-109          [-1, 256, 16, 16]               0\n",
      "            SiLU-110          [-1, 256, 16, 16]               0\n",
      "            SiLU-111          [-1, 256, 16, 16]               0\n",
      "            SiLU-112          [-1, 256, 16, 16]               0\n",
      "            SiLU-113          [-1, 256, 16, 16]               0\n",
      "            SiLU-114          [-1, 256, 16, 16]               0\n",
      "            SiLU-115          [-1, 256, 16, 16]               0\n",
      "            SiLU-116          [-1, 256, 16, 16]               0\n",
      "         Dropout-117          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-118          [-1, 256, 16, 16]         590,080\n",
      "   ResnetBlock2D-119          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-120          [-1, 256, 32, 32]         590,080\n",
      "      Upsample2D-121          [-1, 256, 32, 32]               0\n",
      "UpDecoderBlock2D-122          [-1, 256, 32, 32]               0\n",
      "       GroupNorm-123          [-1, 256, 32, 32]             512\n",
      "            SiLU-124          [-1, 256, 32, 32]               0\n",
      "            SiLU-125          [-1, 256, 32, 32]               0\n",
      "            SiLU-126          [-1, 256, 32, 32]               0\n",
      "            SiLU-127          [-1, 256, 32, 32]               0\n",
      "            SiLU-128          [-1, 256, 32, 32]               0\n",
      "            SiLU-129          [-1, 256, 32, 32]               0\n",
      "            SiLU-130          [-1, 256, 32, 32]               0\n",
      "            SiLU-131          [-1, 256, 32, 32]               0\n",
      "LoRACompatibleConv-132          [-1, 128, 32, 32]         295,040\n",
      "       GroupNorm-133          [-1, 128, 32, 32]             256\n",
      "            SiLU-134          [-1, 128, 32, 32]               0\n",
      "            SiLU-135          [-1, 128, 32, 32]               0\n",
      "            SiLU-136          [-1, 128, 32, 32]               0\n",
      "            SiLU-137          [-1, 128, 32, 32]               0\n",
      "            SiLU-138          [-1, 128, 32, 32]               0\n",
      "            SiLU-139          [-1, 128, 32, 32]               0\n",
      "            SiLU-140          [-1, 128, 32, 32]               0\n",
      "            SiLU-141          [-1, 128, 32, 32]               0\n",
      "         Dropout-142          [-1, 128, 32, 32]               0\n",
      "LoRACompatibleConv-143          [-1, 128, 32, 32]         147,584\n",
      "LoRACompatibleConv-144          [-1, 128, 32, 32]          32,896\n",
      "   ResnetBlock2D-145          [-1, 128, 32, 32]               0\n",
      "       GroupNorm-146          [-1, 128, 32, 32]             256\n",
      "            SiLU-147          [-1, 128, 32, 32]               0\n",
      "            SiLU-148          [-1, 128, 32, 32]               0\n",
      "            SiLU-149          [-1, 128, 32, 32]               0\n",
      "            SiLU-150          [-1, 128, 32, 32]               0\n",
      "            SiLU-151          [-1, 128, 32, 32]               0\n",
      "            SiLU-152          [-1, 128, 32, 32]               0\n",
      "            SiLU-153          [-1, 128, 32, 32]               0\n",
      "            SiLU-154          [-1, 128, 32, 32]               0\n",
      "LoRACompatibleConv-155          [-1, 128, 32, 32]         147,584\n",
      "       GroupNorm-156          [-1, 128, 32, 32]             256\n",
      "            SiLU-157          [-1, 128, 32, 32]               0\n",
      "            SiLU-158          [-1, 128, 32, 32]               0\n",
      "            SiLU-159          [-1, 128, 32, 32]               0\n",
      "            SiLU-160          [-1, 128, 32, 32]               0\n",
      "            SiLU-161          [-1, 128, 32, 32]               0\n",
      "            SiLU-162          [-1, 128, 32, 32]               0\n",
      "            SiLU-163          [-1, 128, 32, 32]               0\n",
      "            SiLU-164          [-1, 128, 32, 32]               0\n",
      "         Dropout-165          [-1, 128, 32, 32]               0\n",
      "LoRACompatibleConv-166          [-1, 128, 32, 32]         147,584\n",
      "   ResnetBlock2D-167          [-1, 128, 32, 32]               0\n",
      "LoRACompatibleConv-168          [-1, 128, 64, 64]         147,584\n",
      "      Upsample2D-169          [-1, 128, 64, 64]               0\n",
      "UpDecoderBlock2D-170          [-1, 128, 64, 64]               0\n",
      "       GroupNorm-171          [-1, 128, 64, 64]             256\n",
      "            SiLU-172          [-1, 128, 64, 64]               0\n",
      "            SiLU-173          [-1, 128, 64, 64]               0\n",
      "            SiLU-174          [-1, 128, 64, 64]               0\n",
      "            SiLU-175          [-1, 128, 64, 64]               0\n",
      "            SiLU-176          [-1, 128, 64, 64]               0\n",
      "            SiLU-177          [-1, 128, 64, 64]               0\n",
      "            SiLU-178          [-1, 128, 64, 64]               0\n",
      "            SiLU-179          [-1, 128, 64, 64]               0\n",
      "LoRACompatibleConv-180           [-1, 64, 64, 64]          73,792\n",
      "       GroupNorm-181           [-1, 64, 64, 64]             128\n",
      "            SiLU-182           [-1, 64, 64, 64]               0\n",
      "            SiLU-183           [-1, 64, 64, 64]               0\n",
      "            SiLU-184           [-1, 64, 64, 64]               0\n",
      "            SiLU-185           [-1, 64, 64, 64]               0\n",
      "            SiLU-186           [-1, 64, 64, 64]               0\n",
      "            SiLU-187           [-1, 64, 64, 64]               0\n",
      "            SiLU-188           [-1, 64, 64, 64]               0\n",
      "            SiLU-189           [-1, 64, 64, 64]               0\n",
      "         Dropout-190           [-1, 64, 64, 64]               0\n",
      "LoRACompatibleConv-191           [-1, 64, 64, 64]          36,928\n",
      "LoRACompatibleConv-192           [-1, 64, 64, 64]           8,256\n",
      "   ResnetBlock2D-193           [-1, 64, 64, 64]               0\n",
      "       GroupNorm-194           [-1, 64, 64, 64]             128\n",
      "            SiLU-195           [-1, 64, 64, 64]               0\n",
      "            SiLU-196           [-1, 64, 64, 64]               0\n",
      "            SiLU-197           [-1, 64, 64, 64]               0\n",
      "            SiLU-198           [-1, 64, 64, 64]               0\n",
      "            SiLU-199           [-1, 64, 64, 64]               0\n",
      "            SiLU-200           [-1, 64, 64, 64]               0\n",
      "            SiLU-201           [-1, 64, 64, 64]               0\n",
      "            SiLU-202           [-1, 64, 64, 64]               0\n",
      "LoRACompatibleConv-203           [-1, 64, 64, 64]          36,928\n",
      "       GroupNorm-204           [-1, 64, 64, 64]             128\n",
      "            SiLU-205           [-1, 64, 64, 64]               0\n",
      "            SiLU-206           [-1, 64, 64, 64]               0\n",
      "            SiLU-207           [-1, 64, 64, 64]               0\n",
      "            SiLU-208           [-1, 64, 64, 64]               0\n",
      "            SiLU-209           [-1, 64, 64, 64]               0\n",
      "            SiLU-210           [-1, 64, 64, 64]               0\n",
      "            SiLU-211           [-1, 64, 64, 64]               0\n",
      "            SiLU-212           [-1, 64, 64, 64]               0\n",
      "         Dropout-213           [-1, 64, 64, 64]               0\n",
      "LoRACompatibleConv-214           [-1, 64, 64, 64]          36,928\n",
      "   ResnetBlock2D-215           [-1, 64, 64, 64]               0\n",
      "UpDecoderBlock2D-216           [-1, 64, 64, 64]               0\n",
      "       GroupNorm-217           [-1, 64, 64, 64]             128\n",
      "            SiLU-218           [-1, 64, 64, 64]               0\n",
      "          Conv2d-219            [-1, 4, 64, 64]           2,308\n",
      "         Decoder-220            [-1, 4, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 138,188,228\n",
      "Trainable params: 138,188,228\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.15\n",
      "Forward/backward pass size (MB): 235.31\n",
      "Params size (MB): 527.15\n",
      "Estimated Total Size (MB): 762.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(voxel2sd, (39548,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some hyperparameters\n",
    "batch_size = 16\n",
    "num_epochs = 200\n",
    "num_train = 5000\n",
    "lr_scheduler = 'cycle'\n",
    "initial_lr = 1e-4\n",
    "max_lr = 5e-4\n",
    "random_seed = 42\n",
    "train_size = 0.7\n",
    "valid_size = 1 - train_size\n",
    "num_workers = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some path information\n",
    "dataset_path = './dataset/'\n",
    "training_path = dataset_path + 'subj0{}/training_split/'\n",
    "training_fmri_path = training_path + 'training_fmri/'\n",
    "training_images_path = training_path + 'training_images/'\n",
    "testing_path = dataset_path + 'subj0{}/test_split/test_fmri/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "  def __init__(self, fmri_data, images_folder, transform=None):\n",
    "    self.fmri_data = fmri_data\n",
    "    self.images_folder = images_folder\n",
    "    self.image_paths = [f\"{images_folder}/{filename}\" for filename in os.listdir(images_folder)]\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.fmri_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    fmri = self.fmri_data[idx]\n",
    "    image_path = self.image_paths[idx]\n",
    "    image = load_image(image_path)\n",
    "\n",
    "    if(self.transform):\n",
    "      image = self.transform(image)\n",
    "\n",
    "    return fmri, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Resize([512, 512])\n",
    "my_dataset = MyDataset(lrh, training_images_path.format(1), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-val split\n",
    "generator = torch.Generator().manual_seed(random_seed)\n",
    "trainset, validset = random_split(my_dataset, [train_size, valid_size], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataloader\n",
    "train_dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(validset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in voxel2sd.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in voxel2sd.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=initial_lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr,\n",
    "                                            total_steps=num_epochs*((num_train//batch_size)//num_workers),\n",
    "                                            final_div_factor=1000,\n",
    "                                            last_epoch=-1, pct_start=2/num_epochs)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "#                                                    milestones=[50*i for i in range(num_epochs*((num_train//batch_size)//num_workers//50))],\n",
    "#                                                    gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Owner/Desktop/Machine Learning/Final Project/CNN/wandb/run-20231214_002815-u5g1p1yx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tkoioslin/MindEye/runs/u5g1p1yx\" target=\"_blank\">hardy-capybara-63</a></strong> to <a href=\"https://wandb.ai/tkoioslin/MindEye\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tkoioslin/MindEye/runs/u5g1p1yx?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5ca43fe3b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize wandb\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"MindEye\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": initial_lr,\n",
    "        \"architecture\": \"MLP\",\n",
    "        \"dataset\": \"NSD\",\n",
    "        \"epochs\": num_epochs,\n",
    "        \"random_seed\": random_seed,\n",
    "        \"train_size\": train_size,\n",
    "        \"valid_size\": valid_size\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 0\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Voxel2StableDiffusionModel(\n",
       "  (lin0): Sequential(\n",
       "    (0): Linear(in_features=39548, out_features=2048, bias=False)\n",
       "    (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): SiLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (mlp): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (lin1): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "  (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "  (upsampler): Decoder(\n",
       "    (conv_in): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): LoRACompatibleConv(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (to_q): LoRACompatibleLinear(in_features=256, out_features=256, bias=True)\n",
       "          (to_k): LoRACompatibleLinear(in_features=256, out_features=256, bias=True)\n",
       "          (to_v): LoRACompatibleLinear(in_features=256, out_features=256, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): LoRACompatibleLinear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('./Models/100')\n",
    "voxel2sd.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch'] + 1\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "progress_bar = tqdm(range(epoch, num_epochs), ncols=150)\n",
    "\n",
    "voxel2sd.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|        | 0/99 [04:11<?, ?it/s, _runtime=264, _timestamp=1.7e+9, train/loss=0.499, train/loss_mse=0.499, train/lr=3.65e-5, train/num_steps=21975]"
     ]
    }
   ],
   "source": [
    "for epoch in progress_bar:\n",
    "    voxel2sd.train()\n",
    "\n",
    "    loss_sum = 0\n",
    "    val_loss_sum = 0\n",
    "\n",
    "    reconst_fails = []\n",
    "\n",
    "    for train_i, data in enumerate(train_dataloader):\n",
    "        voxels, images = data\n",
    "        voxels = voxels.to(device).float()\n",
    "        images = images.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # run image encoder\n",
    "        encoded_latents = torch.cat([encode_img(image, vae).to(device) for image in images])\n",
    "        # MLP forward\n",
    "        encoded_predict = voxel2sd(voxels)\n",
    "        # calulate loss\n",
    "        loss = F.l1_loss(encoded_predict, encoded_latents)\n",
    "        loss_sum += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "            \"train/lr\": lrs[-1],\n",
    "            \"train/num_steps\": 100*219+len(losses),\n",
    "            \"train/loss_mse\": loss_sum / (train_i + 1)\n",
    "        }\n",
    "        wandb.log(logs)\n",
    "\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "    # After training one epoch, evaluation\n",
    "    # save ckpt first\n",
    "    print('saving model')\n",
    "    torch.save({\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': voxel2sd.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'loss': loss,\n",
    "      }, './Models/{}'.format(epoch)\n",
    "    )\n",
    "    print('model saved')\n",
    "    \n",
    "\n",
    "    # print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
