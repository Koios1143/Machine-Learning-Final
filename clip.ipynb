{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MExzTjqAdPIY","executionInfo":{"status":"ok","timestamp":1702106370716,"user_tz":-480,"elapsed":3207,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}},"outputId":"e2fec16d-de8c-4f9c-9820-2b2208a6154f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# use colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["import os\n","%cd '/content/drive/My Drive/MindEye'\n","os.chdir('/content/drive/My Drive/MindEye')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_63iSnwvTex5","executionInfo":{"status":"ok","timestamp":1702106370716,"user_tz":-480,"elapsed":2,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}},"outputId":"e4532286-c027-4ca2-e4b3-9a759e66c429"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/MindEye\n"]}]},{"cell_type":"code","source":["!pip install open_clip_torch dalle2-pytorch wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LDFxHLjZgRiJ","executionInfo":{"status":"ok","timestamp":1702106377714,"user_tz":-480,"elapsed":7000,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}},"outputId":"55cefe9c-da82-420e-9e9e-d30164129ff7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.10/dist-packages (2.23.0)\n","Requirement already satisfied: dalle2-pytorch in /usr/local/lib/python3.10/dist-packages (1.15.6)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.1)\n","Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.16.0+cu118)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2023.6.3)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (6.1.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (4.66.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.19.4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.1.99)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (3.20.3)\n","Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.9.12)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (0.25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (8.1.7)\n","Requirement already satisfied: clip-anytorch>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (2.5.2)\n","Requirement already satisfied: coca-pytorch>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (0.1.0)\n","Requirement already satisfied: ema-pytorch>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (0.3.1)\n","Requirement already satisfied: einops>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (0.7.0)\n","Requirement already satisfied: embedding-reader in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (1.5.1)\n","Requirement already satisfied: kornia>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (0.7.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (23.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (9.4.0)\n","Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (2.5.2)\n","Requirement already satisfied: pytorch-warmup in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (0.1.1)\n","Requirement already satisfied: resize-right>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (0.0.2)\n","Requirement already satisfied: rotary-embedding-torch in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (0.4.0)\n","Requirement already satisfied: vector-quantize-pytorch in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (1.12.0)\n","Requirement already satisfied: x-clip>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (0.14.4)\n","Requirement already satisfied: webdataset>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (0.2.86)\n","Requirement already satisfied: fsspec>=2022.1.0 in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (2023.6.0)\n","Requirement already satisfied: torchmetrics[image]>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (1.2.1)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.38.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: beartype in /usr/local/lib/python3.10/dist-packages (from ema-pytorch>=0.0.7->dalle2-pytorch) (0.16.4)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->dalle2-pytorch) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->dalle2-pytorch) (2.14.5)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->dalle2-pytorch) (4.8.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (2.1.0)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[image]>=0.8.0->dalle2-pytorch) (0.10.0)\n","Requirement already satisfied: torch-fidelity<=0.4.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[image]>=0.8.0->dalle2-pytorch) (0.3.0)\n","Requirement already satisfied: scipy>1.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[image]>=0.8.0->dalle2-pytorch) (1.11.4)\n","Requirement already satisfied: braceexpand in /usr/local/lib/python3.10/dist-packages (from webdataset>=0.2.5->dalle2-pytorch) (0.1.7)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->dalle2-pytorch) (0.4.1)\n","Requirement already satisfied: pandas<2,>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from embedding-reader->dalle2-pytorch) (1.5.3)\n","Requirement already satisfied: pyarrow<13,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from embedding-reader->dalle2-pytorch) (9.0.0)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->open_clip_torch) (0.2.12)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.1.5->embedding-reader->dalle2-pytorch) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.1.5->embedding-reader->dalle2-pytorch) (2023.3.post1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import csv\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from functools import partial\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from dalle2_pytorch import DiffusionPrior\n","from PIL import Image\n","from dalle2_pytorch.dalle2_pytorch import l2norm, default, exists\n","from dalle2_pytorch.dalle2_pytorch import RotaryEmbedding, CausalTransformer, SinusoidalPosEmb, MLP, Rearrange, repeat, rearrange, prob_mask_like, LayerNorm, RelPosBias, Attention, FeedForward\n","from torchvision import transforms\n","from utils import load_image, save_image, encode_img, decode_img, to_PIL"],"metadata":{"id":"M1WWkMK1g3qg","executionInfo":{"status":"ok","timestamp":1702106386777,"user_tz":-480,"elapsed":9066,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import wandb\n","!wandb login 72af0b7211cd6c1e899bf475205c6a9df94d43d3\n","wandb.login()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-2g8fXMSdV4","executionInfo":{"status":"ok","timestamp":1702106395100,"user_tz":-480,"elapsed":8329,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}},"outputId":"8ac936d0-ffdd-4dbc-abc4-5c477fe7e177"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mst1020575\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Ixzc60AkdPIZ","executionInfo":{"status":"ok","timestamp":1702106395100,"user_tz":-480,"elapsed":6,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"outputs":[],"source":["dataset_path = '../2023-Machine-Learning-Dataset/'\n","training_path = dataset_path + 'subj0{}/training_split/'\n","training_fmri_path = training_path + 'training_fmri/'\n","training_images_path = training_path + 'training_images/'\n","testing_path = dataset_path + 'subj0{}/test_split/test_fmri/'"]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","  def __init__(self, fmri_data, images_folder, transform=None):\n","    self.fmri_data = fmri_data\n","    self.images_folder = images_folder\n","    self.image_paths = [f\"{images_folder}/{filename}\" for filename in os.listdir(images_folder)]\n","    self.transform = transform\n","\n","  def __len__(self):\n","    return len(self.fmri_data)\n","\n","  def __getitem__(self, idx):\n","    fmri = self.fmri_data[idx]\n","    image_path = self.image_paths[idx]\n","    image = load_image(image_path)\n","\n","    if(self.transform):\n","      image = self.transform(image)\n","\n","    return fmri, image"],"metadata":{"id":"ZelWK8SCT2_P","executionInfo":{"status":"ok","timestamp":1702106395100,"user_tz":-480,"elapsed":5,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Resize([512, 512])\n","\n","# Load dataset, now only subj01\n","lh = np.load(training_path.format(1) + 'training_fmri/lh_training_fmri.npy')\n","rh = np.load(training_path.format(1) + 'training_fmri/rh_training_fmri.npy')\n","lrh = np.concatenate((lh, rh), axis=1)\n","\n","my_dataset = MyDataset(lrh, training_images_path.format(1), transform=transform)"],"metadata":{"id":"yQyS1n8kT74V","executionInfo":{"status":"ok","timestamp":1702106401227,"user_tz":-480,"elapsed":6131,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"FN9yYqRndPIZ","executionInfo":{"status":"ok","timestamp":1702106403054,"user_tz":-480,"elapsed":1830,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"outputs":[],"source":["# pip install open_clip_torch\n","\n","import open_clip\n","from PIL import Image\n","from torchvision import transforms\n","\n","# model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion2b_s32b_b82k')"]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KcEtvLDEhMkU","executionInfo":{"status":"ok","timestamp":1702106403054,"user_tz":-480,"elapsed":8,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}},"outputId":"73fb6398-e24f-4adf-faa8-22315469ffcd"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["class Clipper(torch.nn.Module):\n","    def __init__(self, clip_variant='ViT-H-14', hidden_state=False, norm_embs=False, device=torch.device('cpu')):\n","        super().__init__()\n","        print(clip_variant, device)\n","        clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion2b_s32b_b82k')\n","\n","        clip_model.eval() # dont want to train model\n","        for param in clip_model.parameters():\n","            param.requires_grad = False # dont need to calculate gradients\n","\n","        # overwrite preprocess to accept torch inputs instead of PIL Image\n","        preprocess = transforms.Compose([\n","                transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC, antialias=None),\n","                transforms.CenterCrop(224),\n","                transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n","        ])\n","\n","        self.clip = clip_model\n","        self.preprocess = preprocess\n","        self.device = device\n","        self.norm_embs = norm_embs\n","        self.clamp_embs=False\n","        self.clip_size = (425,425)\n","        self.preproc = transforms.Compose([\n","                transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC, antialias=None),\n","                transforms.CenterCrop(224),\n","                transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n","        ])\n","        if hidden_state:\n","            print(\"THIS IS NOT WORKING CURRENTLY!\")\n","            clip_model.visual.transformer.resblocks[31].mlp = nn.Identity()\n","            clip_model.visual.ln_post = nn.Identity()\n","            clip_model.token_embedding = nn.Identity()\n","            clip_model.ln_final = nn.Identity()\n","\n","    def embed_image(self, image):\n","        \"\"\"Expects images in -1 to 1 range\"\"\"\n","        clip_emb = self.preprocess(image.to(self.device)).unsqueeze(0)\n","        clip_emb = self.clip.encode_image(clip_emb)\n","        if self.norm_embs:\n","            clip_emb = nn.functional.normalize(clip_emb.flatten(1), dim=-1)\n","            clip_emb = clip_emb.reshape(len(clip_emb),-1,1024)\n","        return clip_emb\n","    def embed_text(self, text_samples):\n","        clip_text = clip.tokenize(text_samples).to(self.device)\n","        clip_text = self.clip.encode_text(clip_text)\n","        if self.clamp_embs:\n","            clip_text = torch.clamp(clip_text, -1.5, 1.5)\n","        if self.norm_embs:\n","            clip_text = nn.functional.normalize(clip_text, dim=-1)\n","        return clip_text"],"metadata":{"id":"gQERIkyTezBi","executionInfo":{"status":"ok","timestamp":1702106403055,"user_tz":-480,"elapsed":7,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class BrainDiffusionPrior(DiffusionPrior):\n","    \"\"\"\n","    Differences from original:\n","    - Allow for passing of generators to torch random functions\n","    - Option to include the voxel2clip model and pass voxels into forward method\n","    - Return predictions when computing loss\n","    - Load pretrained model from @nousr trained on LAION aesthetics\n","    \"\"\"\n","    def __init__(self, *args, **kwargs):\n","        voxel2clip = kwargs.pop('voxel2clip', None)\n","        super().__init__(*args, **kwargs)\n","        self.voxel2clip = voxel2clip\n","\n","    @torch.no_grad()\n","    def p_sample(self, x, t, text_cond = None, self_cond = None, clip_denoised = True, cond_scale = 1.,\n","                generator=None):\n","        b, *_, device = *x.shape, x.device\n","        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = t, text_cond = text_cond, self_cond = self_cond, clip_denoised = clip_denoised, cond_scale = cond_scale)\n","        if generator is None:\n","            noise = torch.randn_like(x)\n","        else:\n","            #noise = torch.randn_like(x)\n","            noise = torch.randn(x.size(), device=x.device, dtype=x.dtype, generator=generator)\n","        # no noise when t == 0\n","        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n","        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n","        return pred, x_start\n","\n","    @torch.no_grad()\n","    def p_sample_loop_ddpm(self, shape, text_cond, cond_scale = 1., generator=None):\n","        batch, device = shape[0], self.device\n","\n","        if generator is None:\n","            image_embed = torch.randn(shape, device = device)\n","        else:\n","            image_embed = torch.randn(shape, device = device, generator=generator)\n","        x_start = None # for self-conditioning\n","\n","        if self.init_image_embed_l2norm:\n","            image_embed = l2norm(image_embed) * self.image_embed_scale\n","\n","        for i in tqdm(reversed(range(0, self.noise_scheduler.num_timesteps)), desc='sampling loop time step', total=self.noise_scheduler.num_timesteps, disable=True):\n","            times = torch.full((batch,), i, device = device, dtype = torch.long)\n","\n","            self_cond = x_start if self.net.self_cond else None\n","            image_embed, x_start = self.p_sample(image_embed, times, text_cond = text_cond, self_cond = self_cond, cond_scale = cond_scale,\n","                                                 generator=generator)\n","\n","        if self.sampling_final_clamp_l2norm and self.predict_x_start:\n","            image_embed = self.l2norm_clamp_embed(image_embed)\n","\n","        return image_embed\n","\n","    def p_losses(self, image_embed, times, text_cond, noise = None):\n","        noise = default(noise, lambda: torch.randn_like(image_embed))\n","\n","        image_embed_noisy = self.noise_scheduler.q_sample(x_start = image_embed, t = times, noise = noise)\n","\n","        self_cond = None\n","        if self.net.self_cond and random.random() < 0.5:\n","            with torch.no_grad():\n","                self_cond = self.net(image_embed_noisy, times, **text_cond).detach()\n","\n","        pred = self.net(\n","            image_embed_noisy,\n","            times,\n","            self_cond = self_cond,\n","            text_cond_drop_prob = self.text_cond_drop_prob,\n","            image_cond_drop_prob = self.image_cond_drop_prob,\n","            **text_cond\n","        )\n","\n","        if self.predict_x_start and self.training_clamp_l2norm:\n","            pred = self.l2norm_clamp_embed(pred)\n","\n","        if self.predict_v:\n","            target = self.noise_scheduler.calculate_v(image_embed, times, noise)\n","        elif self.predict_x_start:\n","            target = image_embed\n","        else:\n","            target = noise\n","\n","        loss = self.noise_scheduler.loss_fn(pred, target)\n","        return loss, pred\n","\n","    def forward(\n","        self,\n","        text = None,\n","        image = None,\n","        voxel = None,\n","        text_embed = None,      # allow for training on preprocessed CLIP text and image embeddings\n","        image_embed = None,\n","        text_encodings = None,  # as well as CLIP text encodings\n","        *args,\n","        **kwargs\n","    ):\n","        assert exists(text) ^ exists(text_embed) ^ exists(voxel), 'either text, text embedding, or voxel must be supplied'\n","        assert exists(image) ^ exists(image_embed), 'either image or image embedding must be supplied'\n","        assert not (self.condition_on_text_encodings and (not exists(text_encodings) and not exists(text))), 'text encodings must be present if you specified you wish to condition on it on initialization'\n","\n","        if exists(voxel):\n","            assert exists(self.voxel2clip), 'voxel2clip must be trained if you wish to pass in voxels'\n","            assert not exists(text_embed), 'cannot pass in both text and voxels'\n","            if self.voxel2clip.use_projector:\n","                clip_voxels_mse, clip_voxels = self.voxel2clip(voxel)\n","                text_embed = clip_voxels_mse\n","            else:\n","                clip_voxels = self.voxel2clip(voxel)\n","                text_embed = clip_voxels_mse = clip_voxels\n","            # text_embed = self.voxel2clip(voxel)\n","\n","        if exists(image):\n","            image_embed, _ = self.clip.embed_image(image)\n","\n","        # calculate text conditionings, based on what is passed in\n","\n","        if exists(text):\n","            text_embed, text_encodings = self.clip.embed_text(text)\n","\n","        text_cond = dict(text_embed = text_embed)\n","\n","        if self.condition_on_text_encodings:\n","            assert exists(text_encodings), 'text encodings must be present for diffusion prior if specified'\n","            text_cond = {**text_cond, 'text_encodings': text_encodings}\n","\n","        # timestep conditioning from ddpm\n","\n","        batch, device = image_embed.shape[0], image_embed.device\n","        times = self.noise_scheduler.sample_random_times(batch)\n","\n","        # PS: I dont think we need this? also if uncommented this does in-place global variable change\n","        # scale image embed (Katherine)\n","        # image_embed *= self.image_embed_scale\n","\n","        # calculate forward loss\n","\n","        loss, pred = self.p_losses(image_embed*self.image_embed_scale, times, text_cond = text_cond, *args, **kwargs)\n","\n","        # undo the scaling so we can directly use it for real mse loss and reconstruction\n","        return loss, pred"],"metadata":{"id":"VyfBRLe9maJV","executionInfo":{"status":"ok","timestamp":1702106403055,"user_tz":-480,"elapsed":7,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class DiffusionPriorNetwork(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        num_timesteps = None,\n","        num_time_embeds = 1,\n","        # num_image_embeds = 1,\n","        # num_brain_embeds = 1,\n","        num_tokens = 257,\n","        causal = True,\n","        learned_query_mode = 'none',\n","        **kwargs\n","    ):\n","        super().__init__()\n","        self.dim = dim\n","        self.num_time_embeds = num_time_embeds\n","        self.continuous_embedded_time = not exists(num_timesteps)\n","        self.learned_query_mode = learned_query_mode\n","\n","        self.to_time_embeds = nn.Sequential(\n","            nn.Embedding(num_timesteps, dim * num_time_embeds) if exists(num_timesteps) else nn.Sequential(SinusoidalPosEmb(dim), MLP(dim, dim * num_time_embeds)), # also offer a continuous version of timestep embeddings, with a 2 layer MLP\n","            Rearrange('b (n d) -> b n d', n = num_time_embeds)\n","        )\n","\n","        if self.learned_query_mode == 'token':\n","            self.learned_query = nn.Parameter(torch.randn(num_tokens, dim))\n","        if self.learned_query_mode == 'pos_emb':\n","            scale = dim ** -0.5\n","            self.learned_query = nn.Parameter(torch.randn(num_tokens, dim) * scale)\n","        if self.learned_query_mode == 'all_pos_emb':\n","            scale = dim ** -0.5\n","            self.learned_query = nn.Parameter(torch.randn(num_tokens*2+1, dim) * scale)\n","        self.causal_transformer = FlaggedCausalTransformer(dim = dim, causal=causal, **kwargs)\n","\n","        self.null_brain_embeds = nn.Parameter(torch.randn(num_tokens, dim))\n","        self.null_image_embed = nn.Parameter(torch.randn(num_tokens, dim))\n","\n","        self.num_tokens = num_tokens\n","        self.self_cond = False\n","\n","    def forward_with_cond_scale(\n","        self,\n","        *args,\n","        cond_scale = 1.,\n","        **kwargs\n","    ):\n","        logits = self.forward(*args, **kwargs)\n","\n","        if cond_scale == 1:\n","            return logits\n","\n","        null_logits = self.forward(*args, brain_cond_drop_prob = 1., image_cond_drop_prob = 1, **kwargs)\n","        return null_logits + (logits - null_logits) * cond_scale\n","\n","    def forward(\n","        self,\n","        image_embed,\n","        diffusion_timesteps,\n","        *,\n","        self_cond=None,\n","        brain_embed=None,\n","        text_embed=None,\n","        brain_cond_drop_prob = 0.,\n","        text_cond_drop_prob = None,\n","        image_cond_drop_prob = 0.\n","    ):\n","        if text_embed is not None:\n","            brain_embed = text_embed\n","        if text_cond_drop_prob is not None:\n","            brain_cond_drop_prob = text_cond_drop_prob\n","\n","        image_embed = image_embed.view(len(image_embed),-1,768)\n","        # text_embed = text_embed.view(len(text_embed),-1,768)\n","        brain_embed = brain_embed.view(len(brain_embed),-1,768)\n","        # print(*image_embed.shape)\n","        # print(*image_embed.shape, image_embed.device, image_embed.dtype)\n","\n","        batch, _, dim, device, dtype = *image_embed.shape, image_embed.device, image_embed.dtype\n","        # num_time_embeds, num_image_embeds, num_brain_embeds = self.num_time_embeds, self.num_image_embeds, self.num_brain_embeds\n","\n","        # classifier free guidance masks\n","        brain_keep_mask = prob_mask_like((batch,), 1 - brain_cond_drop_prob, device = device)\n","        brain_keep_mask = rearrange(brain_keep_mask, 'b -> b 1 1')\n","\n","        image_keep_mask = prob_mask_like((batch,), 1 - image_cond_drop_prob, device = device)\n","        image_keep_mask = rearrange(image_keep_mask, 'b -> b 1 1')\n","\n","        # mask out brain embeddings with null brain embeddings\n","\n","        # import pdb; pdb.set_trace()\n","        null_brain_embeds = self.null_brain_embeds.to(brain_embed.dtype)\n","        brain_embed = torch.where(\n","            brain_keep_mask,\n","            brain_embed,\n","            null_brain_embeds[None]\n","        )\n","\n","        # mask out image embeddings with null image embeddings\n","        null_image_embed = self.null_image_embed.to(image_embed.dtype)\n","        image_embed = torch.where(\n","            image_keep_mask,\n","            image_embed,\n","            null_image_embed[None]\n","        )\n","\n","        # whether brain embedding is used for conditioning depends on whether brain encodings are available for attention (for classifier free guidance, even though it seems from the paper it was not used in the prior ddpm, as the objective is different)\n","        # but let's just do it right\n","        if self.continuous_embedded_time:\n","            # if continuous cast to flat, else keep int for indexing embeddings\n","            diffusion_timesteps = diffusion_timesteps.type(dtype)\n","        time_embed = self.to_time_embeds(diffusion_timesteps)\n","\n","        if self.learned_query_mode == 'token':\n","            learned_queries = repeat(self.learned_query, 'n d -> b n d', b = batch)\n","        elif self.learned_query_mode == 'pos_emb':\n","            pos_embs = repeat(self.learned_query, 'n d -> b n d', b = batch)\n","            image_embed = image_embed + pos_embs\n","            learned_queries = torch.empty((batch, 0, dim), device=brain_embed.device)\n","        elif self.learned_query_mode == 'all_pos_emb':\n","            pos_embs = repeat(self.learned_query, 'n d -> b n d', b = batch)\n","            learned_queries = torch.empty((batch, 0, dim), device=brain_embed.device)\n","        else:\n","            learned_queries = torch.empty((batch, 0, dim), device=brain_embed.device)\n","\n","        tokens = torch.cat((\n","            brain_embed,  # 257\n","            time_embed,  # 1\n","            image_embed,  # 257\n","            learned_queries  # 257\n","        ), dim = -2)\n","        if self.learned_query_mode == 'all_pos_emb':\n","            tokens = tokens + pos_embs\n","\n","        # attend\n","        tokens = self.causal_transformer(tokens)\n","\n","        # get learned query, which should predict the image embedding (per DDPM timestep)\n","        pred_image_embed = tokens[..., -self.num_tokens:, :]\n","\n","        return pred_image_embed"],"metadata":{"id":"ACKoyS3xprx-","executionInfo":{"status":"ok","timestamp":1702106403055,"user_tz":-480,"elapsed":6,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class BrainNetwork(nn.Module):\n","    def __init__(self, out_dim=768, in_dim=39548, clip_size=768, h=2048, n_blocks=4, norm_type='ln', act_first=False, use_projector=True):\n","        super().__init__()\n","        norm_func = partial(nn.BatchNorm1d, num_features=h) if norm_type == 'bn' else partial(nn.LayerNorm, normalized_shape=h)\n","        act_fn = partial(nn.ReLU, inplace=True) if norm_type == 'bn' else nn.GELU\n","        act_and_norm = (act_fn, norm_func) if act_first else (norm_func, act_fn)\n","        # self.temp = nn.Parameter(torch.tensor(.006))\n","        self.lin0 = nn.Sequential(\n","            nn.Linear(in_dim, h),\n","            *[item() for item in act_and_norm],\n","            nn.Dropout(0.5),\n","        )\n","        self.mlp = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(h, h),\n","                *[item() for item in act_and_norm],\n","                nn.Dropout(0.15)\n","            ) for _ in range(n_blocks)\n","        ])\n","        self.lin1 = nn.Linear(h, out_dim, bias=True)\n","        self.n_blocks = n_blocks\n","        self.clip_size = clip_size\n","\n","        self.use_projector = use_projector\n","        if use_projector:\n","            self.projector = nn.Sequential(\n","                nn.LayerNorm(clip_size),\n","                nn.GELU(),\n","                nn.Linear(clip_size, 2048),\n","                nn.LayerNorm(2048),\n","                nn.GELU(),\n","                nn.Linear(2048, 2048),\n","                nn.LayerNorm(2048),\n","                nn.GELU(),\n","                nn.Linear(2048, clip_size)\n","            )\n","\n","    def forward(self, x):\n","        '''\n","            bs, 1, 15724 -> bs, 32, h\n","            bs, 32, h -> bs, 32h\n","            b2, 32h -> bs, 768\n","        '''\n","        if x.ndim == 4:\n","            # case when we passed 3D data of shape [N, 81, 104, 83]\n","            assert x.shape[1] == 81 and x.shape[2] == 104 and x.shape[3] == 83\n","            # [N, 699192]\n","            x = x.reshape(x.shape[0], -1)\n","\n","        x = self.lin0(x)  # bs, h\n","        residual = x\n","        for res_block in range(self.n_blocks):\n","            x = self.mlp[res_block](x)\n","            x += residual\n","            residual = x\n","        x = x.reshape(len(x), -1)\n","        x = self.lin1(x)\n","        if self.use_projector:\n","            return x, self.projector(x.reshape(len(x), -1, self.clip_size))\n","        return x"],"metadata":{"id":"-ZWMi77dqXQa","executionInfo":{"status":"ok","timestamp":1702106403055,"user_tz":-480,"elapsed":6,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class FlaggedCausalTransformer(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        dim,\n","        depth,\n","        dim_head = 64,\n","        heads = 8,\n","        ff_mult = 4,\n","        norm_in = False,\n","        norm_out = True,\n","        attn_dropout = 0.,\n","        ff_dropout = 0.,\n","        final_proj = True,\n","        normformer = False,\n","        rotary_emb = True,\n","        causal=True\n","    ):\n","        super().__init__()\n","        self.init_norm = LayerNorm(dim) if norm_in else nn.Identity() # from latest BLOOM model and Yandex's YaLM\n","\n","        self.rel_pos_bias = RelPosBias(heads = heads)\n","\n","        rotary_emb = RotaryEmbedding(dim = min(32, dim_head)) if rotary_emb else None\n","\n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                Attention(dim = dim, causal = causal, dim_head = dim_head, heads = heads, dropout = attn_dropout, rotary_emb = rotary_emb),\n","                FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout, post_activation_norm = normformer)\n","            ]))\n","\n","        self.norm = LayerNorm(dim, stable = True) if norm_out else nn.Identity()  # unclear in paper whether they projected after the classic layer norm for the final denoised image embedding, or just had the transformer output it directly: plan on offering both options\n","        self.project_out = nn.Linear(dim, dim, bias = False) if final_proj else nn.Identity()\n","\n","    def forward(self, x):\n","        n, device = x.shape[1], x.device\n","\n","        x = self.init_norm(x)\n","\n","        attn_bias = self.rel_pos_bias(n, n + 1, device = device)\n","\n","        for attn, ff in self.layers:\n","            x = attn(x, attn_bias = attn_bias) + x\n","            x = ff(x) + x\n","\n","        out = self.norm(x)\n","        return self.project_out(out)\n"],"metadata":{"id":"pDXRPZjzrq4_","executionInfo":{"status":"ok","timestamp":1702106403055,"user_tz":-480,"elapsed":6,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["clip_extractor = Clipper(device=torch.device(device)).to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oo-qgG73hgO-","executionInfo":{"status":"ok","timestamp":1702106432565,"user_tz":-480,"elapsed":29516,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}},"outputId":"5bda9a9e-1449-4397-a308-a7f8b94295d1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["ViT-H-14 cuda\n"]}]},{"cell_type":"code","source":["clip_size = 768\n","out_dim = clip_size #257*clip_size\n","depth = 6\n","dim_head = 64\n","timesteps = 100\n","heads = clip_size//64\n","num_epochs = 120\n","epoch = 0\n","losses = []\n","val_losses = []\n","lrs = []"],"metadata":{"id":"9AgsYED0pM3R","executionInfo":{"status":"ok","timestamp":1702106432566,"user_tz":-480,"elapsed":3,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["batch_size = 16\n","num_epochs = 120\n","num_train = 5000\n","lr_scheduler = 'cycle'\n","initial_lr = 1e-3\n","max_lr = 5e-4\n","random_seed = 42\n","train_size = 0.8\n","valid_size = 1 - train_size\n","num_workers = torch.cuda.device_count() if torch.cuda.device_count()>0 else 1\n","prior_mult = .03"],"metadata":{"id":"-HZFSq79SxUA","executionInfo":{"status":"ok","timestamp":1702106432566,"user_tz":-480,"elapsed":2,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["voxel2clip_kwargs = dict(out_dim=out_dim,clip_size=clip_size,use_projector=True)\n","voxel2clip = BrainNetwork(**voxel2clip_kwargs).to(device)"],"metadata":{"id":"VDcRllLxqnZg","executionInfo":{"status":"ok","timestamp":1702106433928,"user_tz":-480,"elapsed":1364,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["prior_network = DiffusionPriorNetwork(\n","            dim=out_dim,\n","            depth=depth,\n","            dim_head=dim_head,\n","            heads=heads,\n","            causal=False,\n","            num_tokens = 257,\n","            learned_query_mode=\"pos_emb\"\n","        ).to(device)\n","diffusion_prior = BrainDiffusionPrior(\n","        net=prior_network,\n","        image_embed_dim=out_dim,\n","        condition_on_text_encodings=False,\n","        timesteps=timesteps,\n","        cond_drop_prob=0.2,\n","        image_embed_scale=None,\n","        voxel2clip=voxel2clip,\n","    ).to(device)"],"metadata":{"id":"IimikxtFoyeM","executionInfo":{"status":"ok","timestamp":1702106433929,"user_tz":-480,"elapsed":6,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"clipTrain\",\n","\n","    # track hyperparameters and run metadata\n","    config={\n","        \"learning_rate\": initial_lr,\n","        \"architecture\": \"MLP\",\n","        \"dataset\": \"NSD\",\n","        \"epochs\": num_epochs,\n","        \"random_seed\": random_seed,\n","        \"train_size\": train_size,\n","        \"valid_size\": valid_size\n","    }\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"mZHrE1trSrRQ","executionInfo":{"status":"ok","timestamp":1702106437785,"user_tz":-480,"elapsed":3862,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}},"outputId":"7711d180-2b25-41a9-a342-061cbd099a37"},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/My Drive/MindEye/wandb/run-20231209_072030-ozxgm605</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/st1020575/clipTrain/runs/ozxgm605' target=\"_blank\">zesty-elevator-5</a></strong> to <a href='https://wandb.ai/st1020575/clipTrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/st1020575/clipTrain' target=\"_blank\">https://wandb.ai/st1020575/clipTrain</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/st1020575/clipTrain/runs/ozxgm605' target=\"_blank\">https://wandb.ai/st1020575/clipTrain/runs/ozxgm605</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/st1020575/clipTrain/runs/ozxgm605?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7d18786677c0>"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","opt_grouped_parameters = [\n","    {'params': [p for n, p in diffusion_prior.net.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n","    {'params': [p for n, p in diffusion_prior.net.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","    {'params': [p for n, p in diffusion_prior.voxel2clip.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n","    {'params': [p for n, p in diffusion_prior.voxel2clip.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]"],"metadata":{"id":"uk0yXOfZTNt7","executionInfo":{"status":"ok","timestamp":1702106437785,"user_tz":-480,"elapsed":3,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=initial_lr)\n","lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr,\n","                          total_steps=num_epochs*((num_train//batch_size)//num_workers),\n","                          final_div_factor=1000,\n","                          last_epoch=-1, pct_start=2/num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nGfMp6oKSDOE","executionInfo":{"status":"ok","timestamp":1702106437785,"user_tz":-480,"elapsed":3,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}},"outputId":"a2479733-b0fb-492e-94b0-f13f5245c555"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|                                                                                                                         | 0/120 [00:00<?, ?it/s]"]}]},{"cell_type":"code","source":["# train-val split\n","generator = torch.Generator().manual_seed(random_seed)\n","trainset, validset = random_split(my_dataset, [train_size, valid_size], generator=generator)"],"metadata":{"id":"YrhBl4y0Ylm1","executionInfo":{"status":"ok","timestamp":1702106437785,"user_tz":-480,"elapsed":2,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# build dataloader\n","train_dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","val_dataloader = DataLoader(validset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"],"metadata":{"id":"ZxwDAON5Yd1E","executionInfo":{"status":"ok","timestamp":1702106437785,"user_tz":-480,"elapsed":2,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# checkpoint = torch.load(\"./ModelsClip/31\", map_location=device)\n","# epoch = checkpoint['epoch']\n","# loss = checkpoint['loss']\n","# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","# diffusion_prior.load_state_dict(checkpoint['model_state_dict'])\n","# del checkpoint"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4aqBSzeMcV4B","executionInfo":{"status":"ok","timestamp":1702106606719,"user_tz":-480,"elapsed":10571,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}},"outputId":"48fc8da1-0b10-477e-f17f-b8a312e1e275"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["progress_bar = tqdm(range(epoch, num_epochs), ncols=150)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q248z-pnduRC","executionInfo":{"status":"ok","timestamp":1702106697232,"user_tz":-480,"elapsed":771,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"}},"outputId":"e094aa49-e798-496c-bffc-cb93b6658107"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","  0%|                                                                                                                         | 0/120 [04:19<?, ?it/s]\n"]}]},{"cell_type":"code","source":["for epoch in progress_bar:\n","    diffusion_prior.train()\n","\n","\n","    loss_sum = 0\n","    val_loss_sum = 0\n","\n","    reconst_fails = []\n","    for train_i, data in enumerate(train_dataloader):\n","        voxels, images = data\n","        optimizer.zero_grad()\n","        voxels = voxels.to(device).float()\n","        images = images.to(device).float()\n","\n","\n","        encoded_latents = torch.cat([clip_extractor.embed_image(torch.squeeze(image,0)).to(device) for image in images])\n","        clip_voxels, clip_voxels_proj = diffusion_prior.voxel2clip(voxels)\n","\n","        # calulate loss\n","        loss = F.mse_loss(clip_voxels, encoded_latents)\n","        loss_sum += loss.item()\n","        losses.append(loss.item())\n","        lrs.append(optimizer.param_groups[0]['lr'])\n","\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","\n","        logs = {\n","            \"train/loss\": np.mean(losses[-(train_i+1):]),\n","            \"train/lr\": lrs[-1],\n","            \"train/num_steps\": len(losses),\n","            \"train/loss_mse\": loss_sum / (train_i + 1)\n","        }\n","        wandb.log(logs)\n","\n","        progress_bar.set_postfix(**logs)\n","    torch.save({\n","      'epoch': epoch,\n","      'model_state_dict': diffusion_prior.state_dict(),\n","      'optimizer_state_dict': optimizer.state_dict(),\n","      'loss': loss,\n","      }, './ModelsClip/{}'.format(epoch)\n","    )\n","    diffusion_prior.eval()\n","    for val_i, data in enumerate(val_dataloader):\n","        voxels, images = data\n","        voxels = voxels.to(device).float()\n","        images = images.to(device).float()\n","\n","        encoded_latents = torch.cat([clip_extractor.embed_image(torch.squeeze(image,0)).to(device) for image in images])\n","        clip_voxels, clip_voxels_proj = diffusion_prior.voxel2clip(voxels)\n","        # calulate loss\n","        loss = F.mse_loss(clip_voxels, encoded_latents)\n","        val_loss_sum += loss.item()\n","        val_losses.append(loss.item())\n","\n","    logs = {\n","        \"train/loss\": np.mean(losses[-(train_i+1):]),\n","        \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n","        \"train/lr\": lrs[-1],\n","        \"train/num_steps\": len(losses),\n","        \"train/loss_mse\": loss_sum / (train_i + 1),\n","        \"val/loss_mse\": val_loss_sum / (val_i + 1)\n","    }\n","    wandb.log(logs)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HeWi2uQrtV0G","outputId":"06ee86c2-945a-465b-eda1-11d23e7fbac5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\n"," 63%|███████████████         | 56/89 [6:12:49<3:38:36, 397.48s/it, train/loss=0.0655, train/loss_mse=0.0655, train/lr=0.000351, train/num_steps=14150]\u001b[A\n"," 63%|███████████████         | 56/89 [6:12:50<3:38:36, 397.48s/it, train/loss=0.0655, train/loss_mse=0.0655, train/lr=0.000351, train/num_steps=14151]\u001b[A\n"," 63%|███████████████         | 56/89 [6:12:51<3:38:36, 397.48s/it, train/loss=0.0654, train/loss_mse=0.0654, train/lr=0.000351, train/num_steps=14152]\u001b[A\n"," 63%|███████████████         | 56/89 [6:12:52<3:38:36, 397.48s/it, train/loss=0.0654, train/loss_mse=0.0654, train/lr=0.000351, train/num_steps=14153]\u001b[A\n"," 63%|███████████████         | 56/89 [6:12:53<3:38:36, 397.48s/it, train/loss=0.0654, train/loss_mse=0.0654, train/lr=0.000351, train/num_steps=14154]\u001b[A\n"," 63%|███████████████         | 56/89 [6:12:55<3:38:36, 397.48s/it, train/loss=0.0654, train/loss_mse=0.0654, train/lr=0.000351, train/num_steps=14155]\u001b[A\n"," 63%|███████████████         | 56/89 [6:12:56<3:38:36, 397.48s/it, train/loss=0.0655, train/loss_mse=0.0655, train/lr=0.000351, train/num_steps=14156]\u001b[A\n"," 63%|███████████████         | 56/89 [6:12:57<3:38:36, 397.48s/it, train/loss=0.0655, train/loss_mse=0.0655, train/lr=0.000351, train/num_steps=14157]\u001b[A\n"," 63%|███████████████         | 56/89 [6:12:58<3:38:36, 397.48s/it, train/loss=0.0654, train/loss_mse=0.0654, train/lr=0.000351, train/num_steps=14158]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:00<3:38:36, 397.48s/it, train/loss=0.0654, train/loss_mse=0.0654, train/lr=0.000351, train/num_steps=14159]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:01<3:38:36, 397.48s/it, train/loss=0.0654, train/loss_mse=0.0654, train/lr=0.000351, train/num_steps=14160]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:02<3:38:36, 397.48s/it, train/loss=0.0654, train/loss_mse=0.0654, train/lr=0.000351, train/num_steps=14161]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:03<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14162]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:05<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14163]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:06<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14164]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:07<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14165]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:08<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14166]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:10<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14167]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:11<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14168]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:12<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14169]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:13<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14170]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:15<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14171]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:16<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14172]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:17<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14173]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:18<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14174]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:20<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14175]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:21<3:38:36, 397.48s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000351, train/num_steps=14176]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:22<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.000351, train/num_steps=14177]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:23<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.000351, train/num_steps=14178]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:25<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.000351, train/num_steps=14179]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:26<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.000351, train/num_steps=14180]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:27<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.000351, train/num_steps=14181]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:28<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.000351, train/num_steps=14182]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:29<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.000351, train/num_steps=14183]\u001b[A\n"," 63%|███████████████         | 56/89 [6:13:31<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.000351, train/num_steps=14184]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:32<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.00035, train/num_steps=14185]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:33<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.00035, train/num_steps=14186]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:34<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.00035, train/num_steps=14187]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:36<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.00035, train/num_steps=14188]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:37<3:38:36, 397.48s/it, train/loss=0.0651, train/loss_mse=0.0651, train/lr=0.00035, train/num_steps=14189]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:38<3:38:36, 397.48s/it, train/loss=0.0651, train/loss_mse=0.0651, train/lr=0.00035, train/num_steps=14190]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:39<3:38:36, 397.48s/it, train/loss=0.0651, train/loss_mse=0.0651, train/lr=0.00035, train/num_steps=14191]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:41<3:38:36, 397.48s/it, train/loss=0.0651, train/loss_mse=0.0651, train/lr=0.00035, train/num_steps=14192]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:42<3:38:36, 397.48s/it, train/loss=0.0651, train/loss_mse=0.0651, train/lr=0.00035, train/num_steps=14193]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:43<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.00035, train/num_steps=14194]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:44<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.00035, train/num_steps=14195]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:45<3:38:36, 397.48s/it, train/loss=0.0652, train/loss_mse=0.0652, train/lr=0.00035, train/num_steps=14196]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:47<3:38:36, 397.48s/it, train/loss=0.0651, train/loss_mse=0.0651, train/lr=0.00035, train/num_steps=14197]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:48<3:38:36, 397.48s/it, train/loss=0.0651, train/loss_mse=0.0651, train/lr=0.00035, train/num_steps=14198]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:49<3:38:36, 397.48s/it, train/loss=0.0651, train/loss_mse=0.0651, train/lr=0.00035, train/num_steps=14199]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:50<3:38:36, 397.48s/it, train/loss=0.0651, train/loss_mse=0.0651, train/lr=0.00035, train/num_steps=14200]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:13:52<3:38:36, 397.48s/it, train/loss=0.0651, train/loss_mse=0.0651, train/lr=0.00035, train/num_steps=14201]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:13:53<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14202]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:13:54<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14203]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:13:55<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14204]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:13:57<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14205]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:13:58<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14206]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:13:59<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14207]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:00<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14208]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:02<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14209]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:03<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14210]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:04<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14211]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:05<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14212]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:06<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14213]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:08<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14214]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:09<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14215]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:10<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14216]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:11<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14217]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:13<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14218]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:14<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14219]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:15<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14220]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:16<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14221]\u001b[A\n"," 63%|████████████████▉          | 56/89 [6:14:18<3:38:36, 397.48s/it, train/loss=0.065, train/loss_mse=0.065, train/lr=0.00035, train/num_steps=14222]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:19<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14223]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:20<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14224]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:21<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14225]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:23<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14226]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:24<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14227]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:25<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14228]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:26<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14229]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:27<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14230]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:29<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14231]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:30<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14232]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:31<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14233]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:32<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14234]\u001b[A\n"," 63%|███████████████▋         | 56/89 [6:14:34<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.00035, train/num_steps=14235]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:35<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.000349, train/num_steps=14236]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:36<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.000349, train/num_steps=14237]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:37<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.000349, train/num_steps=14238]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:39<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.000349, train/num_steps=14239]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:40<3:38:36, 397.48s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.000349, train/num_steps=14240]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:41<3:38:36, 397.48s/it, train/loss=0.0648, train/loss_mse=0.0648, train/lr=0.000349, train/num_steps=14241]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:42<3:38:36, 397.48s/it, train/loss=0.0648, train/loss_mse=0.0648, train/lr=0.000349, train/num_steps=14242]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:44<3:38:36, 397.48s/it, train/loss=0.0648, train/loss_mse=0.0648, train/lr=0.000349, train/num_steps=14243]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:45<3:38:36, 397.48s/it, train/loss=0.0648, train/loss_mse=0.0648, train/lr=0.000349, train/num_steps=14244]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:46<3:38:36, 397.48s/it, train/loss=0.0648, train/loss_mse=0.0648, train/lr=0.000349, train/num_steps=14245]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:47<3:38:36, 397.48s/it, train/loss=0.0648, train/loss_mse=0.0648, train/lr=0.000349, train/num_steps=14246]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:49<3:38:36, 397.48s/it, train/loss=0.0648, train/loss_mse=0.0648, train/lr=0.000349, train/num_steps=14247]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:50<3:38:36, 397.48s/it, train/loss=0.0648, train/loss_mse=0.0648, train/lr=0.000349, train/num_steps=14248]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:51<3:38:36, 397.48s/it, train/loss=0.0647, train/loss_mse=0.0647, train/lr=0.000349, train/num_steps=14249]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:52<3:38:36, 397.48s/it, train/loss=0.0647, train/loss_mse=0.0647, train/lr=0.000349, train/num_steps=14250]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:54<3:38:36, 397.48s/it, train/loss=0.0647, train/loss_mse=0.0647, train/lr=0.000349, train/num_steps=14251]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:55<3:38:36, 397.48s/it, train/loss=0.0647, train/loss_mse=0.0647, train/lr=0.000349, train/num_steps=14252]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:56<3:38:36, 397.48s/it, train/loss=0.0646, train/loss_mse=0.0646, train/lr=0.000349, train/num_steps=14253]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:57<3:38:36, 397.48s/it, train/loss=0.0646, train/loss_mse=0.0646, train/lr=0.000349, train/num_steps=14254]\u001b[A\n"," 63%|███████████████         | 56/89 [6:14:59<3:38:36, 397.48s/it, train/loss=0.0646, train/loss_mse=0.0646, train/lr=0.000349, train/num_steps=14255]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:00<3:38:36, 397.48s/it, train/loss=0.0646, train/loss_mse=0.0646, train/lr=0.000349, train/num_steps=14256]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:01<3:38:36, 397.48s/it, train/loss=0.0646, train/loss_mse=0.0646, train/lr=0.000349, train/num_steps=14257]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:02<3:38:36, 397.48s/it, train/loss=0.0646, train/loss_mse=0.0646, train/lr=0.000349, train/num_steps=14258]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:03<3:38:36, 397.48s/it, train/loss=0.0646, train/loss_mse=0.0646, train/lr=0.000349, train/num_steps=14259]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:05<3:38:36, 397.48s/it, train/loss=0.0646, train/loss_mse=0.0646, train/lr=0.000349, train/num_steps=14260]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:06<3:38:36, 397.48s/it, train/loss=0.0646, train/loss_mse=0.0646, train/lr=0.000349, train/num_steps=14261]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:07<3:38:36, 397.48s/it, train/loss=0.0646, train/loss_mse=0.0646, train/lr=0.000349, train/num_steps=14262]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:08<3:38:36, 397.48s/it, train/loss=0.0646, train/loss_mse=0.0646, train/lr=0.000349, train/num_steps=14263]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:10<3:38:36, 397.48s/it, train/loss=0.0646, train/loss_mse=0.0646, train/lr=0.000349, train/num_steps=14264]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:11<3:38:36, 397.48s/it, train/loss=0.0645, train/loss_mse=0.0645, train/lr=0.000349, train/num_steps=14265]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:12<3:38:36, 397.48s/it, train/loss=0.0645, train/loss_mse=0.0645, train/lr=0.000349, train/num_steps=14266]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:13<3:38:36, 397.48s/it, train/loss=0.0645, train/loss_mse=0.0645, train/lr=0.000349, train/num_steps=14267]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:15<3:38:36, 397.48s/it, train/loss=0.0645, train/loss_mse=0.0645, train/lr=0.000349, train/num_steps=14268]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:16<3:38:36, 397.48s/it, train/loss=0.0645, train/loss_mse=0.0645, train/lr=0.000349, train/num_steps=14269]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:17<3:38:36, 397.48s/it, train/loss=0.0645, train/loss_mse=0.0645, train/lr=0.000349, train/num_steps=14270]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:18<3:38:36, 397.48s/it, train/loss=0.0645, train/loss_mse=0.0645, train/lr=0.000349, train/num_steps=14271]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:20<3:38:36, 397.48s/it, train/loss=0.0645, train/loss_mse=0.0645, train/lr=0.000349, train/num_steps=14272]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:21<3:38:36, 397.48s/it, train/loss=0.0645, train/loss_mse=0.0645, train/lr=0.000349, train/num_steps=14273]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:22<3:38:36, 397.48s/it, train/loss=0.0645, train/loss_mse=0.0645, train/lr=0.000349, train/num_steps=14274]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:23<3:38:36, 397.48s/it, train/loss=0.0645, train/loss_mse=0.0645, train/lr=0.000349, train/num_steps=14275]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:25<3:38:36, 397.48s/it, train/loss=0.0644, train/loss_mse=0.0644, train/lr=0.000349, train/num_steps=14276]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:26<3:38:36, 397.48s/it, train/loss=0.0644, train/loss_mse=0.0644, train/lr=0.000349, train/num_steps=14277]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:27<3:38:36, 397.48s/it, train/loss=0.0644, train/loss_mse=0.0644, train/lr=0.000349, train/num_steps=14278]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:28<3:38:36, 397.48s/it, train/loss=0.0644, train/loss_mse=0.0644, train/lr=0.000349, train/num_steps=14279]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:29<3:38:36, 397.48s/it, train/loss=0.0644, train/loss_mse=0.0644, train/lr=0.000349, train/num_steps=14280]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:31<3:38:36, 397.48s/it, train/loss=0.0644, train/loss_mse=0.0644, train/lr=0.000349, train/num_steps=14281]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:32<3:38:36, 397.48s/it, train/loss=0.0644, train/loss_mse=0.0644, train/lr=0.000349, train/num_steps=14282]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:33<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000349, train/num_steps=14283]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:34<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000349, train/num_steps=14284]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:36<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000349, train/num_steps=14285]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:37<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000349, train/num_steps=14286]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:38<3:38:36, 397.48s/it, train/loss=0.0644, train/loss_mse=0.0644, train/lr=0.000348, train/num_steps=14287]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:39<3:38:36, 397.48s/it, train/loss=0.0644, train/loss_mse=0.0644, train/lr=0.000348, train/num_steps=14288]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:41<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14289]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:42<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14290]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:43<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14291]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:44<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14292]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:46<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14293]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:47<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14294]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:48<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14295]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:49<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14296]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:51<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14297]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:52<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14298]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:53<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14299]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:54<3:38:36, 397.48s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14300]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:56<3:38:36, 397.48s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000348, train/num_steps=14301]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:57<3:38:36, 397.48s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000348, train/num_steps=14302]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:58<3:38:36, 397.48s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000348, train/num_steps=14303]\u001b[A\n"," 63%|███████████████         | 56/89 [6:15:59<3:38:36, 397.48s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000348, train/num_steps=14304]\u001b[A\n"," 63%|███████████████         | 56/89 [6:16:01<3:38:36, 397.48s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000348, train/num_steps=14305]\u001b[A\n"," 63%|███████████████         | 56/89 [6:16:02<3:38:36, 397.48s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000348, train/num_steps=14306]\u001b[A\n"," 63%|███████████████         | 56/89 [6:16:02<3:38:36, 397.48s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000348, train/num_steps=14307]\u001b[A/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","\n"," 64%|███████████████▎        | 57/89 [6:17:24<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000348, train/num_steps=14307]\u001b[A/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","\n"," 64%|███████████████▎        | 57/89 [6:17:26<3:31:11, 395.99s/it, train/loss=0.0624, train/loss_mse=0.0624, train/lr=0.000348, train/num_steps=14308]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:27<3:31:11, 395.99s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000348, train/num_steps=14309]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:29<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000348, train/num_steps=14310]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:30<3:31:11, 395.99s/it, train/loss=0.0631, train/loss_mse=0.0631, train/lr=0.000348, train/num_steps=14311]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:31<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000348, train/num_steps=14312]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:32<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000348, train/num_steps=14313]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:34<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000348, train/num_steps=14314]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:35<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000348, train/num_steps=14315]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:36<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000348, train/num_steps=14316]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:37<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000348, train/num_steps=14317]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:38<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000348, train/num_steps=14318]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:40<3:31:11, 395.99s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14319]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:41<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000348, train/num_steps=14320]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:42<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000348, train/num_steps=14321]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:17:43<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000348, train/num_steps=14322]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:45<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000348, train/num_steps=14323]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:46<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000348, train/num_steps=14324]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:17:47<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000348, train/num_steps=14325]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:17:48<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000348, train/num_steps=14326]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:50<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000348, train/num_steps=14327]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:51<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000348, train/num_steps=14328]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:52<3:31:11, 395.99s/it, train/loss=0.0645, train/loss_mse=0.0645, train/lr=0.000348, train/num_steps=14329]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:53<3:31:11, 395.99s/it, train/loss=0.0644, train/loss_mse=0.0644, train/lr=0.000348, train/num_steps=14330]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:55<3:31:11, 395.99s/it, train/loss=0.0644, train/loss_mse=0.0644, train/lr=0.000348, train/num_steps=14331]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:56<3:31:11, 395.99s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14332]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:57<3:31:11, 395.99s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000348, train/num_steps=14333]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:17:58<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000348, train/num_steps=14334]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:00<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000348, train/num_steps=14335]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:01<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000348, train/num_steps=14336]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:02<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000348, train/num_steps=14337]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:03<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000347, train/num_steps=14338]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:05<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000347, train/num_steps=14339]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:06<3:31:11, 395.99s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000347, train/num_steps=14340]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:07<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000347, train/num_steps=14341]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:18:08<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000347, train/num_steps=14342]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:10<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000347, train/num_steps=14343]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:11<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000347, train/num_steps=14344]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:12<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000347, train/num_steps=14345]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:13<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000347, train/num_steps=14346]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:15<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000347, train/num_steps=14347]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:16<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000347, train/num_steps=14348]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:17<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000347, train/num_steps=14349]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:18<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000347, train/num_steps=14350]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:20<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000347, train/num_steps=14351]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:21<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000347, train/num_steps=14352]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:22<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000347, train/num_steps=14353]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:23<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000347, train/num_steps=14354]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:25<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000347, train/num_steps=14355]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:18:26<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000347, train/num_steps=14356]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:18:27<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000347, train/num_steps=14357]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:18:28<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000347, train/num_steps=14358]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:18:30<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000347, train/num_steps=14359]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:31<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000347, train/num_steps=14360]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:32<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000347, train/num_steps=14361]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:33<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000347, train/num_steps=14362]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:34<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000347, train/num_steps=14363]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:18:36<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000347, train/num_steps=14364]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:18:37<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000347, train/num_steps=14365]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:38<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14366]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:39<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14367]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:41<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14368]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:42<3:31:11, 395.99s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000347, train/num_steps=14369]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:43<3:31:11, 395.99s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000347, train/num_steps=14370]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:44<3:31:11, 395.99s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000347, train/num_steps=14371]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:46<3:31:11, 395.99s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000347, train/num_steps=14372]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:47<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000347, train/num_steps=14373]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:48<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14374]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:49<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14375]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:18:51<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000347, train/num_steps=14376]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:18:52<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000347, train/num_steps=14377]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:18:53<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000347, train/num_steps=14378]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:54<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14379]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:18:55<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000347, train/num_steps=14380]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:57<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14381]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:58<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14382]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:18:59<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14383]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:00<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14384]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:02<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14385]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:03<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14386]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:04<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14387]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:05<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000347, train/num_steps=14388]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:07<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14389]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:08<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14390]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:09<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14391]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:10<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14392]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:12<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14393]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:13<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14394]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:14<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14395]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:15<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14396]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:17<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14397]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:18<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14398]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:19<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14399]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:20<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14400]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:21<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14401]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:23<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14402]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:24<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14403]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:25<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14404]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:26<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14405]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:28<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14406]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:29<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000346, train/num_steps=14407]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:30<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000346, train/num_steps=14408]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:31<3:31:11, 395.99s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000346, train/num_steps=14409]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:32<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14410]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:34<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14411]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:35<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14412]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:36<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14413]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:37<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14414]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:39<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14415]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:40<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14416]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:41<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14417]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:42<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14418]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:44<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14419]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:45<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14420]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:46<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14421]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:47<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14422]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:49<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14423]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:50<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14424]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:51<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14425]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:52<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14426]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:53<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14427]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:19:55<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14428]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:56<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14429]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:57<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14430]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:19:58<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14431]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:00<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14432]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:01<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14433]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:02<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14434]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:03<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14435]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:05<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000346, train/num_steps=14436]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:06<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14437]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:07<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000346, train/num_steps=14438]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:08<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14439]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:10<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14440]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:11<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14441]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:12<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14442]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:13<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14443]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:14<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14444]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:16<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14445]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:17<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14446]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:18<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14447]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:19<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14448]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:21<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14449]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:22<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14450]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:23<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14451]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:24<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14452]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:26<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14453]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:27<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000345, train/num_steps=14454]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:28<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14455]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:29<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14456]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:31<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14457]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:32<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14458]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:33<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14459]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:34<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14460]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:36<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14461]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:37<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14462]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:38<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14463]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:39<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14464]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:41<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14465]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:42<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14466]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:43<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14467]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:44<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14468]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:46<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14469]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:47<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14470]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:48<3:31:11, 395.99s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000345, train/num_steps=14471]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:49<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14472]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:51<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14473]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:52<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14474]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:53<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14475]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:54<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14476]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:55<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000345, train/num_steps=14477]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:57<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14478]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:20:58<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000345, train/num_steps=14479]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:20:59<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14480]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:21:00<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14481]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:21:02<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14482]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:21:03<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14483]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:21:04<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14484]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:21:05<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14485]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:21:07<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14486]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:21:08<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14487]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:21:09<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14488]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:21:10<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000345, train/num_steps=14489]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:21:12<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000344, train/num_steps=14490]\u001b[A\n"," 64%|████████████████▋         | 57/89 [6:21:13<3:31:11, 395.99s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000344, train/num_steps=14491]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:14<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000344, train/num_steps=14492]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:15<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000344, train/num_steps=14493]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:17<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000344, train/num_steps=14494]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:18<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000344, train/num_steps=14495]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:19<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000344, train/num_steps=14496]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:20<3:31:11, 395.99s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000344, train/num_steps=14497]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:22<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000344, train/num_steps=14498]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:23<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000344, train/num_steps=14499]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:24<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000344, train/num_steps=14500]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:25<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000344, train/num_steps=14501]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:26<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000344, train/num_steps=14502]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:28<3:31:11, 395.99s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000344, train/num_steps=14503]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:29<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000344, train/num_steps=14504]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:30<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000344, train/num_steps=14505]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:31<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000344, train/num_steps=14506]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:33<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000344, train/num_steps=14507]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:34<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000344, train/num_steps=14508]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:35<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000344, train/num_steps=14509]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:36<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000344, train/num_steps=14510]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:38<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000344, train/num_steps=14511]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:39<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000344, train/num_steps=14512]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:40<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000344, train/num_steps=14513]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:41<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000344, train/num_steps=14514]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:43<3:31:11, 395.99s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000344, train/num_steps=14515]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:44<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000344, train/num_steps=14516]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:45<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000344, train/num_steps=14517]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:46<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000344, train/num_steps=14518]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:48<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000344, train/num_steps=14519]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:49<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000344, train/num_steps=14520]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:50<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000344, train/num_steps=14521]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:51<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000344, train/num_steps=14522]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:53<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000344, train/num_steps=14523]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:54<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000344, train/num_steps=14524]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:55<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000344, train/num_steps=14525]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:56<3:31:11, 395.99s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000344, train/num_steps=14526]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:57<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14527]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:21:59<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14528]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:00<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14529]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:01<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14530]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:02<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14531]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:04<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14532]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:05<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14533]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:06<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14534]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:07<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14535]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:09<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14536]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:10<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14537]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:11<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14538]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:12<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14539]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:14<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000344, train/num_steps=14540]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:15<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000343, train/num_steps=14541]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:16<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000343, train/num_steps=14542]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:17<3:31:11, 395.99s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000343, train/num_steps=14543]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:18<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14544]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:20<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14545]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:21<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14546]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:22<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14547]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:23<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14548]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:25<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14549]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:26<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14550]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:27<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14551]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:28<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14552]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:30<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14553]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:31<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14554]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:32<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14555]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:33<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14556]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:35<3:31:11, 395.99s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000343, train/num_steps=14557]\u001b[A\n"," 64%|███████████████▎        | 57/89 [6:22:35<3:31:11, 395.99s/it, train/loss=0.0632, train/loss_mse=0.0632, train/lr=0.000343, train/num_steps=14558]\u001b[A/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","\n"," 65%|███████████████▋        | 58/89 [6:24:02<3:24:54, 396.59s/it, train/loss=0.0632, train/loss_mse=0.0632, train/lr=0.000343, train/num_steps=14558]\u001b[A/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","\n"," 65%|███████████████▋        | 58/89 [6:24:04<3:24:54, 396.59s/it, train/loss=0.0651, train/loss_mse=0.0651, train/lr=0.000343, train/num_steps=14559]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:05<3:24:54, 396.59s/it, train/loss=0.0668, train/loss_mse=0.0668, train/lr=0.000343, train/num_steps=14560]\u001b[A\n"," 65%|████████████████▉         | 58/89 [6:24:06<3:24:54, 396.59s/it, train/loss=0.066, train/loss_mse=0.066, train/lr=0.000343, train/num_steps=14561]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:08<3:24:54, 396.59s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000343, train/num_steps=14562]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:09<3:24:54, 396.59s/it, train/loss=0.0658, train/loss_mse=0.0658, train/lr=0.000343, train/num_steps=14563]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:10<3:24:54, 396.59s/it, train/loss=0.0663, train/loss_mse=0.0663, train/lr=0.000343, train/num_steps=14564]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:11<3:24:54, 396.59s/it, train/loss=0.0653, train/loss_mse=0.0653, train/lr=0.000343, train/num_steps=14565]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:13<3:24:54, 396.59s/it, train/loss=0.0659, train/loss_mse=0.0659, train/lr=0.000343, train/num_steps=14566]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:14<3:24:54, 396.59s/it, train/loss=0.0657, train/loss_mse=0.0657, train/lr=0.000343, train/num_steps=14567]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:15<3:24:54, 396.59s/it, train/loss=0.0655, train/loss_mse=0.0655, train/lr=0.000343, train/num_steps=14568]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:16<3:24:54, 396.59s/it, train/loss=0.0651, train/loss_mse=0.0651, train/lr=0.000343, train/num_steps=14569]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:18<3:24:54, 396.59s/it, train/loss=0.0649, train/loss_mse=0.0649, train/lr=0.000343, train/num_steps=14570]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:19<3:24:54, 396.59s/it, train/loss=0.0647, train/loss_mse=0.0647, train/lr=0.000343, train/num_steps=14571]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:20<3:24:54, 396.59s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000343, train/num_steps=14572]\u001b[A\n"," 65%|████████████████▉         | 58/89 [6:24:21<3:24:54, 396.59s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000343, train/num_steps=14573]\u001b[A\n"," 65%|████████████████▉         | 58/89 [6:24:23<3:24:54, 396.59s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000343, train/num_steps=14574]\u001b[A\n"," 65%|████████████████▉         | 58/89 [6:24:24<3:24:54, 396.59s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000343, train/num_steps=14575]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:25<3:24:54, 396.59s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000343, train/num_steps=14576]\u001b[A\n"," 65%|████████████████▉         | 58/89 [6:24:26<3:24:54, 396.59s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000343, train/num_steps=14577]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:28<3:24:54, 396.59s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000343, train/num_steps=14578]\u001b[A\n"," 65%|████████████████▉         | 58/89 [6:24:29<3:24:54, 396.59s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000343, train/num_steps=14579]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:30<3:24:54, 396.59s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000343, train/num_steps=14580]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:31<3:24:54, 396.59s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000343, train/num_steps=14581]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:32<3:24:54, 396.59s/it, train/loss=0.0644, train/loss_mse=0.0644, train/lr=0.000343, train/num_steps=14582]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:34<3:24:54, 396.59s/it, train/loss=0.0643, train/loss_mse=0.0643, train/lr=0.000343, train/num_steps=14583]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:35<3:24:54, 396.59s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000343, train/num_steps=14584]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:36<3:24:54, 396.59s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000343, train/num_steps=14585]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:37<3:24:54, 396.59s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000343, train/num_steps=14586]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:39<3:24:54, 396.59s/it, train/loss=0.0642, train/loss_mse=0.0642, train/lr=0.000343, train/num_steps=14587]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:40<3:24:54, 396.59s/it, train/loss=0.0641, train/loss_mse=0.0641, train/lr=0.000343, train/num_steps=14588]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:41<3:24:54, 396.59s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000343, train/num_steps=14589]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:43<3:24:54, 396.59s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000343, train/num_steps=14590]\u001b[A\n"," 65%|████████████████▉         | 58/89 [6:24:44<3:24:54, 396.59s/it, train/loss=0.064, train/loss_mse=0.064, train/lr=0.000342, train/num_steps=14591]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:45<3:24:54, 396.59s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000342, train/num_steps=14592]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:46<3:24:54, 396.59s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000342, train/num_steps=14593]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:47<3:24:54, 396.59s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000342, train/num_steps=14594]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:49<3:24:54, 396.59s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000342, train/num_steps=14595]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:50<3:24:54, 396.59s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000342, train/num_steps=14596]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:51<3:24:54, 396.59s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000342, train/num_steps=14597]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:52<3:24:54, 396.59s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000342, train/num_steps=14598]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:54<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14599]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:55<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14600]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:56<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14601]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:57<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14602]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:24:59<3:24:54, 396.59s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000342, train/num_steps=14603]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:00<3:24:54, 396.59s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000342, train/num_steps=14604]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:01<3:24:54, 396.59s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000342, train/num_steps=14605]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:02<3:24:54, 396.59s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000342, train/num_steps=14606]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:04<3:24:54, 396.59s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000342, train/num_steps=14607]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:05<3:24:54, 396.59s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000342, train/num_steps=14608]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:06<3:24:54, 396.59s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000342, train/num_steps=14609]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:07<3:24:54, 396.59s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000342, train/num_steps=14610]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:08<3:24:54, 396.59s/it, train/loss=0.0639, train/loss_mse=0.0639, train/lr=0.000342, train/num_steps=14611]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:10<3:24:54, 396.59s/it, train/loss=0.0638, train/loss_mse=0.0638, train/lr=0.000342, train/num_steps=14612]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:11<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14613]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:12<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14614]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:13<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14615]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:15<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14616]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:16<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14617]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:17<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14618]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:18<3:24:54, 396.59s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000342, train/num_steps=14619]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:20<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14620]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:21<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14621]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:22<3:24:54, 396.59s/it, train/loss=0.0637, train/loss_mse=0.0637, train/lr=0.000342, train/num_steps=14622]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:23<3:24:54, 396.59s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000342, train/num_steps=14623]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:25<3:24:54, 396.59s/it, train/loss=0.0636, train/loss_mse=0.0636, train/lr=0.000342, train/num_steps=14624]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:26<3:24:54, 396.59s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000342, train/num_steps=14625]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:27<3:24:54, 396.59s/it, train/loss=0.0635, train/loss_mse=0.0635, train/lr=0.000342, train/num_steps=14626]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:28<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000342, train/num_steps=14627]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:30<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000342, train/num_steps=14628]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:31<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000342, train/num_steps=14629]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:32<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000342, train/num_steps=14630]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:33<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000342, train/num_steps=14631]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:34<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000342, train/num_steps=14632]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:36<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000342, train/num_steps=14633]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:37<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000342, train/num_steps=14634]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:38<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000342, train/num_steps=14635]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:40<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000342, train/num_steps=14636]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:41<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000342, train/num_steps=14637]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:42<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000342, train/num_steps=14638]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:44<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000342, train/num_steps=14639]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:45<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000342, train/num_steps=14640]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:46<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000342, train/num_steps=14641]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:47<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000341, train/num_steps=14642]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:48<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14643]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:50<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000341, train/num_steps=14644]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:51<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14645]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:52<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14646]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:53<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14647]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:55<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14648]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:56<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000341, train/num_steps=14649]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:57<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000341, train/num_steps=14650]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:25:58<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14651]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:00<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14652]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:01<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14653]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:02<3:24:54, 396.59s/it, train/loss=0.0632, train/loss_mse=0.0632, train/lr=0.000341, train/num_steps=14654]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:03<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14655]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:04<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14656]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:06<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14657]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:07<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14658]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:08<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000341, train/num_steps=14659]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:09<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14660]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:11<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14661]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:12<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14662]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:13<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14663]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:14<3:24:54, 396.59s/it, train/loss=0.0632, train/loss_mse=0.0632, train/lr=0.000341, train/num_steps=14664]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:15<3:24:54, 396.59s/it, train/loss=0.0632, train/loss_mse=0.0632, train/lr=0.000341, train/num_steps=14665]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:17<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14666]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:18<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14667]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:19<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14668]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:20<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14669]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:22<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14670]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:23<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14671]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:24<3:24:54, 396.59s/it, train/loss=0.0634, train/loss_mse=0.0634, train/lr=0.000341, train/num_steps=14672]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:25<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14673]\u001b[A\n"," 65%|███████████████▋        | 58/89 [6:26:27<3:24:54, 396.59s/it, train/loss=0.0633, train/loss_mse=0.0633, train/lr=0.000341, train/num_steps=14674]\u001b[A\n"]}]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}