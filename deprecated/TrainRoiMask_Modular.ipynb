{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HHkuK63-YIem"
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from diffusers.models.vae import Decoder\n",
    "from diffusers.models import AutoencoderKL\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from collections import OrderedDict\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "def load_image(path):\n",
    "    fp = open(path, 'rb')\n",
    "    image = Image.open(fp).convert(\"RGB\")\n",
    "    image = np.array(image).astype(np.float32) / 255.0   #(425, 425, 3)\n",
    "    fp.close()\n",
    "    image = image[None].transpose(0, 3, 1, 2)           # (1, 3, 425, 425)\n",
    "    image = torch.from_numpy(image)\n",
    "    return image\n",
    "\n",
    "def save_image(samples, path):     \n",
    "    samples = 255 * samples.clamp(0,1)    # (1, 3, 425, 425)\n",
    "    samples = samples.detach().numpy()\n",
    "    samples = samples.transpose(0, 2, 3, 1)       #(1, 425, 425, 3)\n",
    "    image = samples[0]                            #(425, 425, 3)\n",
    "    image = Image.fromarray(image.astype(np.uint8))\n",
    "    image.save(path)\n",
    "\n",
    "def encode_img(input_img, vae):\n",
    "    # Single image -> single latent in a batch (so size 1, 4, 53, 53)\n",
    "    if len(input_img.shape)<4:\n",
    "        input_img = input_img.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(input_img*2 - 1) # Note scaling\n",
    "    return 0.18215 * latent.latent_dist.sample()\n",
    "\n",
    "def decode_img(latents, vae):\n",
    "    # bath of latents -> list of images\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach()\n",
    "    return image\n",
    "\n",
    "def transform(image):\n",
    "    return transforms.Resize([512, 512])(image)\n",
    "\n",
    "def to_PIL(tensor):\n",
    "    return torchvision.transforms.ToPILImage()(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Y4Vtj4v0YR2a"
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "def DoOneRoiMask(data_path, subject_num, data, LR, roi, maskedFmri):\n",
    "\n",
    "  if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
    "      roi_class = 'prf-visualrois'\n",
    "  elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
    "      roi_class = 'floc-bodies'\n",
    "  elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
    "      roi_class = 'floc-faces'\n",
    "  elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
    "      roi_class = 'floc-places'\n",
    "  elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
    "      roi_class = 'floc-words'\n",
    "  elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
    "      roi_class = 'streams'\n",
    "\n",
    "  # load mask file\n",
    "  roi_space_dir = os.path.join(data_path, 'subj0{}'.format(subject_num), 'roi_masks',\n",
    "      LR[0]+'h.'+roi_class+'_space.npy')\n",
    "  roi_map_dir = os.path.join(data_path, 'subj0{}'.format(subject_num), 'roi_masks',\n",
    "      'mapping_'+roi_class+'.npy')\n",
    "  roi_space = np.load(roi_space_dir)\n",
    "  roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n",
    "\n",
    "  # Select the vertices corresponding to the ROI of interest\n",
    "  roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
    "\n",
    "  target_index = np.where(np.isin(roi_space, roi_mapping))[0]\n",
    "\n",
    "  for i in target_index:\n",
    "    maskedFmri[:, i] = data[:, i]\n",
    "\n",
    "  return maskedFmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FdCcMGlUY6Ba"
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "def GetRoiMaskedFmri(data_path, subject_num, LR, regions):\n",
    "  \"\"\"\n",
    "    data_path: string -- path to folder for subj0x\n",
    "    subject_num: int -- subject number for training ex: 1\n",
    "    LR: string \"left\" or \"right\" -- specify the fmri data is left or right hemisphere\n",
    "    regoins: list of strings of interseted regions, split by bankspace -- ex: [\"FFA-1\", \"OPA\"]\n",
    "\n",
    "    return: masked fmri data -- (n, 19004) or (n, 20054)\n",
    "  \"\"\"\n",
    "  if LR == \"left\":\n",
    "    data = np.load(os.path.join(data_path, 'subj0{}'.format(subject_num), 'training_split/training_fmri',\n",
    "    LR[0]+'h_training_fmri.npy'))\n",
    "    assert (data.shape[1] == 19004)\n",
    "  elif LR == \"right\":\n",
    "    data = np.load(os.path.join(data_path, 'subj0{}'.format(subject_num), 'training_split/training_fmri',\n",
    "    LR[0]+'h_training_fmri.npy'))\n",
    "    assert (data.shape[1] == 20544)\n",
    "\n",
    "  maskedFmriData = np.zeros_like(data, dtype=float)\n",
    "\n",
    "  for region in regions:\n",
    "    DoOneRoiMask(data_path=data_path, subject_num=subject_num, data=data, LR=LR, roi=region, maskedFmri=maskedFmriData)\n",
    "\n",
    "  return maskedFmriData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "A4UG6FDCZhSZ"
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "# usage\n",
    "\n",
    "# determine the datapath and training subject\n",
    "data_path = './dataset/'\n",
    "subject_num = 1\n",
    "\n",
    "# get masked fmri data\n",
    "ROIs_test1 = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"FFA-1\", \"FFA-2\", \"PPA\"]\n",
    "ROIs_test2 = [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\"]\n",
    "ROIs_test3 = [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]\n",
    "\n",
    "lh = GetRoiMaskedFmri(data_path=data_path, subject_num=subject_num, LR=\"left\", regions=ROIs_test3)\n",
    "rh = GetRoiMaskedFmri(data_path=data_path, subject_num=subject_num, LR=\"right\", regions=ROIs_test3)\n",
    "lrh = np.concatenate((lh, rh), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozpzTPeQd_vj",
    "outputId": "ea69ba17-4cb0-43fe-f720-790d4808c49b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkoioslin\u001b[0m (\u001b[33mtkoioslin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "#!wandb login 93a448bfae48a1b2cc86ef37d909472923b4a0fd\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2djcXv9sZOLa",
    "outputId": "803b7547-f542-4e46-f3d9-d595a137a831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RIM5R0qOZOLa"
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "class Voxel2StableDiffusionModel(torch.nn.Module):\n",
    "    # define the prototype of the module\n",
    "    def __init__(self, in_dim=39548, h=2048, n_blocks=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin0 = nn.Sequential(\n",
    "            nn.Linear(in_dim, h, bias=False),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(h, h, bias=False),\n",
    "                nn.LayerNorm(h),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(0.3),\n",
    "            ) for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "        self.lin1 = nn.Linear(h, 16384, bias=False)\n",
    "        self.norm = nn.GroupNorm(1, 64)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "        init_weights(self.lin0)\n",
    "        self.mlp.apply(init_weights)\n",
    "        init_weights(self.lin1)\n",
    "\n",
    "        self.upsampler = Decoder(\n",
    "            in_channels=64,\n",
    "            out_channels=4,\n",
    "            up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
    "            block_out_channels=[64, 128, 256],\n",
    "            layers_per_block=1,\n",
    "        )\n",
    "        for parm in self.upsampler.parameters():\n",
    "            parm.require_grad = False\n",
    "        self.upsampler.eval()\n",
    "\n",
    "    # define how it forward, using the module defined above\n",
    "    def forward(self, x):\n",
    "        x = self.lin0(x)\n",
    "        residual = x\n",
    "        for res_block in self.mlp:\n",
    "            x = res_block(x)\n",
    "            x = x + residual\n",
    "            residual = x\n",
    "        x = x.reshape(len(x), -1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.norm(x.reshape(x.shape[0], -1, 16, 16).contiguous())\n",
    "        return self.upsampler(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFMY6XMAZOLb",
    "outputId": "10244222-c100-4246-8e9b-00632325395b"
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "voxel2sd = Voxel2StableDiffusionModel()\n",
    "voxel2sd.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-FeJT21ZOLb",
    "outputId": "8c821d99-f430-4109-ad2d-a3917699c373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 2048]      80,994,304\n",
      "         LayerNorm-2                 [-1, 2048]           4,096\n",
      "              SiLU-3                 [-1, 2048]               0\n",
      "           Dropout-4                 [-1, 2048]               0\n",
      "            Linear-5                 [-1, 2048]       4,194,304\n",
      "         LayerNorm-6                 [-1, 2048]           4,096\n",
      "              SiLU-7                 [-1, 2048]               0\n",
      "           Dropout-8                 [-1, 2048]               0\n",
      "            Linear-9                 [-1, 2048]       4,194,304\n",
      "        LayerNorm-10                 [-1, 2048]           4,096\n",
      "             SiLU-11                 [-1, 2048]               0\n",
      "          Dropout-12                 [-1, 2048]               0\n",
      "           Linear-13                 [-1, 2048]       4,194,304\n",
      "        LayerNorm-14                 [-1, 2048]           4,096\n",
      "             SiLU-15                 [-1, 2048]               0\n",
      "          Dropout-16                 [-1, 2048]               0\n",
      "           Linear-17                 [-1, 2048]       4,194,304\n",
      "        LayerNorm-18                 [-1, 2048]           4,096\n",
      "             SiLU-19                 [-1, 2048]               0\n",
      "          Dropout-20                 [-1, 2048]               0\n",
      "           Linear-21                [-1, 16384]      33,554,432\n",
      "        GroupNorm-22           [-1, 64, 16, 16]             128\n",
      "           Conv2d-23          [-1, 256, 16, 16]         147,712\n",
      "        GroupNorm-24          [-1, 256, 16, 16]             512\n",
      "             SiLU-25          [-1, 256, 16, 16]               0\n",
      "             SiLU-26          [-1, 256, 16, 16]               0\n",
      "             SiLU-27          [-1, 256, 16, 16]               0\n",
      "             SiLU-28          [-1, 256, 16, 16]               0\n",
      "             SiLU-29          [-1, 256, 16, 16]               0\n",
      "             SiLU-30          [-1, 256, 16, 16]               0\n",
      "             SiLU-31          [-1, 256, 16, 16]               0\n",
      "             SiLU-32          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-33          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-34          [-1, 256, 16, 16]             512\n",
      "             SiLU-35          [-1, 256, 16, 16]               0\n",
      "             SiLU-36          [-1, 256, 16, 16]               0\n",
      "             SiLU-37          [-1, 256, 16, 16]               0\n",
      "             SiLU-38          [-1, 256, 16, 16]               0\n",
      "             SiLU-39          [-1, 256, 16, 16]               0\n",
      "             SiLU-40          [-1, 256, 16, 16]               0\n",
      "             SiLU-41          [-1, 256, 16, 16]               0\n",
      "             SiLU-42          [-1, 256, 16, 16]               0\n",
      "          Dropout-43          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-44          [-1, 256, 16, 16]         590,080\n",
      "    ResnetBlock2D-45          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-46             [-1, 256, 256]             512\n",
      "LoRACompatibleLinear-47             [-1, 256, 256]          65,792\n",
      "LoRACompatibleLinear-48             [-1, 256, 256]          65,792\n",
      "LoRACompatibleLinear-49             [-1, 256, 256]          65,792\n",
      "LoRACompatibleLinear-50             [-1, 256, 256]          65,792\n",
      "          Dropout-51             [-1, 256, 256]               0\n",
      "        Attention-52          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-53          [-1, 256, 16, 16]             512\n",
      "             SiLU-54          [-1, 256, 16, 16]               0\n",
      "             SiLU-55          [-1, 256, 16, 16]               0\n",
      "             SiLU-56          [-1, 256, 16, 16]               0\n",
      "             SiLU-57          [-1, 256, 16, 16]               0\n",
      "             SiLU-58          [-1, 256, 16, 16]               0\n",
      "             SiLU-59          [-1, 256, 16, 16]               0\n",
      "             SiLU-60          [-1, 256, 16, 16]               0\n",
      "             SiLU-61          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-62          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-63          [-1, 256, 16, 16]             512\n",
      "             SiLU-64          [-1, 256, 16, 16]               0\n",
      "             SiLU-65          [-1, 256, 16, 16]               0\n",
      "             SiLU-66          [-1, 256, 16, 16]               0\n",
      "             SiLU-67          [-1, 256, 16, 16]               0\n",
      "             SiLU-68          [-1, 256, 16, 16]               0\n",
      "             SiLU-69          [-1, 256, 16, 16]               0\n",
      "             SiLU-70          [-1, 256, 16, 16]               0\n",
      "             SiLU-71          [-1, 256, 16, 16]               0\n",
      "          Dropout-72          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-73          [-1, 256, 16, 16]         590,080\n",
      "    ResnetBlock2D-74          [-1, 256, 16, 16]               0\n",
      "   UNetMidBlock2D-75          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-76          [-1, 256, 16, 16]             512\n",
      "             SiLU-77          [-1, 256, 16, 16]               0\n",
      "             SiLU-78          [-1, 256, 16, 16]               0\n",
      "             SiLU-79          [-1, 256, 16, 16]               0\n",
      "             SiLU-80          [-1, 256, 16, 16]               0\n",
      "             SiLU-81          [-1, 256, 16, 16]               0\n",
      "             SiLU-82          [-1, 256, 16, 16]               0\n",
      "             SiLU-83          [-1, 256, 16, 16]               0\n",
      "             SiLU-84          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-85          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-86          [-1, 256, 16, 16]             512\n",
      "             SiLU-87          [-1, 256, 16, 16]               0\n",
      "             SiLU-88          [-1, 256, 16, 16]               0\n",
      "             SiLU-89          [-1, 256, 16, 16]               0\n",
      "             SiLU-90          [-1, 256, 16, 16]               0\n",
      "             SiLU-91          [-1, 256, 16, 16]               0\n",
      "             SiLU-92          [-1, 256, 16, 16]               0\n",
      "             SiLU-93          [-1, 256, 16, 16]               0\n",
      "             SiLU-94          [-1, 256, 16, 16]               0\n",
      "          Dropout-95          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-96          [-1, 256, 16, 16]         590,080\n",
      "    ResnetBlock2D-97          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-98          [-1, 256, 16, 16]             512\n",
      "             SiLU-99          [-1, 256, 16, 16]               0\n",
      "            SiLU-100          [-1, 256, 16, 16]               0\n",
      "            SiLU-101          [-1, 256, 16, 16]               0\n",
      "            SiLU-102          [-1, 256, 16, 16]               0\n",
      "            SiLU-103          [-1, 256, 16, 16]               0\n",
      "            SiLU-104          [-1, 256, 16, 16]               0\n",
      "            SiLU-105          [-1, 256, 16, 16]               0\n",
      "            SiLU-106          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-107          [-1, 256, 16, 16]         590,080\n",
      "       GroupNorm-108          [-1, 256, 16, 16]             512\n",
      "            SiLU-109          [-1, 256, 16, 16]               0\n",
      "            SiLU-110          [-1, 256, 16, 16]               0\n",
      "            SiLU-111          [-1, 256, 16, 16]               0\n",
      "            SiLU-112          [-1, 256, 16, 16]               0\n",
      "            SiLU-113          [-1, 256, 16, 16]               0\n",
      "            SiLU-114          [-1, 256, 16, 16]               0\n",
      "            SiLU-115          [-1, 256, 16, 16]               0\n",
      "            SiLU-116          [-1, 256, 16, 16]               0\n",
      "         Dropout-117          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-118          [-1, 256, 16, 16]         590,080\n",
      "   ResnetBlock2D-119          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-120          [-1, 256, 32, 32]         590,080\n",
      "      Upsample2D-121          [-1, 256, 32, 32]               0\n",
      "UpDecoderBlock2D-122          [-1, 256, 32, 32]               0\n",
      "       GroupNorm-123          [-1, 256, 32, 32]             512\n",
      "            SiLU-124          [-1, 256, 32, 32]               0\n",
      "            SiLU-125          [-1, 256, 32, 32]               0\n",
      "            SiLU-126          [-1, 256, 32, 32]               0\n",
      "            SiLU-127          [-1, 256, 32, 32]               0\n",
      "            SiLU-128          [-1, 256, 32, 32]               0\n",
      "            SiLU-129          [-1, 256, 32, 32]               0\n",
      "            SiLU-130          [-1, 256, 32, 32]               0\n",
      "            SiLU-131          [-1, 256, 32, 32]               0\n",
      "LoRACompatibleConv-132          [-1, 128, 32, 32]         295,040\n",
      "       GroupNorm-133          [-1, 128, 32, 32]             256\n",
      "            SiLU-134          [-1, 128, 32, 32]               0\n",
      "            SiLU-135          [-1, 128, 32, 32]               0\n",
      "            SiLU-136          [-1, 128, 32, 32]               0\n",
      "            SiLU-137          [-1, 128, 32, 32]               0\n",
      "            SiLU-138          [-1, 128, 32, 32]               0\n",
      "            SiLU-139          [-1, 128, 32, 32]               0\n",
      "            SiLU-140          [-1, 128, 32, 32]               0\n",
      "            SiLU-141          [-1, 128, 32, 32]               0\n",
      "         Dropout-142          [-1, 128, 32, 32]               0\n",
      "LoRACompatibleConv-143          [-1, 128, 32, 32]         147,584\n",
      "LoRACompatibleConv-144          [-1, 128, 32, 32]          32,896\n",
      "   ResnetBlock2D-145          [-1, 128, 32, 32]               0\n",
      "       GroupNorm-146          [-1, 128, 32, 32]             256\n",
      "            SiLU-147          [-1, 128, 32, 32]               0\n",
      "            SiLU-148          [-1, 128, 32, 32]               0\n",
      "            SiLU-149          [-1, 128, 32, 32]               0\n",
      "            SiLU-150          [-1, 128, 32, 32]               0\n",
      "            SiLU-151          [-1, 128, 32, 32]               0\n",
      "            SiLU-152          [-1, 128, 32, 32]               0\n",
      "            SiLU-153          [-1, 128, 32, 32]               0\n",
      "            SiLU-154          [-1, 128, 32, 32]               0\n",
      "LoRACompatibleConv-155          [-1, 128, 32, 32]         147,584\n",
      "       GroupNorm-156          [-1, 128, 32, 32]             256\n",
      "            SiLU-157          [-1, 128, 32, 32]               0\n",
      "            SiLU-158          [-1, 128, 32, 32]               0\n",
      "            SiLU-159          [-1, 128, 32, 32]               0\n",
      "            SiLU-160          [-1, 128, 32, 32]               0\n",
      "            SiLU-161          [-1, 128, 32, 32]               0\n",
      "            SiLU-162          [-1, 128, 32, 32]               0\n",
      "            SiLU-163          [-1, 128, 32, 32]               0\n",
      "            SiLU-164          [-1, 128, 32, 32]               0\n",
      "         Dropout-165          [-1, 128, 32, 32]               0\n",
      "LoRACompatibleConv-166          [-1, 128, 32, 32]         147,584\n",
      "   ResnetBlock2D-167          [-1, 128, 32, 32]               0\n",
      "LoRACompatibleConv-168          [-1, 128, 64, 64]         147,584\n",
      "      Upsample2D-169          [-1, 128, 64, 64]               0\n",
      "UpDecoderBlock2D-170          [-1, 128, 64, 64]               0\n",
      "       GroupNorm-171          [-1, 128, 64, 64]             256\n",
      "            SiLU-172          [-1, 128, 64, 64]               0\n",
      "            SiLU-173          [-1, 128, 64, 64]               0\n",
      "            SiLU-174          [-1, 128, 64, 64]               0\n",
      "            SiLU-175          [-1, 128, 64, 64]               0\n",
      "            SiLU-176          [-1, 128, 64, 64]               0\n",
      "            SiLU-177          [-1, 128, 64, 64]               0\n",
      "            SiLU-178          [-1, 128, 64, 64]               0\n",
      "            SiLU-179          [-1, 128, 64, 64]               0\n",
      "LoRACompatibleConv-180           [-1, 64, 64, 64]          73,792\n",
      "       GroupNorm-181           [-1, 64, 64, 64]             128\n",
      "            SiLU-182           [-1, 64, 64, 64]               0\n",
      "            SiLU-183           [-1, 64, 64, 64]               0\n",
      "            SiLU-184           [-1, 64, 64, 64]               0\n",
      "            SiLU-185           [-1, 64, 64, 64]               0\n",
      "            SiLU-186           [-1, 64, 64, 64]               0\n",
      "            SiLU-187           [-1, 64, 64, 64]               0\n",
      "            SiLU-188           [-1, 64, 64, 64]               0\n",
      "            SiLU-189           [-1, 64, 64, 64]               0\n",
      "         Dropout-190           [-1, 64, 64, 64]               0\n",
      "LoRACompatibleConv-191           [-1, 64, 64, 64]          36,928\n",
      "LoRACompatibleConv-192           [-1, 64, 64, 64]           8,256\n",
      "   ResnetBlock2D-193           [-1, 64, 64, 64]               0\n",
      "       GroupNorm-194           [-1, 64, 64, 64]             128\n",
      "            SiLU-195           [-1, 64, 64, 64]               0\n",
      "            SiLU-196           [-1, 64, 64, 64]               0\n",
      "            SiLU-197           [-1, 64, 64, 64]               0\n",
      "            SiLU-198           [-1, 64, 64, 64]               0\n",
      "            SiLU-199           [-1, 64, 64, 64]               0\n",
      "            SiLU-200           [-1, 64, 64, 64]               0\n",
      "            SiLU-201           [-1, 64, 64, 64]               0\n",
      "            SiLU-202           [-1, 64, 64, 64]               0\n",
      "LoRACompatibleConv-203           [-1, 64, 64, 64]          36,928\n",
      "       GroupNorm-204           [-1, 64, 64, 64]             128\n",
      "            SiLU-205           [-1, 64, 64, 64]               0\n",
      "            SiLU-206           [-1, 64, 64, 64]               0\n",
      "            SiLU-207           [-1, 64, 64, 64]               0\n",
      "            SiLU-208           [-1, 64, 64, 64]               0\n",
      "            SiLU-209           [-1, 64, 64, 64]               0\n",
      "            SiLU-210           [-1, 64, 64, 64]               0\n",
      "            SiLU-211           [-1, 64, 64, 64]               0\n",
      "            SiLU-212           [-1, 64, 64, 64]               0\n",
      "         Dropout-213           [-1, 64, 64, 64]               0\n",
      "LoRACompatibleConv-214           [-1, 64, 64, 64]          36,928\n",
      "   ResnetBlock2D-215           [-1, 64, 64, 64]               0\n",
      "UpDecoderBlock2D-216           [-1, 64, 64, 64]               0\n",
      "       GroupNorm-217           [-1, 64, 64, 64]             128\n",
      "            SiLU-218           [-1, 64, 64, 64]               0\n",
      "          Conv2d-219            [-1, 4, 64, 64]           2,308\n",
      "         Decoder-220            [-1, 4, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 138,188,228\n",
      "Trainable params: 138,188,228\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.15\n",
      "Forward/backward pass size (MB): 235.31\n",
      "Params size (MB): 527.15\n",
      "Estimated Total Size (MB): 762.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "from torchsummary import summary\n",
    "summary(voxel2sd, (39548,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ECD7Kp8VZOLc"
   },
   "outputs": [],
   "source": [
    "# some hyperparameters\n",
    "batch_size = 16\n",
    "num_epochs = 200\n",
    "num_train = 5000\n",
    "lr_scheduler = 'cycle'\n",
    "initial_lr = 1e-4\n",
    "max_lr = 5e-4\n",
    "random_seed = 3124\n",
    "train_size = 0.7\n",
    "valid_size = 1 - train_size\n",
    "num_workers = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mArxER00ZOLc"
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "# some path information\n",
    "dataset_path = './dataset/'\n",
    "training_path = dataset_path + 'subj0{}/training_split/'\n",
    "training_fmri_path = training_path + 'training_fmri/'\n",
    "training_images_path = training_path + 'training_images/'\n",
    "testing_path = dataset_path + 'subj0{}/test_split/test_fmri/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1QWOOGMTQ5kn"
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zD9IwmfeZOLc"
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self, fmri_data, images_folder, transform=None):\n",
    "    self.fmri_data = fmri_data\n",
    "    self.transform = transform\n",
    "    self.image_latents = []\n",
    "    for i in range(len(self.fmri_data)):\n",
    "        image = load_image(f\"{images_folder}/{i}.png\").to(device)\n",
    "        tr = self.transform(image)\n",
    "        self.image_latents.append(encode_img(tr, vae).to('cpu'))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.fmri_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    fmri = self.fmri_data[idx]\n",
    "    image_latent = self.image_latents[idx]\n",
    "\n",
    "    return fmri, image_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TXrYUxf1ZOLc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: Execution was interrupted, reason: signal SIGSTOP.\n",
      "The process has been left at the point where it was interrupted, use \"thread return -x\" to return to the state before expression evaluation.\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "transform = transforms.Resize([512, 512])\n",
    "my_dataset = MyDataset(lrh, training_images_path.format(1), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "F4U_HWgYZOLd"
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "# train-val split\n",
    "generator = torch.Generator().manual_seed(random_seed)\n",
    "trainset, validset = random_split(my_dataset, [train_size, valid_size], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ovlcL_YkZOLd"
   },
   "outputs": [],
   "source": [
    "# build dataloader\n",
    "train_dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(validset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "NO9DAON7ZOLd"
   },
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in voxel2sd.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in voxel2sd.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "PMt7XDcvZOLd"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=initial_lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr,\n",
    "                                            total_steps=num_epochs*((num_train//batch_size)//num_workers),\n",
    "                                            final_div_factor=1000,\n",
    "                                            last_epoch=-1, pct_start=2/num_epochs)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "#                                                    milestones=[50*i for i in range(num_epochs*((num_train//batch_size)//num_workers//50))],\n",
    "#                                                    gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "FrIZYL5wZOLd",
    "outputId": "e4832614-59dd-48c6-9a3b-fea0b3fbdc69"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2730brj8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">laced-night-87</strong>: <a href=\"https://wandb.ai/tkoioslin/MindEye/runs/2730brj8\" target=\"_blank\">https://wandb.ai/tkoioslin/MindEye/runs/2730brj8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231217_000317-2730brj8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2730brj8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Owner/Desktop/Machine Learning/Final Project/CNN/wandb/run-20231217_003324-2yu1epnf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tkoioslin/MindEye/runs/2yu1epnf\" target=\"_blank\">dauntless-thunder-88</a></strong> to <a href=\"https://wandb.ai/tkoioslin/MindEye\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tkoioslin/MindEye/runs/2yu1epnf?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb8033d8670>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize wandb\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"MindEye\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": initial_lr,\n",
    "        \"architecture\": \"MLP\",\n",
    "        \"dataset\": \"NSD\",\n",
    "        \"epochs\": num_epochs,\n",
    "        \"random_seed\": random_seed,\n",
    "        \"train_size\": train_size,\n",
    "        \"valid_size\": valid_size\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfTJl4OYZOLe",
    "outputId": "86e4a4c6-a442-4056-edbc-2ddab15f60ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                         | 0/200 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "progress_bar = tqdm(range(epoch, num_epochs), ncols=150)\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_PqmEXuzZOLe",
    "outputId": "8db817d0-9c1c-4dd1-d214-7aed8aa1b896"
   },
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('./ROI_ModelSave/70') # [TODO] 修改 Model 編號\n",
    "# voxel2sd.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch'] + 1\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# progress_bar = tqdm(range(epoch, num_epochs), ncols=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "GW4jCM6yZOLe",
    "outputId": "103db0f5-34d4-4803-8a59-b59cd0f1afc6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|▎| 52/200 [10:59:16<31:16:22, 760.69s/it, _runtime=39561, _timestamp=1.7e+9, train/loss=0.518, train/loss_mse=0.518, train/lr=0.000463, train/num\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5171/1398986083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             adamw(params_with_grad,\n\u001b[0m\u001b[1;32m    163\u001b[0m                   \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                   \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    220\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in progress_bar:\n",
    "    voxel2sd.train()\n",
    "\n",
    "    loss_sum = 0\n",
    "    val_loss_sum = 0\n",
    "\n",
    "    for train_i, data in enumerate(train_dataloader):\n",
    "        voxels, image_latents = data\n",
    "        voxels = voxels.to(device).float()\n",
    "        image_latents = image_latents.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # run image encoder\n",
    "        encoded_latents = torch.cat([x for x in image_latents])\n",
    "        # MLP forward\n",
    "        encoded_predict = voxel2sd(voxels)\n",
    "        # calulate loss\n",
    "        loss = F.l1_loss(encoded_predict, encoded_latents)\n",
    "        loss_sum += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "            \"train/lr\": lrs[-1],\n",
    "            \"train/num_steps\": len(losses),\n",
    "            \"train/loss_mse\": loss_sum / (train_i + 1)\n",
    "        }\n",
    "        wandb.log(logs)\n",
    "\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "    # After training one epoch, evaluation\n",
    "    # save ckpt first\n",
    "    torch.save({\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': voxel2sd.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'loss': loss,\n",
    "      }, './Models/{}'.format(epoch)\n",
    "    )\n",
    "\n",
    "    # Then evaluate\n",
    "    # voxel2sd.eval()\n",
    "    # for val_i, data in enumerate(val_dataloader):\n",
    "    #     voxels, image_latents = data\n",
    "    #     voxels = voxels.to(device).float()\n",
    "    #     image_latents = image_latents.to(device).float()\n",
    "\n",
    "    #     # run image encoder\n",
    "    #     encoded_latents = torch.cat([x for x in image_latents])\n",
    "    #     # MLP forward\n",
    "    #     encoded_predict = voxel2sd(voxels)\n",
    "    #     # calulate loss\n",
    "    #     loss = F.l1_loss(encoded_predict, encoded_latents)\n",
    "    #     val_loss_sum += loss.item()\n",
    "    #     val_losses.append(loss.item())\n",
    "\n",
    "    # Print results\n",
    "    # logs = {\n",
    "    #     \"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "    #     \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "    #     \"train/lr\": lrs[-1],\n",
    "    #     \"train/num_steps\": len(losses),\n",
    "    #     \"train/loss_mse\": loss_sum / (train_i + 1),\n",
    "    #     \"val/loss_mse\": val_loss_sum / (val_i + 1)\n",
    "    # }\n",
    "    # wandb.log(logs)\n",
    "\n",
    "    # print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KTt-xcPsOSj4",
    "outputId": "e83fb48b-eec0-43ef-aef1-3e13ab67e2a1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Mojo",
   "language": "mojo",
   "name": "mojo-jupyter-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "mojo"
   },
   "file_extension": ".mojo",
   "mimetype": "text/x-mojo",
   "name": "mojo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
