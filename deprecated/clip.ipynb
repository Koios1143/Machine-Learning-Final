{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14735,"status":"ok","timestamp":1702238297259,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"MExzTjqAdPIY","outputId":"0ceddd3e-f3cf-45ac-ad81-92f5aeaed416"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# use colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702238297260,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"_63iSnwvTex5","outputId":"fa3a70f9-a27e-4779-8d7f-6806e26d9056"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/My Drive/MindEye\n"]}],"source":["import os\n","%cd '/content/drive/My Drive/MindEye'\n","os.chdir('/content/drive/My Drive/MindEye')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27676,"status":"ok","timestamp":1702238324934,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"LDFxHLjZgRiJ","outputId":"afcb7873-c274-4526-c07c-290e9990bcbe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting open_clip_torch\n","  Downloading open_clip_torch-2.23.0-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dalle2-pytorch\n","  Downloading dalle2_pytorch-1.15.6-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.16.0+cu118)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2023.6.3)\n","Collecting ftfy (from open_clip_torch)\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (4.66.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.19.4)\n","Collecting sentencepiece (from open_clip_torch)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (3.20.3)\n","Collecting timm (from open_clip_torch)\n","  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate (from dalle2-pytorch)\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (8.1.7)\n","Collecting clip-anytorch>=2.5.2 (from dalle2-pytorch)\n","  Downloading clip_anytorch-2.5.2-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting coca-pytorch>=0.0.5 (from dalle2-pytorch)\n","  Downloading CoCa_pytorch-0.1.0-py3-none-any.whl (7.0 kB)\n","Collecting ema-pytorch>=0.0.7 (from dalle2-pytorch)\n","  Downloading ema_pytorch-0.3.1-py3-none-any.whl (4.8 kB)\n","Collecting einops>=0.7.0 (from dalle2-pytorch)\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting embedding-reader (from dalle2-pytorch)\n","  Downloading embedding_reader-1.5.1-py3-none-any.whl (18 kB)\n","Collecting kornia>=0.5.4 (from dalle2-pytorch)\n","  Downloading kornia-0.7.0-py2.py3-none-any.whl (705 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.7/705.7 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (23.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (9.4.0)\n","Collecting pydantic>=2 (from dalle2-pytorch)\n","  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-warmup (from dalle2-pytorch)\n","  Downloading pytorch_warmup-0.1.1-py3-none-any.whl (6.6 kB)\n","Collecting resize-right>=0.0.2 (from dalle2-pytorch)\n","  Downloading resize_right-0.0.2-py3-none-any.whl (8.9 kB)\n","Collecting rotary-embedding-torch (from dalle2-pytorch)\n","  Downloading rotary_embedding_torch-0.4.0-py3-none-any.whl (5.1 kB)\n","Collecting vector-quantize-pytorch (from dalle2-pytorch)\n","  Downloading vector_quantize_pytorch-1.12.0-py3-none-any.whl (24 kB)\n","Collecting x-clip>=0.4.4 (from dalle2-pytorch)\n","  Downloading x_clip-0.14.4-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting webdataset>=0.2.5 (from dalle2-pytorch)\n","  Downloading webdataset-0.2.86-py3-none-any.whl (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec>=2022.1.0 in /usr/local/lib/python3.10/dist-packages (from dalle2-pytorch) (2023.6.0)\n","Collecting torchmetrics[image]>=0.8.0 (from dalle2-pytorch)\n","  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting beartype (from ema-pytorch>=0.0.7->dalle2-pytorch)\n","  Downloading beartype-0.16.4-py3-none-any.whl (819 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.1/819.1 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic>=2->dalle2-pytorch)\n","  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n","Collecting pydantic-core==2.14.5 (from pydantic>=2->dalle2-pytorch)\n","  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions>=4.6.1 (from pydantic>=2->dalle2-pytorch)\n","  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (2.1.0)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics[image]>=0.8.0->dalle2-pytorch)\n","  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n","Collecting torch-fidelity<=0.4.0 (from torchmetrics[image]>=0.8.0->dalle2-pytorch)\n","  Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: scipy>1.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics[image]>=0.8.0->dalle2-pytorch) (1.11.4)\n","Collecting braceexpand (from webdataset>=0.2.5->dalle2-pytorch)\n","  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->dalle2-pytorch) (0.4.1)\n","Requirement already satisfied: pandas<2,>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from embedding-reader->dalle2-pytorch) (1.5.3)\n","Requirement already satisfied: pyarrow<13,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from embedding-reader->dalle2-pytorch) (9.0.0)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->open_clip_torch) (0.2.12)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.1.5->embedding-reader->dalle2-pytorch) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.1.5->embedding-reader->dalle2-pytorch) (2023.3.post1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\n","Installing collected packages: sentencepiece, resize-right, braceexpand, webdataset, typing-extensions, smmap, setproctitle, sentry-sdk, ftfy, einops, docker-pycreds, beartype, annotated-types, pydantic-core, lightning-utilities, gitdb, vector-quantize-pytorch, torchmetrics, rotary-embedding-torch, pytorch-warmup, pydantic, kornia, GitPython, embedding-reader, ema-pytorch, coca-pytorch, accelerate, x-clip, wandb, torch-fidelity, timm, clip-anytorch, open_clip_torch, dalle2-pytorch\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 1.10.13\n","    Uninstalling pydantic-1.10.13:\n","      Successfully uninstalled pydantic-1.10.13\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.40 accelerate-0.25.0 annotated-types-0.6.0 beartype-0.16.4 braceexpand-0.1.7 clip-anytorch-2.5.2 coca-pytorch-0.1.0 dalle2-pytorch-1.15.6 docker-pycreds-0.4.0 einops-0.7.0 ema-pytorch-0.3.1 embedding-reader-1.5.1 ftfy-6.1.3 gitdb-4.0.11 kornia-0.7.0 lightning-utilities-0.10.0 open_clip_torch-2.23.0 pydantic-2.5.2 pydantic-core-2.14.5 pytorch-warmup-0.1.1 resize-right-0.0.2 rotary-embedding-torch-0.4.0 sentencepiece-0.1.99 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 timm-0.9.12 torch-fidelity-0.3.0 torchmetrics-1.2.1 typing-extensions-4.9.0 vector-quantize-pytorch-1.12.0 wandb-0.16.1 webdataset-0.2.86 x-clip-0.14.4\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-4h6dvpbx\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-4h6dvpbx\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=fe0c8f7fcc681035e9695f5a6fe66caf0cb1faf0d511c19cc9dd11336b66b4c8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-htxtk5gh/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]}],"source":["!pip install open_clip_torch dalle2-pytorch wandb\n","!pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8628,"status":"ok","timestamp":1702238333558,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"M1WWkMK1g3qg"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import csv\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from PIL import Image\n","from torchvision import transforms\n","from utils import load_image, save_image, encode_img, decode_img, to_PIL\n","from model import Clipper, BrainNetwork, DiffusionPriorNetwork, BrainDiffusionPrior, OpenClipper\n","import clip\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4402,"status":"ok","timestamp":1702238337948,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"f-2g8fXMSdV4","outputId":"ca5212c7-b8d9-479a-d764-765a4d4b3187"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mst1020575\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","!wandb login 72af0b7211cd6c1e899bf475205c6a9df94d43d3\n","wandb.login()\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702238337949,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"Ixzc60AkdPIZ"},"outputs":[],"source":["dataset_path = '../2023-Machine-Learning-Dataset/'\n","training_path = dataset_path + 'subj0{}/training_split/'\n","training_fmri_path = training_path + 'training_fmri/'\n","training_images_path = training_path + 'training_images/'\n","testing_path = dataset_path + 'subj0{}/test_split/test_fmri/'\n","image_infos_path = dataset_path + 'image_infos/subj0{}_infos_train.csv'"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702238337949,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"ZelWK8SCT2_P"},"outputs":[],"source":["class MyDataset(Dataset):\n","  def __init__(self, fmri_data, images_folder, transform=None):\n","    self.fmri_data = fmri_data\n","    self.images_folder = images_folder\n","    self.image_paths = [f\"{images_folder}/{filename}\" for filename in os.listdir(images_folder)]\n","    self.transform = transform\n","\n","  def __len__(self):\n","    return len(self.fmri_data)\n","\n","  def __getitem__(self, idx):\n","    fmri = self.fmri_data[idx]\n","    image_path = self.image_paths[idx]\n","    image = load_image(image_path)\n","\n","    if(self.transform):\n","      image = self.transform(image)\n","\n","    return fmri, image\n","  def get_fmri(self):\n","    return self.fmri_data\n","  def concat(self, target):\n","    self.fmri_data = np.concatenate((self.fmri_data, target.fmri_data), axis=0)\n","    self.image_paths.append(target.image_paths)\n","    return self.fmri_data.shape, len(self.image_paths)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702238337949,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"2WJ_PsR_l9pA"},"outputs":[],"source":["def mixco_nce(preds, targs, temp=0.1, perm=None, betas=None, select=None, distributed=False,\n","              accelerator=None, local_rank=None, bidirectional=True):\n","    brain_clip = (preds @ targs.T)/temp\n","\n","    if perm is not None and betas is not None and select is not None:\n","        probs = torch.diag(betas)\n","        probs[torch.arange(preds.shape[0]).to(preds.device), perm] = 1 - betas\n","\n","        loss = -(brain_clip.log_softmax(-1) * probs).sum(-1).mean()\n","        if bidirectional:\n","            loss2 = -(brain_clip.T.log_softmax(-1) * probs.T).sum(-1).mean()\n","            loss = (loss + loss2)/2\n","        return loss\n","    else:\n","        loss =  F.cross_entropy(brain_clip, torch.arange(brain_clip.shape[0]).to(brain_clip.device))\n","        if bidirectional:\n","            loss2 = F.cross_entropy(brain_clip.T, torch.arange(brain_clip.shape[0]).to(brain_clip.device))\n","            loss = (loss + loss2)/2\n","        return loss\n","def mixco(voxels, beta=0.15, s_thresh=0.5):\n","    perm = torch.randperm(voxels.shape[0])\n","    voxels_shuffle = voxels[perm].to(voxels.device,dtype=voxels.dtype)\n","    betas = torch.distributions.Beta(beta, beta).sample([voxels.shape[0]]).to(voxels.device,dtype=voxels.dtype)\n","    select = (torch.rand(voxels.shape[0]) <= s_thresh).to(voxels.device)\n","    betas_shape = [-1] + [1]*(len(voxels.shape)-1)\n","    voxels[select] = voxels[select] * betas[select].reshape(*betas_shape) + \\\n","        voxels_shuffle[select] * (1 - betas[select]).reshape(*betas_shape)\n","    betas[~select] = 1\n","    return voxels, perm, betas, select\n","def cosine_anneal(start, end, steps):\n","    return end + (start - end)/2 * (1 + torch.cos(torch.pi*torch.arange(steps)/(steps-1)))\n","def soft_clip_loss(preds, targs, temp=0.125):\n","    clip_clip = (targs @ targs.T)/temp\n","    brain_clip = (preds @ targs.T)/temp\n","\n","    loss1 = -(brain_clip.log_softmax(-1) * clip_clip.softmax(-1)).sum(-1).mean()\n","    loss2 = -(brain_clip.T.log_softmax(-1) * clip_clip.softmax(-1)).sum(-1).mean()\n","\n","    loss = (loss1 + loss2)/2\n","    return loss"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":31188,"status":"ok","timestamp":1702238369133,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"yQyS1n8kT74V"},"outputs":[],"source":["transform = transforms.Resize([512, 512])\n","\n","# Load dataset, now only subj01\n","lh = np.load(training_path.format(1) + 'training_fmri/lh_training_fmri.npy')\n","rh = np.load(training_path.format(1) + 'training_fmri/rh_training_fmri.npy')\n","lrh = np.concatenate((lh, rh), axis=1)\n","\n","my_dataset = MyDataset(lrh, training_images_path.format(1), transform=transform)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1702238369133,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"FN9yYqRndPIZ"},"outputs":[],"source":["# pip install open_clip_torch\n","\n","import open_clip\n","from PIL import Image\n","from torchvision import transforms\n","\n","# model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion2b_s32b_b82k')"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1702238369134,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"KcEtvLDEhMkU","outputId":"d25a0e25-baa2-499a-cf37-a1d417eb2470"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21809,"status":"ok","timestamp":1702238390928,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"Oo-qgG73hgO-","outputId":"ed8870d6-0d82-47e2-9f4c-99a35532bc42"},"outputs":[{"name":"stdout","output_type":"stream","text":["ViT-L/14 cuda\n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████| 890M/890M [00:05<00:00, 172MiB/s]\n"]}],"source":["clip_extractor = Clipper(\"ViT-L/14\", device=device).to(device)"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1702242115375,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"9AgsYED0pM3R"},"outputs":[],"source":["clip_size = 768\n","out_dim = clip_size #257*clip_size\n","depth = 6\n","dim_head = 64\n","timesteps = 100\n","heads = clip_size//64\n","epoch = 0\n","losses = []\n","val_losses = []\n","lrs = []"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702242115375,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"-HZFSq79SxUA"},"outputs":[],"source":["batch_size = 16\n","num_epochs = 240\n","num_train = 5000\n","lr_scheduler = 'cycle'\n","initial_lr = 1e-3\n","max_lr = 5e-4\n","random_seed = 42\n","train_size = 0.9\n","valid_size = 1 - train_size\n","num_workers = torch.cuda.device_count() if torch.cuda.device_count()>0 else 1\n","prior_mult = .03\n","mixup_pct = .33\n","soft_loss_temps = cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":1044,"status":"ok","timestamp":1702242120250,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"VDcRllLxqnZg"},"outputs":[],"source":["voxel2clip_kwargs = dict(out_dim=out_dim,clip_size=clip_size,use_projector=True)\n","voxel2clip = BrainNetwork(**voxel2clip_kwargs).to(device)"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":405,"status":"ok","timestamp":1702242120654,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"IimikxtFoyeM"},"outputs":[],"source":["prior_network = DiffusionPriorNetwork(\n","            dim=out_dim,\n","            depth=depth,\n","            dim_head=dim_head,\n","            heads=heads,\n","            causal=False,\n","            num_tokens = 257,\n","            learned_query_mode=\"pos_emb\"\n","        ).to(device)\n","diffusion_prior = BrainDiffusionPrior(\n","        net=prior_network,\n","        image_embed_dim=out_dim,\n","        condition_on_text_encodings=False,\n","        timesteps=timesteps,\n","        cond_drop_prob=0.2,\n","        image_embed_scale=None,\n","        voxel2clip=voxel2clip,\n","    ).to(device)"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410},"executionInfo":{"elapsed":5658,"status":"ok","timestamp":1702242115375,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"mZHrE1trSrRQ","outputId":"a263ac50-43a1-47b5-9ec9-efb92c735d9a"},"outputs":[{"data":{"text/html":["Finishing last run (ID:s820h122) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a82c0c99f244593a1a51d3f469a1a25","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.002 MB of 0.012 MB uploaded\\r'), FloatProgress(value=0.15995040297582144, max=1.…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/loss</td><td>█▄▂▂▂▂▂▂▃▃▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂</td></tr><tr><td>train/loss_mse</td><td>█▄▂▂▂▂▂▂▃▃▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂</td></tr><tr><td>train/lr</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train/num_steps</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/loss</td><td>6.93805</td></tr><tr><td>train/loss_mse</td><td>6.93805</td></tr><tr><td>train/lr</td><td>0.00022</td></tr><tr><td>train/num_steps</td><td>283</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">glorious-breeze-15</strong> at: <a href='https://wandb.ai/st1020575/clipTrain/runs/s820h122' target=\"_blank\">https://wandb.ai/st1020575/clipTrain/runs/s820h122</a><br/> View job at <a href='https://wandb.ai/st1020575/clipTrain/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMjAwOTAwNg==/version_details/v4' target=\"_blank\">https://wandb.ai/st1020575/clipTrain/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyMjAwOTAwNg==/version_details/v4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20231210_205246-s820h122/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:s820h122). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/MindEye/wandb/run-20231210_210141-b7ru2yvp</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/st1020575/clipTrain/runs/b7ru2yvp' target=\"_blank\">eager-microwave-16</a></strong> to <a href='https://wandb.ai/st1020575/clipTrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/st1020575/clipTrain' target=\"_blank\">https://wandb.ai/st1020575/clipTrain</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/st1020575/clipTrain/runs/b7ru2yvp' target=\"_blank\">https://wandb.ai/st1020575/clipTrain/runs/b7ru2yvp</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/st1020575/clipTrain/runs/b7ru2yvp?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a9b0954b8b0>"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"clipTrain\",\n","\n","    # track hyperparameters and run metadata\n","    config={\n","        \"learning_rate\": initial_lr,\n","        \"architecture\": \"MLP\",\n","        \"dataset\": \"NSD\",\n","        \"epochs\": num_epochs,\n","        \"random_seed\": random_seed,\n","        \"train_size\": train_size,\n","        \"valid_size\": valid_size\n","    }\n","    # , resume=True\n","    # , id=\"z36h9ggg\"\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1702238395708,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"uk0yXOfZTNt7"},"outputs":[],"source":["no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","opt_grouped_parameters = [\n","    {'params': [p for n, p in diffusion_prior.net.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n","    {'params': [p for n, p in diffusion_prior.net.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","    {'params': [p for n, p in diffusion_prior.voxel2clip.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n","    {'params': [p for n, p in diffusion_prior.voxel2clip.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1702242122821,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"nGfMp6oKSDOE"},"outputs":[],"source":["optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=initial_lr)\n","lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr,\n","                          total_steps=num_epochs*((num_train//batch_size)//num_workers),\n","                          final_div_factor=1000,\n","                          last_epoch=-1, pct_start=2/num_epochs)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1702238395708,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"YrhBl4y0Ylm1"},"outputs":[],"source":["# train-val split\n","generator = torch.Generator().manual_seed(random_seed)\n","trainset, validset = random_split(my_dataset, [train_size, valid_size], generator=generator)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1702238395708,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"ZxwDAON5Yd1E"},"outputs":[],"source":["# build dataloader\n","train_dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","val_dataloader = DataLoader(validset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1702241582893,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"4aqBSzeMcV4B"},"outputs":[],"source":["# Load model\n","\n","# checkpoint = torch.load(\"./ModelsClip/90\", map_location=device)\n","# epoch = checkpoint['epoch']\n","# loss = checkpoint['loss']\n","# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","# diffusion_prior.load_state_dict(checkpoint['model_state_dict'])\n","# del checkpoint"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702242125022,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"Q248z-pnduRC","outputId":"b2b7cbd0-0b18-4357-abd0-38206edfdd36"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r  0%|                                                                                                                         | 0/240 [00:00<?, ?it/s]"]}],"source":["progress_bar = tqdm(range(epoch, num_epochs), ncols=150)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1702238395709,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"ml5ikhSLFn2A"},"outputs":[],"source":["# testImg = Image.open(f\"{training_images_path.format(1)}/0.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1702238395709,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"EDKPa3w1E578"},"outputs":[],"source":["# for train_i, data in enumerate(train_dataloader):\n","#   voxels, images = data\n","#   optimizer.zero_grad()\n","#   print(voxels.shape,images.shape)\n","#   voxels = voxels.to(device).float()\n","#   images = images.to(device).float()\n","#   clip_voxels, clip_voxels_proj = diffusion_prior.voxel2clip(voxels)\n","#   print(clip_voxels.shape,clip_voxels_proj.shape)\n","#   clip_voxels.view(len(voxels),-1,768)\n","#   print(clip_extractor.embed_image(transforms.functional.to_pil_image(images[0].squeeze(0))).shape)\n","#   break"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1702238395710,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"akYi3cfsQBjE"},"outputs":[],"source":["# from matplotlib import pyplot as plt\n","# plt.imshow(np.transpose(clip_extractor_cpu.preprocess(testImg).numpy(),(1,2,0)))\n","# # plt.imshow(np.transpose(testImg.squeeze(0),(1,2,0)))\n","# plt.show()"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1702238474119,"user":{"displayName":"黃淳侑","userId":"03276885509539470254"},"user_tz":-480},"id":"vIDEIHNDZivV","outputId":"c5c22c92-9c77-4737-984e-77439982e028"},"outputs":[{"data":{"text/plain":["<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7a9975b41cc0>"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["torch.autograd.set_detect_anomaly(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"HeWi2uQrtV0G"},"outputs":[],"source":["for epoch in progress_bar:\n","    diffusion_prior.train()\n","\n","    loss_sum = 0\n","    val_loss_sum = 0\n","\n","    reconst_fails = []\n","    for train_i, data in enumerate(train_dataloader):\n","        voxels, images = data\n","        optimizer.zero_grad()\n","        voxels = voxels.to(device).float()\n","        images = images.to(device).float()\n","        # transforms.functional.to_pil_image(image.squeeze(0))\n","        # print(images)\n","        # print(clip_extractor.embed_image(image[0].to(device)).to(device))\n","        # clip_target = torch.cat([clip_extractor.embed_image(image.to(device)).to(device) for image in images])\n","        if epoch < int(mixup_pct * num_epochs):\n","          voxel, perm, betas, select = mixco(voxels)\n","        clip_target = torch.cat([clip_extractor.embed_image(transforms.functional.to_pil_image(image.squeeze(0))).to(device) for image in images])\n","        clip_voxels, clip_voxels_proj = diffusion_prior.voxel2clip(voxels.clone())\n","\n","        # calulate loss\n","\n","\n","        clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1).float()\n","        clip_voxels_norm = nn.functional.normalize(clip_voxels_proj.flatten(1), dim=-1).float()\n","        if epoch < int(mixup_pct * num_epochs):\n","          loss = mixco_nce(clip_voxels_norm,clip_target_norm,temp=.006,perm=perm, betas=betas, select=select)\n","        else:\n","          epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n","          loss = soft_clip_loss(clip_voxels_norm,clip_target_norm,temp=epoch_temp)\n","        # print(clip_voxels_norm,clip_target_norm.dtype)\n","\n","        # loss = F.mse_loss(clip_voxels_norm, clip_target_norm)\n","\n","        loss_sum += loss.item()\n","        losses.append(loss.item())\n","        lrs.append(optimizer.param_groups[0]['lr'])\n","\n","\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","\n","        logs = {\n","            \"train/loss\": np.mean(losses[-(train_i+1):]),\n","            \"train/lr\": lrs[-1],\n","            \"train/num_steps\": len(losses),\n","            \"train/loss_mse\": loss_sum / (train_i + 1)\n","        }\n","        wandb.log(logs)\n","\n","        progress_bar.set_postfix(**logs)\n","    torch.save({\n","      'epoch': epoch,\n","      'model_state_dict': diffusion_prior.state_dict(),\n","      'optimizer_state_dict': optimizer.state_dict(),\n","      'loss': loss,\n","      }, './ModelsClip/{}'.format(epoch)\n","    )\n","    diffusion_prior.eval()\n","    for val_i, data in enumerate(val_dataloader):\n","        voxels, images = data\n","        voxels = voxels.to(device).float()\n","        images = images.to(device).float()\n","\n","        # encoded_latents = torch.cat([clip_extractor.embed_image(torch.squeeze(image,0)).to(device) for image in images])\n","        # clip_voxels, clip_voxels_proj = diffusion_prior.voxel2clip(voxels)\n","        clip_target = torch.cat([clip_extractor.embed_image(transforms.functional.to_pil_image(image.squeeze(0))).to(device) for image in images])\n","        clip_voxels, clip_voxels_proj = diffusion_prior.voxel2clip(voxels.clone())\n","        clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1).float()\n","        clip_voxels_norm = nn.functional.normalize(clip_voxels_proj.flatten(1), dim=-1).float()\n","        # calulate loss\n","        # loss = F.mse_loss(clip_voxels_norm, clip_target_norm)\n","        if epoch < int(mixup_pct * num_epochs):\n","          loss = mixco_nce(clip_voxels_norm,clip_target_norm,temp=.006,perm=None, betas=None, select=None)\n","        else:\n","          loss = soft_clip_loss(clip_voxels_norm,clip_target_norm,temp=epoch_temp)\n","\n","        val_loss_sum += loss.item()\n","        val_losses.append(loss.item())\n","\n","    logs = {\n","        \"train/loss\": np.mean(losses[-(train_i+1):]),\n","        \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n","        \"train/lr\": lrs[-1],\n","        \"train/num_steps\": len(losses),\n","        \"train/loss_mse\": loss_sum / (train_i + 1),\n","        \"val/loss_mse\": val_loss_sum / (val_i + 1)\n","    }\n","    wandb.log(logs)\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1288a364588644f2bb02ef0d4ae6ee6b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1399037a8a3642ed8e041efd8ae0f5fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a82c0c99f244593a1a51d3f469a1a25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_8dd1bee52d3d4ab191a057b08152d1be","IPY_MODEL_c2e14e6333944647aaa1e5677a34b849"],"layout":"IPY_MODEL_2d6d2b66672e4d4cb624952ac05984ac"}},"2d6d2b66672e4d4cb624952ac05984ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dd1bee52d3d4ab191a057b08152d1be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1399037a8a3642ed8e041efd8ae0f5fd","placeholder":"​","style":"IPY_MODEL_9d4e2f3285c74949990787047d82acc2","value":"0.012 MB of 0.012 MB uploaded\r"}},"9d4e2f3285c74949990787047d82acc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b89cb88ac639493c9303a7bb165a1f40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c2e14e6333944647aaa1e5677a34b849":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1288a364588644f2bb02ef0d4ae6ee6b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b89cb88ac639493c9303a7bb165a1f40","value":1}}}}},"nbformat":4,"nbformat_minor":0}
