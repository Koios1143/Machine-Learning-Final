{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgTxEZ13Xs51",
    "outputId": "f1dba251-cae8-4a86-8a6b-002d71c1411f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[Colab initialization]\\nInitialize Colab in this cell, mount drive and cd to the working directory\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[Colab initialization]\n",
    "Initialize Colab in this cell, mount drive and cd to the working directory\n",
    "\"\"\"\n",
    "\n",
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# %cd '/content/drive/My Drive/MindEye'\n",
    "# os.chdir('/content/drive/My Drive/MindEye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HHkuK63-YIem"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Package import]\n",
    "Import some useful packages here\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from utils import encode_img, GetRoiMaskedLR\n",
    "from Models import Voxel2StableDiffusionModel, MyDataset\n",
    "import torch.nn.functional as F\n",
    "from diffusers.models import AutoencoderKL\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[Select Torch Device]\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Voxel2StableDiffusionModel(\n",
       "  (lin0): Sequential(\n",
       "    (0): Linear(in_features=39548, out_features=2048, bias=False)\n",
       "    (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): SiLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (mlp): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (lin1): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "  (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "  (upsampler): Decoder(\n",
       "    (conv_in): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): LoRACompatibleConv(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (to_q): LoRACompatibleLinear(in_features=256, out_features=256, bias=True)\n",
       "          (to_k): LoRACompatibleLinear(in_features=256, out_features=256, bias=True)\n",
       "          (to_v): LoRACompatibleLinear(in_features=256, out_features=256, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): LoRACompatibleLinear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[Load Low-level Model]\n",
    "\n",
    "Model defined in Models.py\n",
    "\"\"\"\n",
    "voxel2sd = Voxel2StableDiffusionModel()\n",
    "voxel2sd.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 2048]      80,994,304\n",
      "         LayerNorm-2                 [-1, 2048]           4,096\n",
      "              SiLU-3                 [-1, 2048]               0\n",
      "           Dropout-4                 [-1, 2048]               0\n",
      "            Linear-5                 [-1, 2048]       4,194,304\n",
      "         LayerNorm-6                 [-1, 2048]           4,096\n",
      "              SiLU-7                 [-1, 2048]               0\n",
      "           Dropout-8                 [-1, 2048]               0\n",
      "            Linear-9                 [-1, 2048]       4,194,304\n",
      "        LayerNorm-10                 [-1, 2048]           4,096\n",
      "             SiLU-11                 [-1, 2048]               0\n",
      "          Dropout-12                 [-1, 2048]               0\n",
      "           Linear-13                 [-1, 2048]       4,194,304\n",
      "        LayerNorm-14                 [-1, 2048]           4,096\n",
      "             SiLU-15                 [-1, 2048]               0\n",
      "          Dropout-16                 [-1, 2048]               0\n",
      "           Linear-17                 [-1, 2048]       4,194,304\n",
      "        LayerNorm-18                 [-1, 2048]           4,096\n",
      "             SiLU-19                 [-1, 2048]               0\n",
      "          Dropout-20                 [-1, 2048]               0\n",
      "           Linear-21                [-1, 16384]      33,554,432\n",
      "        GroupNorm-22           [-1, 64, 16, 16]             128\n",
      "           Conv2d-23          [-1, 256, 16, 16]         147,712\n",
      "        GroupNorm-24          [-1, 256, 16, 16]             512\n",
      "             SiLU-25          [-1, 256, 16, 16]               0\n",
      "             SiLU-26          [-1, 256, 16, 16]               0\n",
      "             SiLU-27          [-1, 256, 16, 16]               0\n",
      "             SiLU-28          [-1, 256, 16, 16]               0\n",
      "             SiLU-29          [-1, 256, 16, 16]               0\n",
      "             SiLU-30          [-1, 256, 16, 16]               0\n",
      "             SiLU-31          [-1, 256, 16, 16]               0\n",
      "             SiLU-32          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-33          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-34          [-1, 256, 16, 16]             512\n",
      "             SiLU-35          [-1, 256, 16, 16]               0\n",
      "             SiLU-36          [-1, 256, 16, 16]               0\n",
      "             SiLU-37          [-1, 256, 16, 16]               0\n",
      "             SiLU-38          [-1, 256, 16, 16]               0\n",
      "             SiLU-39          [-1, 256, 16, 16]               0\n",
      "             SiLU-40          [-1, 256, 16, 16]               0\n",
      "             SiLU-41          [-1, 256, 16, 16]               0\n",
      "             SiLU-42          [-1, 256, 16, 16]               0\n",
      "          Dropout-43          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-44          [-1, 256, 16, 16]         590,080\n",
      "    ResnetBlock2D-45          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-46             [-1, 256, 256]             512\n",
      "LoRACompatibleLinear-47             [-1, 256, 256]          65,792\n",
      "LoRACompatibleLinear-48             [-1, 256, 256]          65,792\n",
      "LoRACompatibleLinear-49             [-1, 256, 256]          65,792\n",
      "LoRACompatibleLinear-50             [-1, 256, 256]          65,792\n",
      "          Dropout-51             [-1, 256, 256]               0\n",
      "        Attention-52          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-53          [-1, 256, 16, 16]             512\n",
      "             SiLU-54          [-1, 256, 16, 16]               0\n",
      "             SiLU-55          [-1, 256, 16, 16]               0\n",
      "             SiLU-56          [-1, 256, 16, 16]               0\n",
      "             SiLU-57          [-1, 256, 16, 16]               0\n",
      "             SiLU-58          [-1, 256, 16, 16]               0\n",
      "             SiLU-59          [-1, 256, 16, 16]               0\n",
      "             SiLU-60          [-1, 256, 16, 16]               0\n",
      "             SiLU-61          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-62          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-63          [-1, 256, 16, 16]             512\n",
      "             SiLU-64          [-1, 256, 16, 16]               0\n",
      "             SiLU-65          [-1, 256, 16, 16]               0\n",
      "             SiLU-66          [-1, 256, 16, 16]               0\n",
      "             SiLU-67          [-1, 256, 16, 16]               0\n",
      "             SiLU-68          [-1, 256, 16, 16]               0\n",
      "             SiLU-69          [-1, 256, 16, 16]               0\n",
      "             SiLU-70          [-1, 256, 16, 16]               0\n",
      "             SiLU-71          [-1, 256, 16, 16]               0\n",
      "          Dropout-72          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-73          [-1, 256, 16, 16]         590,080\n",
      "    ResnetBlock2D-74          [-1, 256, 16, 16]               0\n",
      "   UNetMidBlock2D-75          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-76          [-1, 256, 16, 16]             512\n",
      "             SiLU-77          [-1, 256, 16, 16]               0\n",
      "             SiLU-78          [-1, 256, 16, 16]               0\n",
      "             SiLU-79          [-1, 256, 16, 16]               0\n",
      "             SiLU-80          [-1, 256, 16, 16]               0\n",
      "             SiLU-81          [-1, 256, 16, 16]               0\n",
      "             SiLU-82          [-1, 256, 16, 16]               0\n",
      "             SiLU-83          [-1, 256, 16, 16]               0\n",
      "             SiLU-84          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-85          [-1, 256, 16, 16]         590,080\n",
      "        GroupNorm-86          [-1, 256, 16, 16]             512\n",
      "             SiLU-87          [-1, 256, 16, 16]               0\n",
      "             SiLU-88          [-1, 256, 16, 16]               0\n",
      "             SiLU-89          [-1, 256, 16, 16]               0\n",
      "             SiLU-90          [-1, 256, 16, 16]               0\n",
      "             SiLU-91          [-1, 256, 16, 16]               0\n",
      "             SiLU-92          [-1, 256, 16, 16]               0\n",
      "             SiLU-93          [-1, 256, 16, 16]               0\n",
      "             SiLU-94          [-1, 256, 16, 16]               0\n",
      "          Dropout-95          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-96          [-1, 256, 16, 16]         590,080\n",
      "    ResnetBlock2D-97          [-1, 256, 16, 16]               0\n",
      "        GroupNorm-98          [-1, 256, 16, 16]             512\n",
      "             SiLU-99          [-1, 256, 16, 16]               0\n",
      "            SiLU-100          [-1, 256, 16, 16]               0\n",
      "            SiLU-101          [-1, 256, 16, 16]               0\n",
      "            SiLU-102          [-1, 256, 16, 16]               0\n",
      "            SiLU-103          [-1, 256, 16, 16]               0\n",
      "            SiLU-104          [-1, 256, 16, 16]               0\n",
      "            SiLU-105          [-1, 256, 16, 16]               0\n",
      "            SiLU-106          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-107          [-1, 256, 16, 16]         590,080\n",
      "       GroupNorm-108          [-1, 256, 16, 16]             512\n",
      "            SiLU-109          [-1, 256, 16, 16]               0\n",
      "            SiLU-110          [-1, 256, 16, 16]               0\n",
      "            SiLU-111          [-1, 256, 16, 16]               0\n",
      "            SiLU-112          [-1, 256, 16, 16]               0\n",
      "            SiLU-113          [-1, 256, 16, 16]               0\n",
      "            SiLU-114          [-1, 256, 16, 16]               0\n",
      "            SiLU-115          [-1, 256, 16, 16]               0\n",
      "            SiLU-116          [-1, 256, 16, 16]               0\n",
      "         Dropout-117          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-118          [-1, 256, 16, 16]         590,080\n",
      "   ResnetBlock2D-119          [-1, 256, 16, 16]               0\n",
      "LoRACompatibleConv-120          [-1, 256, 32, 32]         590,080\n",
      "      Upsample2D-121          [-1, 256, 32, 32]               0\n",
      "UpDecoderBlock2D-122          [-1, 256, 32, 32]               0\n",
      "       GroupNorm-123          [-1, 256, 32, 32]             512\n",
      "            SiLU-124          [-1, 256, 32, 32]               0\n",
      "            SiLU-125          [-1, 256, 32, 32]               0\n",
      "            SiLU-126          [-1, 256, 32, 32]               0\n",
      "            SiLU-127          [-1, 256, 32, 32]               0\n",
      "            SiLU-128          [-1, 256, 32, 32]               0\n",
      "            SiLU-129          [-1, 256, 32, 32]               0\n",
      "            SiLU-130          [-1, 256, 32, 32]               0\n",
      "            SiLU-131          [-1, 256, 32, 32]               0\n",
      "LoRACompatibleConv-132          [-1, 128, 32, 32]         295,040\n",
      "       GroupNorm-133          [-1, 128, 32, 32]             256\n",
      "            SiLU-134          [-1, 128, 32, 32]               0\n",
      "            SiLU-135          [-1, 128, 32, 32]               0\n",
      "            SiLU-136          [-1, 128, 32, 32]               0\n",
      "            SiLU-137          [-1, 128, 32, 32]               0\n",
      "            SiLU-138          [-1, 128, 32, 32]               0\n",
      "            SiLU-139          [-1, 128, 32, 32]               0\n",
      "            SiLU-140          [-1, 128, 32, 32]               0\n",
      "            SiLU-141          [-1, 128, 32, 32]               0\n",
      "         Dropout-142          [-1, 128, 32, 32]               0\n",
      "LoRACompatibleConv-143          [-1, 128, 32, 32]         147,584\n",
      "LoRACompatibleConv-144          [-1, 128, 32, 32]          32,896\n",
      "   ResnetBlock2D-145          [-1, 128, 32, 32]               0\n",
      "       GroupNorm-146          [-1, 128, 32, 32]             256\n",
      "            SiLU-147          [-1, 128, 32, 32]               0\n",
      "            SiLU-148          [-1, 128, 32, 32]               0\n",
      "            SiLU-149          [-1, 128, 32, 32]               0\n",
      "            SiLU-150          [-1, 128, 32, 32]               0\n",
      "            SiLU-151          [-1, 128, 32, 32]               0\n",
      "            SiLU-152          [-1, 128, 32, 32]               0\n",
      "            SiLU-153          [-1, 128, 32, 32]               0\n",
      "            SiLU-154          [-1, 128, 32, 32]               0\n",
      "LoRACompatibleConv-155          [-1, 128, 32, 32]         147,584\n",
      "       GroupNorm-156          [-1, 128, 32, 32]             256\n",
      "            SiLU-157          [-1, 128, 32, 32]               0\n",
      "            SiLU-158          [-1, 128, 32, 32]               0\n",
      "            SiLU-159          [-1, 128, 32, 32]               0\n",
      "            SiLU-160          [-1, 128, 32, 32]               0\n",
      "            SiLU-161          [-1, 128, 32, 32]               0\n",
      "            SiLU-162          [-1, 128, 32, 32]               0\n",
      "            SiLU-163          [-1, 128, 32, 32]               0\n",
      "            SiLU-164          [-1, 128, 32, 32]               0\n",
      "         Dropout-165          [-1, 128, 32, 32]               0\n",
      "LoRACompatibleConv-166          [-1, 128, 32, 32]         147,584\n",
      "   ResnetBlock2D-167          [-1, 128, 32, 32]               0\n",
      "LoRACompatibleConv-168          [-1, 128, 64, 64]         147,584\n",
      "      Upsample2D-169          [-1, 128, 64, 64]               0\n",
      "UpDecoderBlock2D-170          [-1, 128, 64, 64]               0\n",
      "       GroupNorm-171          [-1, 128, 64, 64]             256\n",
      "            SiLU-172          [-1, 128, 64, 64]               0\n",
      "            SiLU-173          [-1, 128, 64, 64]               0\n",
      "            SiLU-174          [-1, 128, 64, 64]               0\n",
      "            SiLU-175          [-1, 128, 64, 64]               0\n",
      "            SiLU-176          [-1, 128, 64, 64]               0\n",
      "            SiLU-177          [-1, 128, 64, 64]               0\n",
      "            SiLU-178          [-1, 128, 64, 64]               0\n",
      "            SiLU-179          [-1, 128, 64, 64]               0\n",
      "LoRACompatibleConv-180           [-1, 64, 64, 64]          73,792\n",
      "       GroupNorm-181           [-1, 64, 64, 64]             128\n",
      "            SiLU-182           [-1, 64, 64, 64]               0\n",
      "            SiLU-183           [-1, 64, 64, 64]               0\n",
      "            SiLU-184           [-1, 64, 64, 64]               0\n",
      "            SiLU-185           [-1, 64, 64, 64]               0\n",
      "            SiLU-186           [-1, 64, 64, 64]               0\n",
      "            SiLU-187           [-1, 64, 64, 64]               0\n",
      "            SiLU-188           [-1, 64, 64, 64]               0\n",
      "            SiLU-189           [-1, 64, 64, 64]               0\n",
      "         Dropout-190           [-1, 64, 64, 64]               0\n",
      "LoRACompatibleConv-191           [-1, 64, 64, 64]          36,928\n",
      "LoRACompatibleConv-192           [-1, 64, 64, 64]           8,256\n",
      "   ResnetBlock2D-193           [-1, 64, 64, 64]               0\n",
      "       GroupNorm-194           [-1, 64, 64, 64]             128\n",
      "            SiLU-195           [-1, 64, 64, 64]               0\n",
      "            SiLU-196           [-1, 64, 64, 64]               0\n",
      "            SiLU-197           [-1, 64, 64, 64]               0\n",
      "            SiLU-198           [-1, 64, 64, 64]               0\n",
      "            SiLU-199           [-1, 64, 64, 64]               0\n",
      "            SiLU-200           [-1, 64, 64, 64]               0\n",
      "            SiLU-201           [-1, 64, 64, 64]               0\n",
      "            SiLU-202           [-1, 64, 64, 64]               0\n",
      "LoRACompatibleConv-203           [-1, 64, 64, 64]          36,928\n",
      "       GroupNorm-204           [-1, 64, 64, 64]             128\n",
      "            SiLU-205           [-1, 64, 64, 64]               0\n",
      "            SiLU-206           [-1, 64, 64, 64]               0\n",
      "            SiLU-207           [-1, 64, 64, 64]               0\n",
      "            SiLU-208           [-1, 64, 64, 64]               0\n",
      "            SiLU-209           [-1, 64, 64, 64]               0\n",
      "            SiLU-210           [-1, 64, 64, 64]               0\n",
      "            SiLU-211           [-1, 64, 64, 64]               0\n",
      "            SiLU-212           [-1, 64, 64, 64]               0\n",
      "         Dropout-213           [-1, 64, 64, 64]               0\n",
      "LoRACompatibleConv-214           [-1, 64, 64, 64]          36,928\n",
      "   ResnetBlock2D-215           [-1, 64, 64, 64]               0\n",
      "UpDecoderBlock2D-216           [-1, 64, 64, 64]               0\n",
      "       GroupNorm-217           [-1, 64, 64, 64]             128\n",
      "            SiLU-218           [-1, 64, 64, 64]               0\n",
      "          Conv2d-219            [-1, 4, 64, 64]           2,308\n",
      "         Decoder-220            [-1, 4, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 138,188,228\n",
      "Trainable params: 138,188,228\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.15\n",
      "Forward/backward pass size (MB): 235.31\n",
      "Params size (MB): 527.15\n",
      "Estimated Total Size (MB): 762.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[Summary of Model]\n",
    "\n",
    "Our input is a (39548, ) vector, we can see summary in this cell\n",
    "\"\"\"\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(voxel2sd, (39548,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Hyperparameters]\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 200\n",
    "num_train = 5000\n",
    "lr_scheduler = 'cycle'\n",
    "initial_lr = 1e-4\n",
    "max_lr = 5e-4\n",
    "train_size = 0.7\n",
    "valid_size = 1 - train_size\n",
    "num_workers = torch.cuda.device_count()\n",
    "\n",
    "# We normally only modify the following hyperparameters\n",
    "random_seed = 42\n",
    "ROI_num = 1 # For ROI Mask. [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Path information]\n",
    "\"\"\"\n",
    "\n",
    "dataset_path = '../dataset/'\n",
    "training_path = dataset_path + 'subj0{}/training_split/'\n",
    "training_fmri_path = training_path + 'training_fmri/'\n",
    "training_images_path = training_path + 'training_images/'\n",
    "testing_path = dataset_path + 'subj0{}/test_split/test_fmri/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[ROI Mask]\n",
    "Perform ROI Mask on subject's fMRI data.\n",
    "We have selected three sets of ROI Masks\n",
    "\n",
    "Note: In this implementation, we only consider subject 1\n",
    "\"\"\"\n",
    "\n",
    "lrh = GetRoiMaskedLR(ROI_num, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Load Training Dataset and Data Preprocessing]\n",
    "Load training dataset, split dataset into train and validation set\n",
    "Finally, build a dataloader for convenience\n",
    "\"\"\"\n",
    "\n",
    "transform = transforms.Resize([512, 512])\n",
    "my_dataset = MyDataset(lrh, training_images_path.format(1), transform=transform)\n",
    "\n",
    "# train-val split\n",
    "generator = torch.Generator().manual_seed(random_seed)\n",
    "trainset, validset = random_split(my_dataset, [train_size, valid_size], generator=generator)\n",
    "\n",
    "# build dataloader\n",
    "train_dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(validset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Optimizer Initialization]\n",
    "\n",
    "Here we choose OneCycleLR\n",
    "\"\"\"\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in voxel2sd.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in voxel2sd.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=initial_lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr,\n",
    "                                            total_steps=num_epochs*((num_train//batch_size)//num_workers),\n",
    "                                            final_div_factor=1000,\n",
    "                                            last_epoch=-1, pct_start=2/num_epochs)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "#                                                    milestones=[50*i for i in range(num_epochs*((num_train//batch_size)//num_workers//50))],\n",
    "#                                                    gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Load VAE]\n",
    "\"\"\"\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkoioslin\u001b[0m (\u001b[33mtkoioslin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/User/Desktop/Machine-Learning-Final/src/wandb/run-20240113_234115-qiybihi7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tkoioslin/MindEye/runs/qiybihi7' target=\"_blank\">vital-cosmos-105</a></strong> to <a href='https://wandb.ai/tkoioslin/MindEye' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tkoioslin/MindEye' target=\"_blank\">https://wandb.ai/tkoioslin/MindEye</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tkoioslin/MindEye/runs/qiybihi7' target=\"_blank\">https://wandb.ai/tkoioslin/MindEye/runs/qiybihi7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tkoioslin/MindEye/runs/qiybihi7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f010bae6080>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[Wandb Initialization]\n",
    "\n",
    "Login first, then record some information\n",
    "\"\"\"\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"MindEye\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": initial_lr,\n",
    "        \"architecture\": \"MLP\",\n",
    "        \"dataset\": \"NSD\",\n",
    "        \"epochs\": num_epochs,\n",
    "        \"random_seed\": random_seed,\n",
    "        \"train_size\": train_size,\n",
    "        \"valid_size\": valid_size\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                         | 0/200 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[Recording Variables]\n",
    "\n",
    "Record epoch, loss, leaning rates duing training.\n",
    "\"\"\"\n",
    "\n",
    "epoch = 0\n",
    "steps = 0\n",
    "losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "progress_bar = tqdm(range(epoch, num_epochs), ncols=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[Resume Training from Pretrained Model]\\n\\nIf you have a model trained before, you may use this cell\\nto resume the training process\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[Resume Training from Pretrained Model]\n",
    "\n",
    "If you have a model trained before, you may use this cell\n",
    "to resume the training process\n",
    "\"\"\"\n",
    "\n",
    "# checkpoint = torch.load('./Models/100') # [TODO] 修改 Model 編號\n",
    "# voxel2sd.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch'] + 1\n",
    "# loss = checkpoint['loss']\n",
    "# steps = checkpoint['steps'] + 1\n",
    "\n",
    "# progress_bar = tqdm(range(epoch, num_epochs), ncols=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Training Process]\n",
    "\n",
    "Training Process will starts from here\n",
    "\"\"\"\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    voxel2sd.train()\n",
    "\n",
    "    loss_sum = 0\n",
    "    val_loss_sum = 0\n",
    "\n",
    "    reconst_fails = []\n",
    "\n",
    "    for train_i, data in enumerate(train_dataloader):\n",
    "        voxels, images = data\n",
    "        voxels = voxels.to(device).float()\n",
    "        images = images.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # run image encoder\n",
    "        encoded_latents = torch.cat([encode_img(image, vae).to(device) for image in images])\n",
    "        # MLP forward\n",
    "        encoded_predict = voxel2sd(voxels)\n",
    "        # calulate loss\n",
    "        loss = F.l1_loss(encoded_predict, encoded_latents)\n",
    "        loss_sum += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        steps += 1\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "            \"train/lr\": lrs[-1],\n",
    "            \"train/num_steps\": steps,\n",
    "            \"train/loss_mse\": loss_sum / (train_i + 1)\n",
    "        }\n",
    "        wandb.log(logs)\n",
    "\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "    # After training one epoch, evaluation\n",
    "    voxel2sd.eval()\n",
    "    for val_i, data in enumerate(val_dataloader):\n",
    "        voxels, images = data\n",
    "        voxels = voxels.to(device).float()\n",
    "        images = images.to(device).float()\n",
    "\n",
    "        # run image encoder\n",
    "        encoded_latents = torch.cat([encode_img(image, vae).to(device) for image in images])\n",
    "        # MLP forward\n",
    "        encoded_predict = voxel2sd(voxels)\n",
    "        # calulate loss\n",
    "        loss = F.l1_loss(encoded_predict, encoded_latents)\n",
    "        val_loss_sum += loss.item()\n",
    "        val_losses.append(loss.item())\n",
    "\n",
    "    # Print results\n",
    "    logs = {\n",
    "        \"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "        \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
    "        \"train/lr\": lrs[-1],\n",
    "        \"train/num_steps\": steps,\n",
    "        \"train/loss_mse\": loss_sum / (train_i + 1),\n",
    "        \"val/loss_mse\": val_loss_sum / (val_i + 1)\n",
    "    }\n",
    "    wandb.log(logs)\n",
    "\n",
    "    # save ckpt first\n",
    "    print('saving model')\n",
    "    torch.save({\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': voxel2sd.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'loss': loss,\n",
    "      'steps': steps,\n",
    "      }, '../Models/{}'.format(epoch)\n",
    "    )\n",
    "    print('model saved')\n",
    "    \n",
    "\n",
    "    # print(logs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
