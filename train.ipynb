{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43028da8-49d0-4aed-a48f-47f191d850f7",
      "metadata": {
        "id": "43028da8-49d0-4aed-a48f-47f191d850f7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from utils import load_image, save_image, encode_img, decode_img, to_PIL\n",
        "import torch.nn.functional as F\n",
        "from diffusers.models.vae import Decoder\n",
        "from diffusers.models import AutoencoderKL\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efee3611-d037-4a9a-ac24-d5d74c10af34",
      "metadata": {
        "id": "efee3611-d037-4a9a-ac24-d5d74c10af34"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "424a158b-b0d9-4f90-abf6-f901574f6579",
      "metadata": {
        "id": "424a158b-b0d9-4f90-abf6-f901574f6579"
      },
      "outputs": [],
      "source": [
        "class Voxel2StableDiffusionModel(torch.nn.Module):\n",
        "    # define the prototype of the module\n",
        "    def __init__(self, in_dim=39548, h=2048, n_blocks=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin0 = nn.Sequential(\n",
        "            nn.Linear(in_dim, h, bias=False),\n",
        "            nn.LayerNorm(h),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.mlp = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(h, h, bias=False),\n",
        "                nn.LayerNorm(h),\n",
        "                nn.SiLU(inplace=True),\n",
        "                nn.Dropout(0.25),\n",
        "            ) for _ in range(n_blocks)\n",
        "        ])\n",
        "\n",
        "        self.lin1 = nn.Linear(h, 16384, bias=False)\n",
        "        self.norm = nn.GroupNorm(1, 64)\n",
        "\n",
        "        self.upsampler = Decoder(\n",
        "            in_channels=64,\n",
        "            out_channels=4,\n",
        "            up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
        "            block_out_channels=[64, 128, 256],\n",
        "            layers_per_block=1,\n",
        "        )\n",
        "\n",
        "    # define how it forward, using the module defined above\n",
        "    def forward(self, x):\n",
        "        x = self.lin0(x)\n",
        "        residual = x\n",
        "        for res_block in self.mlp:\n",
        "            x = res_block(x)\n",
        "            x = x + residual\n",
        "            residual = x\n",
        "        x = x.reshape(len(x), -1)\n",
        "        x = self.lin1(x)\n",
        "        x = self.norm(x.reshape(x.shape[0], -1, 16, 16).contiguous())\n",
        "        return self.upsampler(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "815c8614-d28c-4635-8724-2defd9e861bd",
      "metadata": {
        "id": "815c8614-d28c-4635-8724-2defd9e861bd"
      },
      "outputs": [],
      "source": [
        "voxel2sd = Voxel2StableDiffusionModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80a7763e-d1df-42be-8759-2238b2c21b53",
      "metadata": {
        "id": "80a7763e-d1df-42be-8759-2238b2c21b53"
      },
      "outputs": [],
      "source": [
        "voxel2sd.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SjcJ9_Lf0wgW",
      "metadata": {
        "id": "SjcJ9_Lf0wgW"
      },
      "outputs": [],
      "source": [
        "# from torchsummary import summary\n",
        "# device = 'cuda'\n",
        "# summary(voxel2sd.to(device), input_size=(39548,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4cfaf78-7e5f-43ff-9552-6c64e39ff85f",
      "metadata": {
        "id": "e4cfaf78-7e5f-43ff-9552-6c64e39ff85f"
      },
      "outputs": [],
      "source": [
        "# some hyperparameters\n",
        "batch_size = 32\n",
        "num_epochs = 120\n",
        "num_train = 5000\n",
        "lr_scheduler = 'cycle'\n",
        "initial_lr = 1e-3\n",
        "max_lr = 5e-4\n",
        "random_seed = 42\n",
        "train_size = 0.7\n",
        "valid_size = 1 - train_size\n",
        "num_workers = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2150bff4-ebbe-40f9-b68a-01fab80b7092",
      "metadata": {
        "id": "2150bff4-ebbe-40f9-b68a-01fab80b7092"
      },
      "outputs": [],
      "source": [
        "# some path information\n",
        "dataset_path = '../2023-Machine-Learning-Dataset/'\n",
        "training_path = dataset_path + 'subj0{}/training_split/'\n",
        "training_fmri_path = training_path + 'training_fmri/'\n",
        "training_images_path = training_path + 'training_images/'\n",
        "testing_path = dataset_path + 'subj0{}/test_split/test_fmri/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0-O8h7XutUle",
      "metadata": {
        "id": "0-O8h7XutUle"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, fmri_data, images_folder, transform=None):\n",
        "    self.fmri_data = fmri_data\n",
        "    self.images_folder = images_folder\n",
        "    self.image_paths = [f\"{images_folder}/{filename}\" for filename in os.listdir(images_folder)]\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.fmri_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    fmri = self.fmri_data[idx]\n",
        "    image_path = self.image_paths[idx]\n",
        "    image = load_image(image_path)\n",
        "\n",
        "    if(self.transform):\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return fmri, image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bDKcG8MyvAMH",
      "metadata": {
        "id": "bDKcG8MyvAMH"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Resize([512, 512])\n",
        "\n",
        "# Load dataset, now only subj01\n",
        "lh = np.load(training_path.format(1) + 'training_fmri/lh_training_fmri.npy')\n",
        "rh = np.load(training_path.format(1) + 'training_fmri/rh_training_fmri.npy')\n",
        "lrh = np.concatenate((lh, rh), axis=1)\n",
        "\n",
        "my_dataset = MyDataset(lrh, training_images_path.format(1), transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54329127-9841-412f-a71d-7c49a0145ee4",
      "metadata": {
        "id": "54329127-9841-412f-a71d-7c49a0145ee4"
      },
      "outputs": [],
      "source": [
        "# train-val split\n",
        "generator = torch.Generator().manual_seed(random_seed)\n",
        "trainset, validset = random_split(my_dataset, [train_size, valid_size], generator=generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "726a9f00-a8f1-41a4-a722-4d4c39a894b7",
      "metadata": {
        "id": "726a9f00-a8f1-41a4-a722-4d4c39a894b7"
      },
      "outputs": [],
      "source": [
        "# build dataloader\n",
        "train_dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "val_dataloader = DataLoader(validset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e418e276-7846-43e5-91b5-f011c4c71e5b",
      "metadata": {
        "id": "e418e276-7846-43e5-91b5-f011c4c71e5b"
      },
      "outputs": [],
      "source": [
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "opt_grouped_parameters = [\n",
        "    {'params': [p for n, p in voxel2sd.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
        "    {'params': [p for n, p in voxel2sd.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a888d3d-81c7-407d-8a48-e6c3d46dc973",
      "metadata": {
        "id": "4a888d3d-81c7-407d-8a48-e6c3d46dc973"
      },
      "outputs": [],
      "source": [
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503d8367-5736-4cca-8dae-c6bad94612b6",
      "metadata": {
        "id": "503d8367-5736-4cca-8dae-c6bad94612b6"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=1e-3)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr,\n",
        "                                            total_steps=num_epochs*((num_train//batch_size)//num_workers),\n",
        "                                            final_div_factor=1000,\n",
        "                                            last_epoch=-1, pct_start=2/num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d533bac-9248-4da5-acf6-aab09514a31c",
      "metadata": {
        "id": "6d533bac-9248-4da5-acf6-aab09514a31c"
      },
      "outputs": [],
      "source": [
        "epoch = 0\n",
        "progress_bar = tqdm(range(epoch, num_epochs), ncols=150)\n",
        "losses = []\n",
        "val_losses = []\n",
        "lrs = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd77dc3b-dfb3-4ab3-888f-6588dd584631",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dd77dc3b-dfb3-4ab3-888f-6588dd584631"
      },
      "outputs": [],
      "source": [
        "for epoch in progress_bar:\n",
        "    voxel2sd.train()\n",
        "\n",
        "    loss_sum = 0\n",
        "    val_loss_sum = 0\n",
        "\n",
        "    reconst_fails = []\n",
        "\n",
        "    for train_i, data in enumerate(train_dataloader):\n",
        "        voxels, images = data\n",
        "        voxels = voxels.to(device).float()\n",
        "        images = images.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # run image encoder\n",
        "        encoded_latents = torch.cat([encode_img(image, vae).to(device) for image in images])\n",
        "        # MLP forward\n",
        "        encoded_predict = voxel2sd(voxels)\n",
        "        # calulate loss\n",
        "        loss = F.mse_loss(encoded_predict, encoded_latents)\n",
        "        loss_sum += loss.item()\n",
        "        losses.append(loss.item())\n",
        "        lrs.append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        # backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        logs = {\n",
        "            \"train/loss\": np.mean(losses[-(train_i+1):]),\n",
        "            \"train/lr\": lrs[-1],\n",
        "            \"train/num_steps\": len(losses),\n",
        "            \"train/loss_mse\": loss_sum / (train_i + 1)\n",
        "        }\n",
        "\n",
        "        progress_bar.set_postfix(**logs)\n",
        "\n",
        "    # After training one epoch, evaluation\n",
        "    # save ckpt first\n",
        "    torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': voxel2sd.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "      }, './Models/{}'.format(epoch)\n",
        "    )\n",
        "\n",
        "    # Then evaluate\n",
        "    voxel2sd.eval()\n",
        "    for val_i, data in enumerate(val_dataloader):\n",
        "        voxels, images = data\n",
        "        voxels = voxels.to(device).float()\n",
        "        images = images.to(device).float()\n",
        "\n",
        "        # run image encoder\n",
        "        encoded_latents = torch.cat([encode_img(image, vae).to(device) for image in images])\n",
        "        # MLP forward\n",
        "        encoded_predict = voxel2sd(voxels)\n",
        "        # calulate loss\n",
        "        loss = F.mse_loss(encoded_predict, encoded_latents)\n",
        "        val_loss_sum += loss.item()\n",
        "        val_losses.append(loss.item())\n",
        "\n",
        "    # Print results\n",
        "    logs = {\n",
        "        \"train/loss\": np.mean(losses[-(train_i+1):]),\n",
        "        \"val/loss\": np.mean(val_losses[-(val_i+1):]),\n",
        "        \"train/lr\": lrs[-1],\n",
        "        \"train/num_steps\": len(losses),\n",
        "        \"train/loss_mse\": loss_sum / (train_i + 1),\n",
        "        \"val/loss_mse\": val_loss_sum / (val_i + 1)\n",
        "    }\n",
        "\n",
        "    # print(logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hcmBwB_kzmcc",
      "metadata": {
        "id": "hcmBwB_kzmcc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
