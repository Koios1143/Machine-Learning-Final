{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HHkuK63-YIem"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Package import]\n",
    "Import some useful packages here\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from utils import encode_img, GetRoiMaskedLR\n",
    "from Models import Voxel2StableDiffusionModel, MyDataset\n",
    "import torch.nn.functional as F\n",
    "from diffusers.models import AutoencoderKL\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[Select Torch Device]\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Hyperparameters]\n",
    "\"\"\"\n",
    "\n",
    "train_size = 0.7\n",
    "valid_size = 1 - train_size\n",
    "num_workers = torch.cuda.device_count()\n",
    "\n",
    "# We normally only modify the following hyperparameters\n",
    "random_seed = 42\n",
    "ROI_num = 1 # For ROI Mask. [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Path information]\n",
    "\"\"\"\n",
    "\n",
    "dataset_path = '../dataset/'\n",
    "training_path = dataset_path + 'subj0{}/training_split/'\n",
    "training_fmri_path = training_path + 'training_fmri/'\n",
    "training_images_path = training_path + 'training_images/'\n",
    "training_VAE_laten_path = training_path + 'fMRI_VAE.npy'\n",
    "testing_path = dataset_path + 'subj0{}/test_split/test_fmri/'\n",
    "\n",
    "\"\"\"\n",
    "[Load VAE]\n",
    "\"\"\"\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)\n",
    "\n",
    "\"\"\"\n",
    "[ROI-Masks]\n",
    "\"\"\"\n",
    "\n",
    "ROIs_test1 = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"FFA-1\", \"FFA-2\", \"PPA\"]\n",
    "ROIs_test2 = [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\"]\n",
    "ROIs_test3 = [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]\n",
    "ROIs_tests = [0, ROIs_test1, ROIs_test2, ROIs_test3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_to_CLIP(image):\n",
    "    \"\"\"\n",
    "    Project a image to CLIP space by CLIP model.\n",
    "\n",
    "    Parameters:\n",
    "    - image: Image to project.\n",
    "\n",
    "    Returns:\n",
    "    - Projected CLIP latent\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(image).unsqueeze(0).to(device)\n",
    "        features = model.encode_image(image)\n",
    "    return features\n",
    "\n",
    "def similarity(image1_features, image2_features):\n",
    "    \"\"\"\n",
    "    Calculate similarity of two images by cosine similarity\n",
    "\n",
    "    Parameters:\n",
    "    - image1_features: Feature of 1st image\n",
    "    - image2_features: Feature of 2nd image\n",
    "\n",
    "    Returns:\n",
    "    - Similarity of two images\n",
    "    \"\"\"\n",
    "\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "    return cos(image1_features,image2_features).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model ../Models/92/199\n",
      "Average loss on training set: 0.3989420733668587\n",
      "Average loss on validation set: 0.6483591025074323\n",
      "Processing model ../Models/94/199\n",
      "Average loss on training set: 0.39130232659253206\n",
      "Average loss on validation set: 0.659694088002046\n",
      "Processing model ../Models/97/199\n",
      "Average loss on training set: 0.4028529248454354\n",
      "Average loss on validation set: 0.6492126733064651\n",
      "Processing model ../Models/100/199\n",
      "Average loss on training set: 0.370662676746195\n",
      "Average loss on validation set: 0.6589636156956354\n"
     ]
    }
   ],
   "source": [
    "model_paths = ['../Models/92/199', '../Models/94/199', '../Models/97/199', '../Models/100/199']\n",
    "select_ROIs = [3, 1, 3, 3]\n",
    "random_seeds = [3, 2423, 849, 1921]\n",
    "voxel2sd = Voxel2StableDiffusionModel().to(device)\n",
    "train_losses = {}\n",
    "train_loss_avgs = {}\n",
    "valid_losses = {}\n",
    "valid_loss_avgs = {}\n",
    "\n",
    "for model_idx in range(4):\n",
    "    print(f'Processing model {model_paths[model_idx]}')\n",
    "    voxel2sd.load_state_dict(torch.load(model_paths[model_idx])['model_state_dict'])\n",
    "    voxel2sd.eval()\n",
    "    select_ROI = select_ROIs[model_idx]\n",
    "    random_seed = random_seeds[model_idx]\n",
    "\n",
    "    # Build data loader\n",
    "    transform = transforms.Resize([512, 512])\n",
    "    lrh = GetRoiMaskedLR(select_ROI, '../dataset/')\n",
    "    my_dataset = MyDataset(lrh, training_VAE_laten_path.format(1), transform=transform)\n",
    "    generator = torch.Generator().manual_seed(random_seed)\n",
    "    trainset, validset = random_split(my_dataset, [train_size, valid_size], generator=generator)\n",
    "    train_dataloader = DataLoader(trainset, batch_size=64, shuffle=False, num_workers=num_workers)\n",
    "    val_dataloader = DataLoader(validset, batch_size=64, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    # Process on Training Dataset\n",
    "    train_loss = []\n",
    "    for train_i, data in enumerate(train_dataloader):\n",
    "        # Get data\n",
    "        voxels, image_latents = data\n",
    "        voxels = voxels.to(device).float()\n",
    "        image_latents = image_latents.to(device).float()\n",
    "\n",
    "        # Calculate similarity on voxel\n",
    "        target_latent = torch.cat([x for x in image_latents])\n",
    "        predict_latent = voxel2sd(voxels)\n",
    "        loss = F.l1_loss(target_latent, predict_latent)\n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    train_losses[model_idx] = train_loss\n",
    "    train_loss_avgs[model_idx] = np.mean(train_loss)\n",
    "    print(f'Average loss on training set: {np.mean(train_loss)}')\n",
    "\n",
    "    #################################################################################################################\n",
    "    # Process on Validation Dataset\n",
    "    valid_loss = []\n",
    "    for valid_i, data in enumerate(val_dataloader):\n",
    "        # Get data\n",
    "        voxels, image_latents = data\n",
    "        voxels = voxels.to(device).float()\n",
    "        image_latents = image_latents.to(device).float()\n",
    "\n",
    "        # Calculate similarity on voxel\n",
    "        target_latent = torch.cat([x for x in image_latents])\n",
    "        predict_latent = voxel2sd(voxels)\n",
    "        loss = F.l1_loss(target_latent, predict_latent)\n",
    "        valid_loss.append(loss.item())\n",
    "    \n",
    "    valid_losses[model_idx] = valid_loss\n",
    "    valid_loss_avgs[model_idx] = np.mean(valid_loss)\n",
    "    print(f'Average loss on validation set: {np.mean(valid_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
