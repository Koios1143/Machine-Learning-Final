{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00bd878-c5bb-4faf-aa39-0596a85ea097",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Package import]\n",
    "Import some useful packages here\n",
    "\"\"\"\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from diffusers.models import AutoencoderKL\n",
    "from Models import Voxel2StableDiffusionModel, MyDataset\n",
    "from transformers import BlipProcessor,BlipForConditionalGeneration\n",
    "from utils import load_image, to_PIL, transform, decode_img, GetROI, GetRoiMaskedLR, DoOneRoiMask\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e0f719-e4b8-4b08-8c62-876c19acdb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Load models]\n",
    "\"\"\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "modelGeneration = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", torch_dtype=torch.float16).to(device)\n",
    "sdPipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\",torch_dtype=torch.float16).to(device)\n",
    "\n",
    "\"\"\"\n",
    "[Hyperparameters and arguments]\n",
    "\"\"\"\n",
    "train_size = 0.7\n",
    "valid_size = 1 - train_size\n",
    "num_workers = torch.cuda.device_count()\n",
    "ROIs_test1 = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"FFA-1\", \"FFA-2\", \"PPA\"]\n",
    "ROIs_test2 = [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\"]\n",
    "ROIs_test3 = [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]\n",
    "ROIs_tests = [0, ROIs_test1, ROIs_test2, ROIs_test3]\n",
    "\n",
    "\"\"\"\n",
    "[Path information]\n",
    "\"\"\"\n",
    "\n",
    "dataset_path = '../dataset/'\n",
    "training_path = dataset_path + 'subj0{}/training_split/'\n",
    "training_fmri_path = training_path + 'training_fmri/'\n",
    "training_images_path = training_path + 'training_images/'\n",
    "training_VAE_laten_path = training_path + 'fMRI_VAE.npy'\n",
    "testing_path = dataset_path + 'subj0{}/test_split/test_fmri/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ee020-4de1-46a9-ac82-bc634d14223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_to_CLIP(image):\n",
    "    \"\"\"\n",
    "    Project a image to CLIP space by CLIP model.\n",
    "\n",
    "    Parameters:\n",
    "    - image: Image to project.\n",
    "\n",
    "    Returns:\n",
    "    - Projected CLIP latent\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(image).unsqueeze(0).to(device)\n",
    "        features = model.encode_image(image)\n",
    "    return features\n",
    "\n",
    "def similarity(image1_features, image2_features):\n",
    "    \"\"\"\n",
    "    Calculate similarity of two images by cosine similarity\n",
    "\n",
    "    Parameters:\n",
    "    - image1_features: Feature of 1st image\n",
    "    - image2_features: Feature of 2nd image\n",
    "\n",
    "    Returns:\n",
    "    - Similarity of two images\n",
    "    \"\"\"\n",
    "\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "    return cos(image1_features,image2_features).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fe7a6-8088-45ef-b9a1-6ad1a13db8fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_paths = ['../Models/92/199', '../Models/94/199', '../Models/97/199', '../Models/100/199']\n",
    "select_ROIs = [3, 1, 3, 3]\n",
    "random_seeds = [3, 2423, 849, 1921]\n",
    "voxel2sd = Voxel2StableDiffusionModel().to(device)\n",
    "sims_train_all = {}\n",
    "avgs_train_all = {}\n",
    "sims_valid_all = {}\n",
    "avgs_valid_all = {}\n",
    "\n",
    "for model_idx in range(4):\n",
    "    print(f'Processing model {model_paths[model_idx]}')\n",
    "    voxel2sd.load_state_dict(torch.load(model_paths[model_idx])['model_state_dict'])\n",
    "    voxel2sd.eval()\n",
    "    select_ROI = select_ROIs[model_idx]\n",
    "    random_seed = random_seeds[model_idx]\n",
    "\n",
    "    # Build data loader\n",
    "    transform = transforms.Resize([512, 512])\n",
    "    lrh = GetRoiMaskedLR(select_ROI, '../dataset/')\n",
    "    my_dataset = MyDataset(lrh, training_VAE_laten_path.format(1), transform=transform)\n",
    "    generator = torch.Generator().manual_seed(random_seed)\n",
    "    trainset, validset = random_split(my_dataset, [train_size, valid_size], generator=generator)\n",
    "    train_dataloader = DataLoader(trainset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    val_dataloader = DataLoader(validset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    # Process on Training Dataset\n",
    "    sims = []\n",
    "    avgs = []\n",
    "    cnt = 0\n",
    "    for train_i, data in enumerate(train_dataloader):\n",
    "        if(cnt == 100):\n",
    "            break\n",
    "        else:\n",
    "            cnt += 1\n",
    "        # Get data\n",
    "        voxel, image_latent = data\n",
    "        voxel = voxel.to(device).float()\n",
    "        image_latent = image_latent.to(device).float()\n",
    "\n",
    "        # Mask data\n",
    "        lh = voxel[:, :19004].cpu()\n",
    "        rh = voxel[:, 19004:].cpu()\n",
    "        lhMasked = np.zeros_like(lh, dtype=float)\n",
    "        rhMasked = np.zeros_like(rh, dtype=float)\n",
    "\n",
    "        for region in ROIs_tests[select_ROI]:\n",
    "            DoOneRoiMask(data_path='../dataset/', subject_num=1, data=lh, LR='l', roi=region, maskedFmri=lhMasked)\n",
    "            DoOneRoiMask(data_path='../dataset/', subject_num=1, data=rh, LR='r', roi=region, maskedFmri=rhMasked)\n",
    "        voxel = torch.from_numpy(np.concatenate((lhMasked, rhMasked), axis=1)).to(device).float()\n",
    "\n",
    "        # Calculate similarity on voxel\n",
    "        target_image = to_PIL(decode_img(image_latent[0], vae)[0])\n",
    "        target_image_CLIP = project_to_CLIP(target_image)[0]\n",
    "        latent_image = to_PIL(decode_img(voxel2sd(voxel), vae)[0])\n",
    "        out = transforms.functional.pil_to_tensor(transforms.Resize((384,384))(latent_image)).to(device)\n",
    "        out = modelGeneration.generate(out.unsqueeze(0)).to(device)\n",
    "        prompt = processor.decode(out[0], skip_special_tokens=True).replace('blurry ', '')\n",
    "        del out\n",
    "        predict_image = sdPipe(prompt=prompt, image=latent_image,strength=0.75, guidance_scale=6).images[0]\n",
    "        predict_image_CLIP = project_to_CLIP(predict_image)[0]\n",
    "        sim = similarity(target_image_CLIP, predict_image_CLIP)\n",
    "        sims.append(sim)\n",
    "    \n",
    "    # calculate mean sim\n",
    "    average = 0\n",
    "    for i in sims:\n",
    "        average += i\n",
    "    average /= len(sims)\n",
    "    sims_train_all[model_idx] = sims\n",
    "    avgs_train_all[model_idx] = average\n",
    "    print(f'Average similarity on training set: {average}')\n",
    "\n",
    "    #################################################################################################################\n",
    "    # Process on Validation Dataset\n",
    "    sims = []\n",
    "    avgs = []\n",
    "    cnt = 0\n",
    "    for valid_i, data in enumerate(val_dataloader):\n",
    "        if(cnt == 100):\n",
    "            break\n",
    "        else:\n",
    "            cnt += 1\n",
    "        # Get data\n",
    "        voxel, image_latent = data\n",
    "        voxel = voxel.to(device).float()\n",
    "        image_latent = image_latent.to(device).float()\n",
    "\n",
    "        # Mask data\n",
    "        lh = voxel[:, :19004].cpu()\n",
    "        rh = voxel[:, 19004:].cpu()\n",
    "        lhMasked = np.zeros_like(lh, dtype=float)\n",
    "        rhMasked = np.zeros_like(rh, dtype=float)\n",
    "\n",
    "        for region in ROIs_tests[select_ROI]:\n",
    "            DoOneRoiMask(data_path='../dataset/', subject_num=1, data=lh, LR='l', roi=region, maskedFmri=lhMasked)\n",
    "            DoOneRoiMask(data_path='../dataset/', subject_num=1, data=rh, LR='r', roi=region, maskedFmri=rhMasked)\n",
    "        voxel = torch.from_numpy(np.concatenate((lhMasked, rhMasked), axis=1)).to(device).float()\n",
    "\n",
    "        # Calculate similarity on voxel\n",
    "        target_image = to_PIL(decode_img(image_latent[0], vae)[0])\n",
    "        target_image_CLIP = project_to_CLIP(target_image)[0]\n",
    "        latent_image = to_PIL(decode_img(voxel2sd(voxel), vae)[0])\n",
    "        out = transforms.functional.pil_to_tensor(transforms.Resize((384,384))(latent_image)).to(device)\n",
    "        out = modelGeneration.generate(out.unsqueeze(0)).to(device)\n",
    "        prompt = processor.decode(out[0], skip_special_tokens=True).replace('blurry ', '')\n",
    "        del out\n",
    "        predict_image = sdPipe(prompt=prompt, image=latent_image,strength=0.75, guidance_scale=6).images[0]\n",
    "        predict_image_CLIP = project_to_CLIP(predict_image)[0]\n",
    "        sim = similarity(target_image_CLIP, predict_image_CLIP)\n",
    "        sims.append(sim)\n",
    "    \n",
    "    # calculate mean sim\n",
    "    average = 0\n",
    "    for i in sims:\n",
    "        average += i\n",
    "    average /= len(sims)\n",
    "    sims_valid_all[model_idx] = sims\n",
    "    avgs_valid_all[model_idx] = average\n",
    "    print(f'Average similarity on validation set: {average}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fa9b3-f9a0-4ff8-8e65-eaee7c1cedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sims_train_all)\n",
    "print(avgs_train_all)\n",
    "# print(sims_valid_all)\n",
    "print(avgs_valid_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
