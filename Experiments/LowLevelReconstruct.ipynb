{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be7a7f8-ec9e-4936-bc96-2bc60ff03279",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Package import]\n",
    "Import some useful packages here\n",
    "\"\"\"\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from diffusers.models import AutoencoderKL\n",
    "from Models import Voxel2StableDiffusionModel\n",
    "from transformers import BlipProcessor,BlipForConditionalGeneration\n",
    "from utils import load_image, to_PIL, transform, decode_img, GetROI\n",
    "from torchvision import transforms\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9193361-80c6-469e-ab5e-86ce852cc6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1269d4d-352f-4913-9108-85eff44a4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPredictions(target_idx):\n",
    "    \"\"\"\n",
    "    Generate low-level prediction images to target image \"target_idx\"\n",
    "\n",
    "    Note that the implementation here is not flexible, we directly choose 4 models here.\n",
    "    If you're going to run this code, please make sure you have replace the model_paths\n",
    "    to the path where you're model in.\n",
    "    \n",
    "    Also, here we only select subject 1's \"training\" image\n",
    "\n",
    "    Parameters:\n",
    "    - target_idx: The index number of image to predict.\n",
    "\n",
    "    Returns:\n",
    "    - Predictions provided by models\n",
    "    - Target image\n",
    "    \"\"\"\n",
    "\n",
    "    sample_preds = []\n",
    "    # Load models for bagging\n",
    "    # Here, we choose model 92, 94, 97, 100 since they have better performance\n",
    "    model_paths = ['../Models/92/199', '../Models/94/199', '../Models/97/199', '../Models/100/199']\n",
    "    select_ROIs = [3, 1, 3, 3]\n",
    "    \n",
    "    image_path = f'../dataset/subj01/training_split/training_images/{target_idx}.png'\n",
    "    target_image = transform(load_image(image_path))\n",
    "    voxel2sd = Voxel2StableDiffusionModel().to(device)\n",
    "    low_level_predict = []\n",
    "    \n",
    "    for i in range(len(model_paths)):    \n",
    "        voxel2sd.load_state_dict(torch.load(model_paths[i])['model_state_dict'])\n",
    "        voxel2sd.eval()\n",
    "    \n",
    "        target_ROI = torch.from_numpy(GetROI(target_idx, select_ROIs[i])).to(device).float()\n",
    "        latent = voxel2sd(target_ROI.reshape(1, -1))\n",
    "        \n",
    "        del target_ROI\n",
    "        \n",
    "        latentImg = to_PIL(decode_img(latent, vae)[0])\n",
    "        low_level_predict.append(latentImg)\n",
    "    \n",
    "        del latent\n",
    "    return target_image, low_level_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789e55a3-dab6-485b-aac3-ab6764c53c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_to_CLIP(image):\n",
    "    \"\"\"\n",
    "    Project a image to CLIP space by CLIP model.\n",
    "\n",
    "    Parameters:\n",
    "    - image: Image to project.\n",
    "\n",
    "    Returns:\n",
    "    - Projected CLIP latent\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(image).unsqueeze(0).to(device)\n",
    "        features = model.encode_image(image)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919c714b-ed0c-44cf-92ff-8587e4900937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(image1_features, image2_features):\n",
    "    \"\"\"\n",
    "    Calculate similarity of two images by cosine similarity\n",
    "\n",
    "    Parameters:\n",
    "    - image1_features: Feature of 1st image\n",
    "    - image2_features: Feature of 2nd image\n",
    "\n",
    "    Returns:\n",
    "    - Similarity of two images\n",
    "    \"\"\"\n",
    "\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "    return cos(image1_features,image2_features).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd796c76-621e-49a5-88c3-d253309d8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[Bagging Process]\n",
    "\n",
    "Generate images by our models, then perform bagging by \"CLIP_bagging\" function.\n",
    "Show result with compare images.\n",
    "\n",
    "First image is the original(target) image.\n",
    "Second image is the best image from bagging process.\n",
    "The remaining images are predictions by models.\n",
    "\n",
    "The range of i indicate the images to predict.\n",
    "\"\"\"\n",
    "from PIL import ImageOps\n",
    "\n",
    "sim_records = {}\n",
    "for i in range(201, 300):\n",
    "    target_image, low_level_predict = GetPredictions(i)\n",
    "    best_sim = 0\n",
    "    best_idx = -1\n",
    "    sims = []\n",
    "\n",
    "    bg = Image.new('RGB', (3272, 552), '#ffffff')\n",
    "    for j in range(4):\n",
    "        pred_image = low_level_predict[j]\n",
    "        pred_image_ = ImageOps.expand(pred_image, 20, (255, 255, 255)) \n",
    "        bg.paste(pred_image_, (20+512+20+512+20+40 + (512+20)*j, 0))\n",
    "        sim = similarity(project_to_CLIP(pred_image)[0], project_to_CLIP(to_PIL(target_image[0]))[0])\n",
    "        sims.append(sim)\n",
    "        if(sim > best_sim):\n",
    "            best_sim = sim\n",
    "            best_idx = j\n",
    "    \n",
    "    best_image = low_level_predict[best_idx]\n",
    "    target_image_ = ImageOps.expand(to_PIL(target_image[0]), 20, (255, 255, 255))\n",
    "    best_image_ = ImageOps.expand(best_image, 20, (255, 255, 255))\n",
    "    \n",
    "    bg.paste(target_image_, (0, 0))\n",
    "    bg.paste(best_image_, (20+512, 0))\n",
    "\n",
    "    sim_records[i] = best_sim\n",
    "    print(sims)\n",
    "    print(f'Round {i}: {best_sim}')\n",
    "    bg.save(f'../results/low_level_{i}.png')\n",
    "    bg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43431943-738e-4bc5-9696-404bd4ed8d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5493396577380952\n"
     ]
    }
   ],
   "source": [
    "avg = 0\n",
    "for i in sim_records:\n",
    "    avg += (sim_records[i])\n",
    "print(avg / len(sim_records))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
